"""
Pan123 Scraper - 123äº‘ç›˜æ™ºèƒ½æ–‡ä»¶åˆ®å‰Šå™¨
=====================================

ä¸€ä¸ªåŸºäºFlaskçš„Webåº”ç”¨ç¨‹åºï¼Œç”¨äºç®¡ç†å’Œé‡å‘½å123äº‘ç›˜ä¸­çš„åª’ä½“æ–‡ä»¶ã€‚

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
- ğŸ¬ æ™ºèƒ½æ–‡ä»¶åˆ®å‰Šï¼šè‡ªåŠ¨è¯†åˆ«ç”µå½±ã€ç”µè§†å‰§ä¿¡æ¯
- ğŸ¤– AIæ™ºèƒ½åˆ†ç»„ï¼šåŸºäºå†…å®¹è‡ªåŠ¨åˆ†ç»„å’Œå‘½å
- ğŸ“ æ™ºèƒ½é‡å‘½åï¼šä¸ºæ–‡ä»¶å¤¹ç”Ÿæˆæ ‡å‡†åŒ–åç§°
- ğŸ”„ æ‰¹é‡æ“ä½œï¼šæ”¯æŒæ‰¹é‡é‡å‘½åã€ç§»åŠ¨ã€åˆ é™¤
- ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šå®æ—¶ç›‘æ§APIæ€§èƒ½å’Œç¼“å­˜å‘½ä¸­ç‡
- ğŸ§¹ æ™ºèƒ½ç¼“å­˜ï¼šLRUç¼“å­˜ç³»ç»Ÿï¼Œè‡ªåŠ¨å†…å­˜ç®¡ç†

ğŸ›¡ï¸ æŠ€æœ¯ç‰¹æ€§ï¼š
- å®‰å…¨æ€§ï¼šæ•æ„Ÿä¿¡æ¯ä¿æŠ¤ã€å‘½ä»¤æ³¨å…¥é˜²æŠ¤ã€è¾“å…¥éªŒè¯
- é«˜æ€§èƒ½ï¼šå¤šçº¿ç¨‹å¤„ç†ã€æ™ºèƒ½ç¼“å­˜ã€QPSé™åˆ¶
- å¯ç»´æŠ¤æ€§ï¼šæ¨¡å—åŒ–è®¾è®¡ã€ç»Ÿä¸€å¼‚å¸¸å¤„ç†ã€ä»£ç é‡æ„
- å¯è§‚æµ‹æ€§ï¼šæ€§èƒ½ç›‘æ§ã€æ—¥å¿—è®°å½•ã€ç»Ÿè®¡åˆ†æ

ğŸ“‹ æ¶æ„ç»„ä»¶ï¼š
- Flask Webæ¡†æ¶ï¼šæä¾›RESTful APIæ¥å£
- LRUç¼“å­˜ç³»ç»Ÿï¼šæ™ºèƒ½å†…å­˜ç®¡ç†å’Œè¿‡æœŸæ¸…ç†
- æ€§èƒ½ç›‘æ§ï¼šAPIè°ƒç”¨ç»Ÿè®¡å’Œç¼“å­˜å‘½ä¸­ç‡åˆ†æ
- é…ç½®ç®¡ç†ï¼šç»Ÿä¸€é…ç½®éªŒè¯å’Œç±»å‹è½¬æ¢
- å¼‚å¸¸å¤„ç†ï¼šåˆ†å±‚å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤

ä½œè€…: jonntd@gmail.com
ç‰ˆæœ¬: 3.0 (é‡æ„ç‰ˆ)
æœ€åæ›´æ–°: 2025-07-01
è®¸å¯: MIT License
"""

# æ ‡å‡†åº“å¯¼å…¥
import os
import json
import logging
import threading
import datetime
import requests
import traceback
import re
import sys
import subprocess
import time
import base64
from collections import deque
import hashlib
from threading import Thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from logging.handlers import RotatingFileHandler

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from flask import Flask, render_template, request, jsonify
from collections import OrderedDict

# ================================
# åº”ç”¨ç¨‹åºåˆå§‹åŒ–å’Œå¸¸é‡å®šä¹‰
# ================================

app = Flask(__name__)


# ================================
# LRUç¼“å­˜å®ç°
# ================================

class LRUCache:
    """
    çº¿ç¨‹å®‰å…¨çš„LRUç¼“å­˜å®ç°

    Features:
    - è‡ªåŠ¨è¿‡æœŸæœºåˆ¶
    - çº¿ç¨‹å®‰å…¨
    - å†…å­˜é™åˆ¶
    """

    def __init__(self, max_size=1000, ttl=3600):
        """
        åˆå§‹åŒ–LRUç¼“å­˜

        Args:
            max_size (int): æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl (int): ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.lock = threading.Lock()

    def get(self, key, record_stats=True):
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            if key not in self.cache:
                if record_stats and hasattr(self, '_cache_name'):
                    performance_monitor.record_cache_hit(self._cache_name, hit=False)
                return None

            value, timestamp = self.cache[key]

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - timestamp > self.ttl:
                del self.cache[key]
                if record_stats and hasattr(self, '_cache_name'):
                    performance_monitor.record_cache_hit(self._cache_name, hit=False)
                return None

            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self.cache.move_to_end(key)
            if record_stats and hasattr(self, '_cache_name'):
                performance_monitor.record_cache_hit(self._cache_name, hit=True)
            return value

    def put(self, key, value):
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self.lock:
            current_time = time.time()

            if key in self.cache:
                # æ›´æ–°ç°æœ‰æ¡ç›®
                self.cache[key] = (value, current_time)
                self.cache.move_to_end(key)
            else:
                # æ·»åŠ æ–°æ¡ç›®
                if len(self.cache) >= self.max_size:
                    # ç§»é™¤æœ€æ—§çš„æ¡ç›®
                    self.cache.popitem(last=False)

                self.cache[key] = (value, current_time)

    def delete(self, key):
        """åˆ é™¤ç¼“å­˜æ¡ç›®"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()

    def cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        with self.lock:
            current_time = time.time()
            expired_keys = []

            for key, (_, timestamp) in self.cache.items():
                if current_time - timestamp > self.ttl:
                    expired_keys.append(key)

            for key in expired_keys:
                del self.cache[key]

            return len(expired_keys)

    def size(self):
        """è·å–ç¼“å­˜å¤§å°"""
        with self.lock:
            return len(self.cache)

    def stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'ttl': self.ttl
            }

    def __contains__(self, key):
        """æ”¯æŒ 'in' æ“ä½œç¬¦"""
        with self.lock:
            if key not in self.cache:
                return False

            _, timestamp = self.cache[key]

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - timestamp > self.ttl:
                del self.cache[key]
                return False

            return True

    def __getitem__(self, key):
        """æ”¯æŒ cache[key] æ“ä½œ"""
        return self.get(key)

    def __setitem__(self, key, value):
        """æ”¯æŒ cache[key] = value æ“ä½œ"""
        self.put(key, value)


# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = 'config.json'

# TMDB APIåŸºç¡€URL
TMDB_API_URL_BASE = "https://api.themoviedb.org/3"

# ================================
# AIæç¤ºè¯æ¨¡æ¿å®šä¹‰
# ================================

# åª’ä½“ä¿¡æ¯æå–æç¤ºè¯æ¨¡æ¿
# ç”¨äºä»æ–‡ä»¶åä¸­æå–ç”µå½±ã€ç”µè§†å‰§ç­‰åª’ä½“ä¿¡æ¯å¹¶åŒ¹é…TMDBæ•°æ®
EXTRACTION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“ä¿¡æ¯æå–å’Œå…ƒæ•°æ®åŒ¹é…åŠ©æ‰‹ã€‚
**ç›®æ ‡ï¼š** æ ¹æ®æä¾›çš„ç”µå½±ã€ç”µè§†å‰§æˆ–ç•ªå‰§æ–‡ä»¶ååˆ—è¡¨ï¼Œæ™ºèƒ½è§£ææ¯ä¸ªæ–‡ä»¶åï¼Œå¹¶ä»åœ¨çº¿æ•°æ®åº“ä¸­åŒ¹é…å¹¶æå–è¯¦ç»†å…ƒæ•°æ®ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœæ±‡æ€»ä¸ºä¸€ä¸ªJSONæ•°ç»„ã€‚
**è¾“å…¥ï¼š** ä¸€ä¸ªåŒ…å«å¤šä¸ªç”µå½±/ç”µè§†å‰§/ç•ªå‰§æ–‡ä»¶åï¼Œæ¯è¡Œä¸€ä¸ªæ–‡ä»¶åã€‚
**è¾“å‡ºï¼š** ä¸¥æ ¼çš„JSONæ ¼å¼ç»“æœã€‚

âš ï¸ **é‡è¦æé†’ï¼šä¿¡æ¯æå–ä¼˜å…ˆçº§**
1. **ğŸŒ ä¸­æ–‡ä¼˜å…ˆåŸåˆ™**ï¼š**é»˜è®¤ä½¿ç”¨ä¸­æ–‡æ ‡é¢˜ï¼Œæ¬¡è¦ä½¿ç”¨è‹±æ–‡æ ‡é¢˜**ã€‚æ‰€æœ‰è¾“å‡ºçš„ `title` å­—æ®µå¿…é¡»ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ï¼Œå¦‚æœæ•°æ®åº“æ²¡æœ‰ä¸­æ–‡æ ‡é¢˜æ‰ä½¿ç”¨è‹±æ–‡ã€‚
2. **æ–‡ä»¶å¤¹åç§°ä¿¡æ¯ä¼˜å…ˆ**ï¼šå¦‚æœæ–‡ä»¶è·¯å¾„åŒ…å«æ–‡ä»¶å¤¹åç§°ï¼ˆå¦‚"å†’é™©è€… (1917)[tmdbid=31819]"ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨æ–‡ä»¶å¤¹ä¸­çš„æ ‡é¢˜å’Œå¹´ä»½ä¿¡æ¯
3. **TMDB IDç›´æ¥åŒ¹é…**ï¼šå¦‚æœæ–‡ä»¶ååŒ…å«[tmdbid=æ•°å­—]ï¼Œç›´æ¥ä½¿ç”¨è¯¥IDï¼Œä¸è¦æœç´¢å…¶ä»–ç”µå½±
4. **å¹´ä»½ä¿¡æ¯ä¸¥æ ¼éµå®ˆ**ï¼šæ–‡ä»¶åä¸­çš„å¹´ä»½ä¿¡æ¯å¿…é¡»ä¸¥æ ¼éµå®ˆï¼Œä¸è¦åŒ¹é…å¹´ä»½å·®å¼‚è¿‡å¤§çš„ç”µå½±
5. **æ ‡é¢˜å‡†ç¡®æ€§**ï¼šä¼˜å…ˆä¿æŒåŸå§‹ä¸­æ–‡æ ‡é¢˜ï¼Œä¸è¦éšæ„"ç¿»è¯‘"ä¸ºè‹±æ–‡
6. **ğŸ¯ åŠ¨ç”»è§’è‰²è¯†åˆ«**ï¼šå¦‚æœæ–‡ä»¶ååŒ…å«ç‰¹å®šåŠ¨ç”»è§’è‰²åç§°ï¼Œå¿…é¡»æ­£ç¡®è¯†åˆ«æ‰€å±ç³»åˆ—ï¼š
   - åŸºå°”å…½ã€å¤§è€³å…½ã€å¤æ‹‰å…½ã€äºšå¤å…½ã€åŠ å¸ƒå…½ç­‰ â†’ æ•°ç å®è´ç³»åˆ—
   - çš®å¡ä¸˜ã€å°ç«é¾™ã€æ°å°¼é¾Ÿç­‰ â†’ å®å¯æ¢¦ç³»åˆ—
   - è·¯é£ã€ç´¢éš†ã€å¨œç¾ç­‰ â†’ æµ·è´¼ç‹ç³»åˆ—
   - é¸£äººã€ä½åŠ©ã€å°æ¨±ç­‰ â†’ ç«å½±å¿è€…ç³»åˆ—
**å¤„ç†æ­¥éª¤ï¼ˆå¯¹æ¯ä¸ªæ–‡ä»¶åé‡å¤æ‰§è¡Œï¼‰ï¼š**
1.  **æ–‡ä»¶åè§£æä¸ä¿¡æ¯æå–ï¼š**
    *   **æ ¸å¿ƒåŸåˆ™ï¼š** å°½æœ€å¤§å¯èƒ½è¯†åˆ«å¹¶ç§»é™¤æ‰€æœ‰éæ ‡é¢˜çš„æŠ€æœ¯æ€§åç¼€ã€å‰ç¼€åŠä¸­é—´æ ‡è®°ï¼Œæå–å‡ºæœ€å¯èƒ½ã€æœ€ç®€æ´çš„åŸå§‹æ ‡é¢˜éƒ¨åˆ†ã€‚
    *   **éœ€è¦ç§»é™¤çš„å¸¸è§æ ‡è®°ï¼ˆä½†ä¸é™äºï¼‰ï¼š**
        *   **åˆ†è¾¨ç‡:** 2160p, 1080p, 720p, 4K, UHD, SD
        *   **è§†é¢‘ç¼–ç :** H264, H265, HEVC, x264, x265, AVC, VP9, AV1, DivX, XviD
        *   **æ¥æº/å‹åˆ¶:** WEB-DL, BluRay, HDTV, WEBRip, BDRip, DVDRip, KORSUB, iNTERNAL, Remux, PROPER, REPACK, RETAIL, Disc, VOSTFR, DUBBED, SUBBED, FanSub, CBR, VBR, P2P
        *   **éŸ³é¢‘ç¼–ç /å£°é“:** DDP5.1, Atmos, DTS-HD MA, TrueHD, AC3, AAC, FLAC, DD+7.1, Opus, MP3, 2.0, 5.1, 7.1, Multi-Audio, Dual Audio
        *   **HDR/æœæ¯”è§†ç•Œ:** DV, HDR, HDR10, DoVi, HLG, HDR10+, WCG
        *   **ç‰ˆæœ¬ä¿¡æ¯:** Director's Cut, Extended, Uncut, Theatrical, Special Edition, Ultimate Edition, Remastered, ReCut, Criterion, IMAX, Limited Series
        *   **å‘å¸ƒç»„/ç«™ç‚¹:** [RARBG], [YTS.AM], FGT, CtrlHD, DEFLATE, xixi, EVO, GHOULS, FRDS, PANTHEON, WiKi, CHDBits, OurBits, MTeam, LoL, TRP, FWB, x264-GROUP, VCB-Studio, ANi, Lilith-Raws
        *   **å­£/é›†å·:** S01E01, S1E1, Season 1 Episode 1, Part 1, P1, Ep01, Vol.1, ç¬¬1å­£ç¬¬1é›†, SP (Special), OVA, ONA, Movie (å¯¹äºç•ªå‰§å‰§åœºç‰ˆ), NCED, NCOP (æ— å­—å¹•OP/ED), æ–‡ä»¶åå¼€å¤´çš„æ•°å­—ï¼ˆå¦‚"01. æ ‡é¢˜"ã€"02. æ ‡é¢˜"ç­‰ï¼‰
        *   **å¹´ä»½:** (2023), [2023], .2023., _2023_
        *   **å…¶ä»–:** (R), _ , -, ., ~ , { }, [ ], ` `, + ç­‰å¸¸è§åˆ†éš”ç¬¦ï¼Œä»¥åŠå¹¿å‘Šè¯ã€å¤šä½™çš„ç©ºæ ¼ã€å¤šä½™çš„è¯­è¨€ä»£ç ï¼ˆå¦‚CHS, ENG, JPNï¼‰ç­‰ã€‚
    *   **æå–ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š**
        *   **ä¸­æ–‡æ ‡é¢˜ (title):** æœ€å¯èƒ½ã€æœ€ç®€æ´çš„ç”µå½±/ç”µè§†å‰§/ç•ªå‰§ä¸­æ–‡æ ‡é¢˜ã€‚**ä¼˜å…ˆçº§ï¼šä¸­æ–‡æ ‡é¢˜ > è‹±æ–‡æ ‡é¢˜**ã€‚å¦‚æœæ–‡ä»¶ååŒ…å«ä¸­æ–‡ï¼Œå¿…é¡»æå–ä¸­æ–‡æ ‡é¢˜ï¼›å¦‚æœåªæœ‰è‹±æ–‡ï¼Œåˆ™æå–è‹±æ–‡æ ‡é¢˜ã€‚
        *   **å¹´ä»½ (year):** è¯†åˆ«åˆ°çš„å‘è¡Œå¹´ä»½ã€‚
        *   **å­£å· (season):** ç”µè§†å‰§æˆ–ç•ªå‰§çš„å­£å·ï¼Œé€šå¸¸ä¸ºæ•°å­—ã€‚
        *   **é›†å· (episode):** ç”µè§†å‰§æˆ–ç•ªå‰§çš„é›†å·ï¼Œé€šå¸¸ä¸ºæ•°å­—ï¼Œä¾‹å¦‚ï¼š'/æˆ‘çš„æ¥æ”¶/ä¹ä¸¹ Jiudan.EP01-37.2013.1080p/11.mp4', episode is 11ã€‚
        *   **éƒ¨åˆ† (part):** å¦‚æœæ˜¯ç”µå½±çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆå¦‚ Part 1, Disc 2ï¼Œéç³»åˆ—ç”µå½±çš„ç»­é›†ï¼‰ï¼Œæˆ–ç•ªå‰§çš„OVA/SPç­‰ç‰¹æ®Šé›†ã€‚

2.  **åœ¨çº¿æ•°æ®åº“æœç´¢ä¸åŒ¹é…ï¼š**
    *   **æ“ä½œæŒ‡ç¤ºï¼š** **å¿…é¡»ä½¿ç”¨ä½ çš„è”ç½‘æœç´¢å·¥å…·**ã€‚
    *   **æœç´¢å…³é”®è¯æ„å»ºï¼š**
        *   **ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡é¢˜**ï¼šå¦‚æœè§£æå‡ºä¸­æ–‡ `title`ï¼Œä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡é¢˜è¿›è¡Œæœç´¢ã€‚
        *   **è‹±æ–‡æ ‡é¢˜ä½œä¸ºè¡¥å……**ï¼šå¦‚æœä¸­æ–‡æœç´¢æ— ç»“æœï¼Œå†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜æœç´¢ã€‚
        *   **æœç´¢ç­–ç•¥**ï¼š`"ä¸­æ–‡æ ‡é¢˜ å¹´ä»½ TMDB"` â†’ `"English Title å¹´ä»½ TMDB"`
        *   ç¤ºä¾‹æœç´¢è¯ï¼š`"å†’é™©è€… 1917 TMDB"`, `"ä¸€è·¯å‘ä¸œ 1920 TMDB"`, `"The Adventurer 1917 TMDB"`ã€‚
    *   **ä¼˜å…ˆé¡ºåºï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰ï¼š**
        1.  **è±†ç“£ç”µå½± (Douban):** å¯¹äºä¸­æ–‡å†…å®¹çš„é¦–é€‰ï¼Œæä¾›å‡†ç¡®çš„ä¸­æ–‡æ ‡é¢˜å’Œä¿¡æ¯ã€‚
        2.  **themoviedb.org (TMDB):** é’ˆå¯¹ç”µå½±å’Œç”µè§†å‰§ï¼Œæ”¯æŒå¤šè¯­è¨€åŒ…æ‹¬ä¸­æ–‡ã€‚
        3.  **AniDB:** é’ˆå¯¹åŠ¨ç”»ã€åŠ¨æ¼«ï¼ˆç•ªå‰§ï¼‰ï¼Œæ”¯æŒä¸­æ–‡æ ‡é¢˜ã€‚
        4.  **IMDb:** ä½œä¸ºè¡¥å……æ•°æ®æºï¼Œä¸»è¦æä¾›è‹±æ–‡ä¿¡æ¯ã€‚
        5.  **çƒ‚ç•ªèŒ„ (Rotten Tomatoes):** ä½œä¸ºè¯„åˆ†å’Œè¡¥å……ä¿¡æ¯æ¥æºã€‚
    *   **åŒ¹é…ç­–ç•¥ï¼š**
        *   ä½¿ç”¨æå–å‡ºçš„æ ‡é¢˜ã€å¹´ä»½ã€å­£å·ï¼ˆå¦‚æœé€‚ç”¨ï¼‰è¿›è¡Œç²¾å‡†æœç´¢ã€‚
        *   **é«˜ç½®ä¿¡åº¦åŒ¹é…ï¼š** åªæœ‰å½“æœç´¢ç»“æœä¸è§£æå‡ºçš„æ ‡é¢˜é«˜åº¦ç›¸ä¼¼ï¼ˆè€ƒè™‘å¤§å°å†™ã€æ ‡ç‚¹ç¬¦å·ã€å¸¸è§ç¼©å†™ç­‰ï¼‰ï¼Œå¹´ä»½ç²¾ç¡®åŒ¹é…ï¼Œä¸”åª’ä½“ç±»å‹ï¼ˆç”µå½±/ç”µè§†å‰§/ç•ªå‰§ï¼‰ä¸€è‡´æ—¶ï¼Œæ‰è®¤å®šä¸ºå‡†ç¡®åŒ¹é…ã€‚
        *   **å”¯ä¸€æ€§åŸåˆ™ï¼š** å¦‚æœæœç´¢ç»“æœåŒ…å«å¤šä¸ªæ¡ç›®ï¼Œé€‰æ‹©ä¸æ–‡ä»¶åä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯å¹´ä»½ã€ç‰ˆæœ¬ã€å­£é›†å·ï¼‰æœ€åŒ¹é…çš„**å”¯ä¸€**æ¡ç›®ã€‚
        *   **æ¨¡ç³ŠåŒ¹é…å›é€€ï¼š** å¦‚æœç²¾å‡†åŒ¹é…å¤±è´¥ï¼Œå¯ä»¥å°è¯•è¿›è¡Œè½»å¾®çš„æ¨¡ç³ŠåŒ¹é…ï¼ˆä¾‹å¦‚ç§»é™¤å‰¯æ ‡é¢˜ã€å°è¯•å¸¸è§ç¼©å†™ï¼‰ï¼Œä½†éœ€é™ä½ç½®ä¿¡åº¦ã€‚
        *   **æ— æ³•åŒ¹é…çš„å¤„ç†ï¼š** å¦‚æœæ— æ³•æ‰¾åˆ°é«˜ç½®ä¿¡åº¦çš„åŒ¹é…é¡¹ï¼Œåˆ™è¯¥æ¡ç›®çš„å…ƒæ•°æ®å­—æ®µåº”ä¸ºç©ºæˆ– nullã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
*   è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸”åªåŒ…å«JSONå†…å®¹ï¼Œä¸é™„å¸¦ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–é¢å¤–æ–‡æœ¬ã€‚
*   æ ¹å…ƒç´ å¿…é¡»æ˜¯ä¸€ä¸ªJSONæ•°ç»„ `[]`ã€‚
*   æ•°ç»„çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ªæ–‡ä»¶åçš„è§£æç»“æœã€‚
*   JSONç»“æ„å¦‚ä¸‹ï¼š
    ```json
    [
      {
        "file_name": "string",
        "title": "string",            // ä¸­æ–‡æ ‡é¢˜ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¦‚æ— ä¸­æ–‡åˆ™ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
        "original_title": "string",   // åª’ä½“çš„åŸå§‹è¯­è¨€æ ‡é¢˜ (å¦‚æ—¥æ–‡, éŸ©æ–‡ç­‰ï¼Œéä¸­è‹±æ–‡)
        "year": "string",             // å‘è¡Œå¹´ä»½
        "media_type": "string",       // "movie", "tv_show", "anime"
        "tmdb_id": "string",          // TMDB ID
        "imdb_id": "string",          // IMDb ID
        "anidb_id": "string",         // AniDB ID (å¦‚æœé€‚ç”¨)
        "douban_id": "string",        // è±†ç“£ ID (å¦‚æœé€‚ç”¨)
        "season": "number | null",    // æ–‡ä»¶åè§£æå‡ºçš„å­£å·
        "episode": "number | null"    // æ–‡ä»¶åè§£æå‡ºçš„é›†å·
      }
    ]
    ```
*   **å­—æ®µè¯´æ˜ï¼š**
    *   `file_name`: åŸå§‹æ–‡ä»¶åã€‚
    *   `title`: **ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡é¢˜**ã€‚å¦‚æœæ•°æ®åº“æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼›å¦‚æœæ²¡æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œåˆ™ä½¿ç”¨è‹±æ–‡æ ‡é¢˜ã€‚
    *   `original_title`: åª’ä½“åœ¨åŸäº§åœ°çš„åŸå§‹è¯­è¨€æ ‡é¢˜ (ä¾‹å¦‚ï¼Œæ—¥å‰§çš„æ—¥æ–‡æ ‡é¢˜ï¼ŒéŸ©å‰§çš„éŸ©æ–‡æ ‡é¢˜)ã€‚å¦‚æœæ˜¯ä¸­æ–‡ä½œå“æˆ–ä¸ `title` ç›¸åŒï¼Œåˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸² `""`ã€‚
    *   `year`: ç”µå½±/ç”µè§†å‰§/ç•ªå‰§çš„å‘è¡Œå¹´ä»½ã€‚
    *   `media_type`: è¯†åˆ«å‡ºçš„åª’ä½“ç±»å‹ï¼Œåªèƒ½æ˜¯ `"movie"`, `"tv_show"`, `"anime"` ä¹‹ä¸€ã€‚
    *   `tmdb_id`, `imdb_id`, `anidb_id`, `douban_id`: å¯¹åº”æ•°æ®åº“çš„å”¯ä¸€IDã€‚å¦‚æœæœªæ‰¾åˆ°æˆ–ä¸é€‚ç”¨ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸² `""`ã€‚
*   **å€¼çº¦å®šï¼š**
    *   å­—ç¬¦ä¸²å­—æ®µï¼ˆ`title`, `original_title`, `year`, `media_type`, `tmdb_id` ç­‰ï¼‰å¦‚æœä¿¡æ¯ç¼ºå¤±æˆ–æ— æ³•å‡†ç¡®è¯†åˆ«ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸² `""`ã€‚
    *   æ•°å­—å­—æ®µï¼ˆ`season`, `episode`ï¼‰å¦‚æœä¿¡æ¯ç¼ºå¤±æˆ–ä¸é€‚ç”¨ï¼Œè¯·ä½¿ç”¨ `null`ã€‚
    *   **ä¸¥æ ¼æ€§è¦æ±‚ï¼š** ä»»ä½•æ—¶å€™éƒ½ä¸è¦åœ¨JSONè¾“å‡ºä¸­åŒ…å«é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚
"""


# æ™ºèƒ½æ–‡ä»¶åˆ†ç»„æç¤ºè¯æ¨¡æ¿ - ç®€åŒ–ç‰ˆ
# ç”¨äºå°†ç›¸å…³çš„åª’ä½“æ–‡ä»¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„ï¼Œå¦‚ç³»åˆ—ç”µå½±ã€ç”µè§†å‰§é›†ç­‰
MAGIC_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†æ–‡ä»¶åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ï¼Œæ ¹æ®æ–‡ä»¶åå°†ç›¸å…³æ–‡ä»¶åˆ†ç»„ã€‚

âš ï¸ **ä¸¥æ ¼è­¦å‘Š**ï¼šåªæœ‰ä¸»æ ‡é¢˜ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ä»¶æ‰èƒ½åˆ†ç»„ï¼

ğŸš« **ç»å¯¹ç¦æ­¢çš„é”™è¯¯åˆ†ç»„**ï¼š
- âŒ ä¸è¦å°†"æµ·åº•å°çºµé˜Ÿ"ã€"ç–¯ç‹‚å…ƒç´ åŸ"ã€"è“ç²¾çµ"åˆ†åœ¨ä¸€ç»„
- âŒ ä¸è¦ä»…å› ä¸ºéƒ½æ˜¯åŠ¨ç”»ç‰‡å°±åˆ†ç»„
- âŒ ä¸è¦ä»…å› ä¸ºéƒ½æ˜¯è¿ªå£«å°¼/çš®å…‹æ–¯å°±åˆ†ç»„
- âŒ ä¸è¦ä»…å› ä¸ºå¹´ä»½ç›¸è¿‘å°±åˆ†ç»„
- âŒ ä¸è¦å°†å®Œå…¨ä¸åŒçš„IP/å“ç‰Œæ··åˆ

ğŸ“º **ç”µè§†å‰§å‘½åæ ¼å¼ä¸¥æ ¼è¦æ±‚**ï¼š
- âœ… å¿…é¡»ä½¿ç”¨ï¼š`æ ‡é¢˜ (é¦–æ’­å¹´ä»½) Så­£æ•°`
- âœ… æ­£ç¡®ç¤ºä¾‹ï¼šSEAL Team (2017) S01, SEAL Team (2018) S02
- âŒ ç¦æ­¢æ ¼å¼ï¼šSEAL Team S01, SEAL Team (Season 1), SEAL Team (S01), SEAL Team (2018-2019) S02

âœ… **æ­£ç¡®åˆ†ç»„æ ‡å‡†**ï¼šä¸»æ ‡é¢˜å¿…é¡»ç›¸åŒæˆ–ä¸ºåŒä¸€ç³»åˆ—çš„ç»­é›†/å‰ä¼ 

## ğŸ“‹ æ–‡ä»¶åæ¨¡å¼è¯†åˆ«æŒ‡å—

### ğŸ¬ å¸¸è§æ–‡ä»¶åæ ¼å¼ï¼š
1. **ç”µè§†å‰§æ ¼å¼**ï¼š
   - `æ ‡é¢˜ (å¹´ä»½) S01E01.mkv` â†’ é—´è°è¿‡å®¶å®¶ (2022) S01E01.mkv
   - `æ ‡é¢˜ ç¬¬1å­£ ç¬¬1é›†.mkv` â†’ 180å¤©é‡å¯è®¡åˆ’ ç¬¬1å­£ ç¬¬1é›†.mkv
   - `Title S01E01.mkv` â†’ SPYÃ—FAMILY S01E01.mkv
   - `æ ‡é¢˜.S01E01.mkv` â†’ äº²çˆ±çš„å…¬ä¸»ç—….S01E01.mkv
   - `æ ‡é¢˜ S01 E01.mkv` â†’ æŸå‰§ S01 E01.mkv

2. **ç”µå½±ç³»åˆ—æ ¼å¼**ï¼š
   - `æ ‡é¢˜1 (å¹´ä»½).mkv` + `æ ‡é¢˜2 (å¹´ä»½).mkv` â†’ åŒç³»åˆ—ç”µå½±
   - `æ ‡é¢˜ ç¬¬ä¸€éƒ¨.mkv` + `æ ‡é¢˜ ç¬¬äºŒéƒ¨.mkv` â†’ ç³»åˆ—ç”µå½±
   - `æ ‡é¢˜ä¹‹å‰¯æ ‡é¢˜ (å¹´ä»½).mkv` â†’ ç³»åˆ—ç”µå½±

3. **ç‰¹æ®Šæ ‡è¯†ç¬¦**ï¼š
   - `{tmdb-123456}` â†’ TMDBæ•°æ®åº“IDï¼Œå¯å¿½ç•¥
   - `1080p`, `720p`, `4K` â†’ åˆ†è¾¨ç‡æ ‡è¯†ï¼Œå¯å¿½ç•¥
   - `x264`, `x265`, `HEVC` â†’ ç¼–ç æ ¼å¼ï¼Œå¯å¿½ç•¥
   - `GB`, `MB` â†’ æ–‡ä»¶å¤§å°ï¼Œå¯å¿½ç•¥

### ğŸ¯ åˆ†ç»„è§„åˆ™ï¼š
1. **æå…¶ä¸¥æ ¼çš„ç³»åˆ—åˆ¤æ–­æ ‡å‡†**ï¼š
   - **ä¸»æ ‡é¢˜å¿…é¡»å®Œå…¨ç›¸åŒæˆ–ä¸ºæ˜ç¡®çš„ç»­é›†å…³ç³»**
   - ä¾‹å¦‚ï¼š"è“ç²¾çµ"ã€"è“ç²¾çµ2"ã€"è“ç²¾çµä¹‹åœ£è¯é¢‚æ­Œ"
   - ä¾‹å¦‚ï¼š"åŠŸå¤«ç†ŠçŒ«"ã€"åŠŸå¤«ç†ŠçŒ«2"ã€"åŠŸå¤«ç†ŠçŒ«3"
   - **ç»å¯¹ä¸å…è®¸ä¸åŒIPæ··åˆ**

2. **ä¸¥æ ¼ç¦æ­¢çš„åˆ†ç»„æ¨¡å¼**ï¼š
   - âŒ ç»ä¸æŒ‰ç±»å‹åˆ†ç»„ï¼ˆåŠ¨ç”»ç‰‡ã€åŠ¨ä½œç‰‡ã€å–œå‰§ç‰‡ç­‰ï¼‰
   - âŒ ç»ä¸æŒ‰åˆ¶ä½œå…¬å¸åˆ†ç»„ï¼ˆè¿ªå£«å°¼ã€çš®å…‹æ–¯ã€æ¢¦å·¥å‚ç­‰ï¼‰
   - âŒ ç»ä¸æŒ‰å¹´ä»½åˆ†ç»„
   - âŒ ç»ä¸æŒ‰ä¸»é¢˜åˆ†ç»„ï¼ˆè¶…çº§è‹±é›„ã€å…¬ä¸»ã€åŠ¨ç‰©ç­‰ï¼‰

3. **åˆ†ç»„è¦æ±‚**ï¼š
   - æ¯ç»„è‡³å°‘2ä¸ªæ–‡ä»¶
   - å•ä¸ªæ–‡ä»¶ä¸åˆ†ç»„
   - **å®å¯ä¸åˆ†ç»„ï¼Œä¹Ÿä¸è¦é”™è¯¯åˆ†ç»„**
   - **ç”µè§†å‰§åˆ†å­£åŸåˆ™**ï¼šåŒä¸€éƒ¨å‰§çš„ä¸åŒå­£åº”è¯¥åˆ†åˆ«åˆ†ç»„ï¼Œæ¯å­£ä½¿ç”¨è¯¥å­£çš„é¦–æ’­å¹´ä»½
   - **ç”µè§†å‰§å‘½åæ ¼å¼ä¸¥æ ¼è¦æ±‚**ï¼š
     * ç»å¯¹ä¸å…è®¸ï¼šSEAL Team S01, SEAL Team (Season 1), SEAL Team (S01)
     * ç»å¯¹ä¸å…è®¸ï¼šSEAL Team (2018-2019) S02ï¼ˆå¹´ä»½èŒƒå›´ï¼‰
     * å¿…é¡»ä½¿ç”¨ï¼šSEAL Team (2017) S01, SEAL Team (2018) S02ï¼ˆå•ä¸€å¹´ä»½ï¼‰

4. **å‘½åè§„èŒƒ**ï¼š
   - ç”µè§†å‰§ï¼š**å¿…é¡»ä½¿ç”¨** `æ ‡é¢˜ (é¦–æ’­å¹´ä»½) Så­£æ•°` æ ¼å¼
   - **ä¸¥æ ¼è¦æ±‚**ï¼šæ¯å­£éƒ½å¿…é¡»åŒ…å«è¯¥å­£çš„é¦–æ’­å¹´ä»½ï¼Œå¦‚ SEAL Team (2017) S01, SEAL Team (2018) S02
   - ç”µå½±ç³»åˆ—ï¼š`æ ‡é¢˜ç³»åˆ— (å¹´ä»½èŒƒå›´)`
   - **ç³»åˆ—å®Œæ•´æ€§ä¼˜å…ˆ**ï¼šåŒä¸€ç³»åˆ—çš„æ‰€æœ‰ç”µå½±åº”è¯¥æ”¾åœ¨ä¸€ä¸ªç»„é‡Œ
   - **é‡è¦**ï¼šå®å¯æ¢¦å‰§åœºç‰ˆã€åä¾¦æ¢æŸ¯å—å‰§åœºç‰ˆç­‰é•¿æœŸç³»åˆ—åº”è¯¥å…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼Œä¸è¦æŒ‰å¹´ä»½åˆ†æ®µ
   - **æ–‡ä»¶æ•°é‡ä¸æ˜¯é™åˆ¶**ï¼šå³ä½¿æœ‰20-30ä¸ªæ–‡ä»¶ä¹Ÿåº”è¯¥ä¿æŒç³»åˆ—å®Œæ•´æ€§

5. **å…·ä½“ç¦æ­¢ç¤ºä¾‹**ï¼š
   - âŒ "æµ·åº•å°çºµé˜Ÿ" + "è“ç²¾çµ" = é”™è¯¯ï¼å®Œå…¨ä¸åŒçš„IP
   - âŒ "ç–¯ç‹‚å…ƒç´ åŸ" + "è“ç²¾çµ" = é”™è¯¯ï¼å®Œå…¨ä¸åŒçš„IP
   - âŒ "å†°é›ªå¥‡ç¼˜" + "é­”å‘å¥‡ç¼˜" = é”™è¯¯ï¼è™½ç„¶éƒ½æ˜¯è¿ªå£«å°¼å…¬ä¸»ç‰‡ä½†ä¸æ˜¯åŒä¸€ç³»åˆ—
   - âœ… "è“ç²¾çµ" + "è“ç²¾çµ2" = æ­£ç¡®ï¼åŒä¸€ç³»åˆ—çš„ç»­é›†

### ğŸ’¡ æ­£ç¡®åˆ†æç¤ºä¾‹ï¼š
**ç¤ºä¾‹1 - ç”µè§†å‰§æ­£ç¡®åˆ†ç»„**ï¼š
- `SEAL Team S01E01.mkv` (ID: 101)
- `SEAL Team S01E02.mkv` (ID: 102)
- `SEAL Team S02E01.mkv` (ID: 103)
- `SEAL Team S02E02.mkv` (ID: 104)

**âœ… æ­£ç¡®åˆ†ç»„**ï¼šå¿…é¡»åŒ…å«å¹´ä»½ï¼Œæ ¼å¼ç»Ÿä¸€
```json
[
  {"group_name": "SEAL Team (2017) S01", "fileIds": [101, 102]},
  {"group_name": "SEAL Team (2018) S02", "fileIds": [103, 104]}
]
```

**âŒ é”™è¯¯çš„å‘½åæ ¼å¼**ï¼š
```json
[
  {"group_name": "SEAL Team S01", "fileIds": [101, 102]},           // ç¼ºå°‘å¹´ä»½
  {"group_name": "SEAL Team (Season 1)", "fileIds": [101, 102]},   // æ ¼å¼é”™è¯¯
  {"group_name": "SEAL Team (2017-2018) S01", "fileIds": [101, 102]}, // å¹´ä»½èŒƒå›´é”™è¯¯
  {"group_name": "SEAL Team (S01)", "fileIds": [101, 102]}         // æ ¼å¼é”™è¯¯
]
```

**ç¤ºä¾‹2 - ç”µè§†å‰§æ­£ç¡®åˆ†ç»„**ï¼š
- `é—´è°è¿‡å®¶å®¶ (2022) S01E01.mkv` (ID: 123)
- `é—´è°è¿‡å®¶å®¶ (2022) S01E02.mkv` (ID: 124)
- `SPYÃ—FAMILY S01E03.mkv` (ID: 125)
- `äº²çˆ±çš„å…¬ä¸»ç—… (2016) S01E01.mkv` (ID: 126)

**æ­£ç¡®åˆ†æ**ï¼šè¯†åˆ«"é—´è°è¿‡å®¶å®¶"å’Œ"SPYÃ—FAMILY"ä¸ºåŒä¸€ç³»åˆ—ï¼ˆä¸åŒè¯­è¨€ï¼‰ï¼Œ"äº²çˆ±çš„å…¬ä¸»ç—…"ä¸ºä¸åŒç³»åˆ—
```json
[{"group_name": "é—´è°è¿‡å®¶å®¶ (2022) S01", "fileIds": [123, 124, 125]}]
```

**ç¤ºä¾‹2 - é”™è¯¯åˆ†ç»„è­¦ç¤º**ï¼š
- `è“ç²¾çµ2 (2013).mkv` (ID: 201)
- `è“ç²¾çµä¹‹åœ£è¯é¢‚æ­Œ (2011).mkv` (ID: 202)
- `æµ·åº•å°çºµé˜Ÿï¼šæ´ç©´å¤§å†’é™© (2020).mp4` (ID: 203)
- `ç–¯ç‹‚å…ƒç´ åŸ (2023).mp4` (ID: 204)

**âŒ ç»å¯¹é”™è¯¯çš„åˆ†ç»„**ï¼š
```json
[{"group_name": "è“ç²¾çµç³»åˆ— (2011-2017)", "fileIds": [201, 202, 203, 204]}]
```
è¿™æ˜¯é”™è¯¯çš„ï¼æµ·åº•å°çºµé˜Ÿå’Œç–¯ç‹‚å…ƒç´ åŸä¸è“ç²¾çµå®Œå…¨æ— å…³ï¼

**âœ… æ­£ç¡®åˆ†ç»„**ï¼šåªå°†çœŸæ­£ç›¸å…³çš„æ–‡ä»¶åˆ†ç»„
```json
[{"group_name": "è“ç²¾çµç³»åˆ— (2011-2013)", "fileIds": [201, 202]}]
```

**ç¤ºä¾‹3 - æ›´å¤šé”™è¯¯åˆ†ç»„è­¦ç¤º**ï¼š
- `å†°é›ªå¥‡ç¼˜ (2013).mkv` (ID: 301)
- `é­”å‘å¥‡ç¼˜ (2010).mkv` (ID: 302)
- `æµ·æ´‹å¥‡ç¼˜ (2016).mkv` (ID: 303)

**âŒ é”™è¯¯**ï¼šå°†è¿ªå£«å°¼å…¬ä¸»ç”µå½±åˆ†åœ¨ä¸€ç»„
**âœ… æ­£ç¡®**ï¼šä¸åˆ†ç»„ï¼ˆå®ƒä»¬æ˜¯ä¸åŒçš„ç‹¬ç«‹ç”µå½±ï¼‰

**ç¤ºä¾‹4 - å‰§åœºç‰ˆç³»åˆ—æ­£ç¡®åˆ†ç»„**ï¼š
- `å®å¯æ¢¦å‰§åœºç‰ˆï¼šä¸ƒå¤œçš„è®¸æ„¿æ˜Ÿ åŸºæ‹‰ç¥ˆ (2003).mkv` (ID: 401)
- `å®å¯æ¢¦å‰§åœºç‰ˆï¼šè£‚ç©ºçš„è®¿é—®è€… ä»£æ¬§å¥‡å¸Œæ–¯ (2004).mkv` (ID: 402)
- `å®å¯æ¢¦å‰§åœºç‰ˆï¼šæ¢¦å¹»ä¸æ³¢å¯¼çš„å‹‡è€… è·¯å¡åˆ©æ¬§ (2005).mkv` (ID: 403)
- `å®å¯æ¢¦å‰§åœºç‰ˆï¼šå¸ç‰™å¢å¡VSå¸•è·¯å¥‡äºšVSè¾¾å…‹è±ä¼Š (2007).mkv` (ID: 404)
- `å®å¯æ¢¦å‰§åœºç‰ˆï¼šé˜¿å°”å®™æ–¯ è¶…å…‹çš„æ—¶ç©º (2009).mkv` (ID: 405)

**âœ… æ­£ç¡®åˆ†ç»„**ï¼šæ‰€æœ‰å®å¯æ¢¦å‰§åœºç‰ˆåº”è¯¥æ”¾åœ¨ä¸€ä¸ªç»„é‡Œ
```json
[{"group_name": "å®å¯æ¢¦å‰§åœºç‰ˆç³»åˆ— (1998-2020)", "fileIds": [401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424]}]
```

**âŒ é”™è¯¯çš„å¹´ä»½åˆ†æ®µ**ï¼š
```json
[
  {"group_name": "å®å¯æ¢¦å‰§åœºç‰ˆ (1998-2002)", "fileIds": [401, 402, 403, 404, 405]},
  {"group_name": "å®å¯æ¢¦å‰§åœºç‰ˆ (2003-2007)", "fileIds": [406, 407, 408, 409, 410]},
  {"group_name": "å®å¯æ¢¦å‰§åœºç‰ˆ (2008-2012)", "fileIds": [411, 412, 413, 414, 415]}
]
```
è¿™æ˜¯é”™è¯¯çš„ï¼ä¸è¦æŒ‰å¹´ä»½åˆ†æ®µï¼

**æ ¸å¿ƒåŸåˆ™**ï¼šä¸»æ ‡é¢˜å¿…é¡»ç›¸åŒï¼Œç³»åˆ—å®Œæ•´æ€§ä¼˜å…ˆäºå¹´ä»½åˆ†æ®µï¼

### ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
```json
[
  {
    "group_name": "é—´è°è¿‡å®¶å®¶ (2022) S01",
    "fileIds": [æ–‡ä»¶ID1, æ–‡ä»¶ID2, æ–‡ä»¶ID3]
  },
  {
    "group_name": "å¤ä»‡è€…è”ç›Ÿç³»åˆ— (2012-2019)",
    "fileIds": [æ–‡ä»¶ID4, æ–‡ä»¶ID5]
  }
]
```

### âš ï¸ æœ€ç»ˆè­¦å‘Šï¼š
- **åªæœ‰ä¸»æ ‡é¢˜ç›¸åŒçš„æ–‡ä»¶æ‰èƒ½åˆ†ç»„ï¼**
- **ç»å¯¹ç¦æ­¢å°†ä¸åŒIP/å“ç‰Œçš„ä½œå“åˆ†åœ¨ä¸€ç»„ï¼**
- **ç»å¯¹ç¦æ­¢æŒ‰ç±»å‹ã€å…¬å¸ã€å¹´ä»½ã€ä¸»é¢˜åˆ†ç»„ï¼**
- **åŒä¸€ç³»åˆ—çš„æ‰€æœ‰æ–‡ä»¶åº”è¯¥æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼ˆå¦‚æ‰€æœ‰å®å¯æ¢¦å‰§åœºç‰ˆï¼‰ï¼**
- å¿…é¡»è¿”å›å®Œæ•´çš„JSONæ ¼å¼
- fileIdsæ•°ç»„å¿…é¡»åŒ…å«æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„ID
- å¦‚æœæ²¡æœ‰å¯åˆ†ç»„çš„æ–‡ä»¶ï¼Œè¿”å› []
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
- **å®å¯è¿”å›ç©ºæ•°ç»„[]ï¼Œä¹Ÿä¸è¦é”™è¯¯åˆ†ç»„ï¼**
- **å†æ¬¡å¼ºè°ƒï¼šæµ·åº•å°çºµé˜Ÿã€ç–¯ç‹‚å…ƒç´ åŸã€è“ç²¾çµæ˜¯å®Œå…¨ä¸åŒçš„IPï¼Œç»ä¸èƒ½åˆ†åœ¨ä¸€ç»„ï¼**
- **å®å¯æ¢¦å‰§åœºç‰ˆç³»åˆ—åº”è¯¥å…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼Œä¸è¦æŒ‰å¹´ä»½åˆ†æ®µï¼**
- **åä¾¦æ¢æŸ¯å—å‰§åœºç‰ˆã€èœ¡ç¬”å°æ–°å‰§åœºç‰ˆç­‰é•¿æœŸç³»åˆ—ä¹Ÿåº”è¯¥å…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼**
- **ç»å¯¹ä¸è¦å› ä¸ºæ–‡ä»¶æ•°é‡å¤šå°±æŒ‰å¹´ä»½åˆ†æ®µï¼ç³»åˆ—å®Œæ•´æ€§æœ€é‡è¦ï¼**
- **ç”µè§†å‰§å‘½åæ ¼å¼å¿…é¡»ä¸¥æ ¼ç»Ÿä¸€ï¼šæ ‡é¢˜ (é¦–æ’­å¹´ä»½) Så­£æ•°ï¼**
- **ç»å¯¹ç¦æ­¢çš„ç”µè§†å‰§æ ¼å¼ï¼šSEAL Team S01, SEAL Team (Season 1), SEAL Team (S01), SEAL Team (2018-2019) S02ï¼**
- **å¿…é¡»ä½¿ç”¨çš„æ­£ç¡®æ ¼å¼ï¼šSEAL Team (2017) S01, SEAL Team (2018) S02, SEAL Team (2019) S03ï¼**

"""
# æ™ºèƒ½åˆ†ç»„åˆå¹¶æç¤ºè¯æ¨¡æ¿
GROUP_MERGE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†åˆ†ç±»ä¸“å®¶ã€‚åˆ†ææä¾›çš„åˆ†ç»„åˆ—è¡¨ï¼Œåˆ¤æ–­å“ªäº›åˆ†ç»„å±äºåŒä¸€ç³»åˆ—åº”è¯¥åˆå¹¶ã€‚

é‡è¦è§„åˆ™ - ç»å¯¹ä¸èƒ½è¿åï¼š
1. ç»å¯¹ä¸è¦å°†ç”µå½±ä¸ç”µè§†å‰§é›†åˆå¹¶ - å®ƒä»¬æ˜¯å®Œå…¨ä¸åŒçš„å†…å®¹ç±»å‹
2. ç»å¯¹ä¸è¦å°†å‰§åœºç‰ˆç”µå½±ä¸ç”µè§†å‰§åˆå¹¶ï¼Œå³ä½¿æ˜¯åŒä¸€IP
3. ç»å¯¹ä¸è¦åˆå¹¶ä¸åŒçš„ç³»åˆ—ï¼Œå³ä½¿çœ‹èµ·æ¥ç›¸ä¼¼
4. ç»å¯¹ä¸è¦ä»…åŸºäºç±»å‹ç›¸ä¼¼æ€§åˆå¹¶ï¼ˆå¦‚æ‰€æœ‰ææ€–ç‰‡ã€æ‰€æœ‰å–œå‰§ï¼‰
5. **ç»å¯¹ä¸è¦å°†ä¸åŒå­£çš„ç”µè§†å‰§åˆå¹¶** - æ¯å­£åº”è¯¥ä¿æŒç‹¬ç«‹
6. åªæœ‰åœ¨æ–‡ä»¶çœŸæ­£å±äºåŒä¸€è¿ç»­ç³»åˆ—ä¸”å†…å®¹ç±»å‹ç›¸åŒæ—¶æ‰åˆå¹¶

æœ‰æ•ˆåˆå¹¶æ ‡å‡†ï¼š
- åŒä¸€ç”µå½±ç³»åˆ—çš„ç»­é›†/å‰ä¼ ï¼ˆå¦‚æŒ‡ç¯ç‹1,2,3ï¼‰
- **åŒä¸€ç”µè§†å‰§çš„åŒä¸€å­£å†…çš„ä¸åŒé›†** - æ³¨æ„ï¼šä¸åŒå­£ç»å¯¹ä¸èƒ½åˆå¹¶
- åŒä¸€ç³»åˆ—çš„ä¸åŒç”µå½±ï¼ˆå¦‚å®å¯æ¢¦ç”µå½±ç³»åˆ—ï¼‰
- åŒä¸€ä½œå“çš„é‡åˆ¶ç‰ˆ/å¯¼æ¼”å‰ªè¾‘ç‰ˆ

**ç‰¹åˆ«å¼ºè°ƒ**ï¼š
- è€å‹è®° S01 å’Œ è€å‹è®° S02 æ˜¯ä¸åŒå­£ï¼Œç»å¯¹ä¸èƒ½åˆå¹¶
- æƒåŠ›çš„æ¸¸æˆ S01 å’Œ æƒåŠ›çš„æ¸¸æˆ S02 æ˜¯ä¸åŒå­£ï¼Œç»å¯¹ä¸èƒ½åˆå¹¶
- æ¯å­£åº”è¯¥ä¿æŒç‹¬ç«‹çš„åˆ†ç»„

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
å¿…é¡»è¿”å›æ ‡å‡†JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ã€‚

```json
{
  "merges": [
    {
      "merged_name": "åŒ…å«å¹´ä»½çš„ç³»åˆ—åç§°",
      "groups_to_merge": ["åˆ†ç»„1", "åˆ†ç»„2"],
      "reason": "åˆå¹¶åŸå› "
    }
  ]
}
```

**é‡è¦**ï¼š
1. åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰éœ€è¦åˆå¹¶çš„åˆ†ç»„ï¼Œè¿”å›ç©ºçš„mergesæ•°ç»„ã€‚
2. **merged_nameå¿…é¡»åŒ…å«å¹´ä»½ä¿¡æ¯**ï¼Œæ ¼å¼è¦æ±‚ï¼š
   - ç”µè§†å‰§åŒå­£å†…åˆå¹¶ï¼š"{æ ‡é¢˜} ({å¹´ä»½}) S{å­£æ•°}" ä¾‹å¦‚ï¼š"æƒåŠ›çš„æ¸¸æˆ (2011) S01"
   - ç”µå½±ç³»åˆ—ï¼š"{æ ‡é¢˜}ç³»åˆ— ({å¹´ä»½èŒƒå›´})" ä¾‹å¦‚ï¼š"å¤ä»‡è€…è”ç›Ÿç³»åˆ— (2012-2019)"
3. **ç»å¯¹ç¦æ­¢**ï¼šä¸è¦åˆ›å»ºè·¨å­£åˆå¹¶ï¼Œå¦‚"è€å‹è®° (1994) S01-S10"è¿™æ ·çš„æ ¼å¼

If no valid merges found, return: {"merges": []}"""


# ================================
# å…¨å±€é…ç½®å’Œå˜é‡å®šä¹‰
# ================================

# åº”ç”¨ç¨‹åºé…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯é…ç½®å‚æ•°çš„é»˜è®¤å€¼
app_config = {
    # æ€§èƒ½é…ç½® - ä¼˜åŒ–é¢„è§ˆåˆ®å‰Šæ€§èƒ½
    "QPS_LIMIT": 8,          # APIè¯·æ±‚é¢‘ç‡é™åˆ¶ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰- æé«˜åˆ°12ä»¥æ”¹å–„æ€§èƒ½
    "CHUNK_SIZE": 50,         # æ‰¹é‡å¤„ç†æ—¶çš„åˆ†å—å¤§å° - å‡å°‘åˆ°25ä»¥æé«˜å¹¶å‘åº¦
    "MAX_WORKERS": 6,         # æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° - å¢åŠ åˆ°6ä»¥æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›

    # 123äº‘ç›˜APIé…ç½®
    "CLIENT_ID": "",           # 123äº‘ç›˜å¼€æ”¾å¹³å°å®¢æˆ·ç«¯ID
    "CLIENT_SECRET": "",       # 123äº‘ç›˜å¼€æ”¾å¹³å°å®¢æˆ·ç«¯å¯†é’¥

    # ç¬¬ä¸‰æ–¹APIé…ç½®
    "TMDB_API_KEY": "",        # The Movie Database APIå¯†é’¥
    "AI_API_KEY": "",          # AI APIå¯†é’¥ï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰
    "AI_API_URL": "",          # AI APIæœåŠ¡åœ°å€ï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰

    # AIæ¨¡å‹é…ç½®
    "MODEL": "",               # é»˜è®¤AIæ¨¡å‹åç§°
    "GROUPING_MODEL": "",      # æ™ºèƒ½åˆ†ç»„ä¸“ç”¨AIæ¨¡å‹åç§°

    # æœ¬åœ°åŒ–é…ç½®
    "LANGUAGE": "zh-CN",       # ç•Œé¢è¯­è¨€è®¾ç½®

    # é‡è¯•å’Œè¶…æ—¶é…ç½®
    "API_MAX_RETRIES": 3,      # APIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
    "API_RETRY_DELAY": 2,      # APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    "AI_API_TIMEOUT": 60,      # AI APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "AI_MAX_RETRIES": 3,       # AIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
    "AI_RETRY_DELAY": 2,       # AIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    "TMDB_API_TIMEOUT": 60,    # TMDB APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "TMDB_MAX_RETRIES": 3,     # TMDB APIæœ€å¤§é‡è¯•æ¬¡æ•°
    "TMDB_RETRY_DELAY": 2,     # TMDB APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    "CLOUD_API_MAX_RETRIES": 3, # 123äº‘ç›˜APIæœ€å¤§é‡è¯•æ¬¡æ•°
    "CLOUD_API_RETRY_DELAY": 2, # 123äº‘ç›˜APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    "GROUPING_MAX_RETRIES": 3, # æ™ºèƒ½åˆ†ç»„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰
    "GROUPING_RETRY_DELAY": 2, # æ™ºèƒ½åˆ†ç»„é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    "TASK_QUEUE_GET_TIMEOUT": 1.0, # ä»»åŠ¡é˜Ÿåˆ—è·å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    # è´¨é‡è¯„ä¼°é…ç½®
    "ENABLE_QUALITY_ASSESSMENT": False,  # æ™ºèƒ½åˆ†ç»„æ˜¯å¦å¯ç”¨è´¨é‡è¯„ä¼°ï¼ˆç¦ç”¨å¯æé«˜æ€§èƒ½ï¼‰
    "ENABLE_SCRAPING_QUALITY_ASSESSMENT": True,  # åˆ®å‰ŠåŠŸèƒ½æ˜¯å¦å¯ç”¨è´¨é‡è¯„ä¼°ï¼ˆå»ºè®®å¼€å¯ï¼‰

    # ç«¯å£ç®¡ç†é…ç½®
    "KILL_OCCUPIED_PORT_PROCESS": True  # æ˜¯å¦è‡ªåŠ¨ç»“æŸå ç”¨ç«¯å£çš„è¿›ç¨‹ï¼ˆå¯ç”¨å¯é¿å…ç«¯å£å†²çªï¼‰
}

# ================================
# å…¨å±€å˜é‡å£°æ˜
# ================================

# æ—¥å¿—é˜Ÿåˆ—ï¼Œç”¨äºWebç•Œé¢å®æ—¶æ˜¾ç¤ºæ—¥å¿—
log_queue = deque(maxlen=5000)

# æ€§èƒ½é…ç½®å…¨å±€å˜é‡
QPS_LIMIT = app_config["QPS_LIMIT"]
CHUNK_SIZE = app_config["CHUNK_SIZE"]
MAX_WORKERS = app_config["MAX_WORKERS"]

# 123äº‘ç›˜APIé…ç½®å…¨å±€å˜é‡
CLIENT_ID = app_config["CLIENT_ID"]
CLIENT_SECRET = app_config["CLIENT_SECRET"]

# ç¬¬ä¸‰æ–¹APIé…ç½®å…¨å±€å˜é‡
TMDB_API_KEY = ""
AI_API_KEY = ""
AI_API_URL = ""

# AIæ¨¡å‹é…ç½®å…¨å±€å˜é‡
MODEL = app_config["MODEL"]
GROUPING_MODEL = app_config["GROUPING_MODEL"]

# æœ¬åœ°åŒ–é…ç½®å…¨å±€å˜é‡
LANGUAGE = app_config["LANGUAGE"]

# è´¨é‡è¯„ä¼°é…ç½®å…¨å±€å˜é‡
ENABLE_QUALITY_ASSESSMENT = app_config["ENABLE_QUALITY_ASSESSMENT"]  # æ™ºèƒ½åˆ†ç»„è´¨é‡è¯„ä¼°
ENABLE_SCRAPING_QUALITY_ASSESSMENT = app_config["ENABLE_SCRAPING_QUALITY_ASSESSMENT"]  # åˆ®å‰Šè´¨é‡è¯„ä¼°

# 123äº‘ç›˜APIåŸºç¡€URL
BASE_API_URL = "https://open-api.123pan.com"

# APIè¯·æ±‚å¤´æ¨¡æ¿
API_HEADERS = {
    "Content-Type": "application/json",
    "Platform": "open_platform",
    "Authorization": ""
}

# æ”¯æŒçš„åª’ä½“æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
SUPPORTED_MEDIA_TYPES = [
    "wma", "wav", "mp3", "aac", "ra", "ram", "mp2", "ogg", "aif",
    "mpega", "amr", "mid", "midi", "m4a", "m4v", "wmv", "rmvb",
    "mpeg4", "mpeg2", "flv", "avi", "3gp", "mpga", "qt", "rm",
    "wmz", "wmd", "wvx", "wmx", "wm", "swf", "mpg", "mp4", "mkv",
    "mpeg", "mov", "mdf", "asf", "webm", "ogv", "m2ts", "ts", "vob",
    "divx", "xvid", "f4v", "m2v", "amv", "drc", "evo", "flc",
    "h264", "h265", "icod", "mod", "tod", "mts", "ogm", "qtff",
    "rec", "vp6", "vp7", "vp8", "vp9", "iso"
]



# è®¿é—®ä»¤ç‰Œï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼‰
access_token = None
access_token_expires_at = None  # è®¿é—®ä»¤ç‰Œè¿‡æœŸæ—¶é—´

# æ—¥å¿—å¤„ç†å™¨å…¨å±€å˜é‡
root_logger = None
file_handler = None
console_handler = None
queue_handler = None

# QPSé™åˆ¶å™¨å…¨å±€å˜é‡ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ï¼‰
qps_limiter = None
v2_list_limiter = None
rename_limiter = None
move_limiter = None
delete_limiter = None

# ä»»åŠ¡å–æ¶ˆæ§åˆ¶å…¨å±€å˜é‡
current_task_cancelled = False
current_task_id = None

# ================================
# ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–ï¼ˆä½¿ç”¨LRUç¼“å­˜ï¼‰
# ================================

# è·¯å¾„ç¼“å­˜ï¼ˆå°å®¹é‡ï¼Œä¸­æœŸæœ‰æ•ˆï¼‰
folder_path_cache = LRUCache(max_size=500, ttl=1800)  # 30åˆ†é’Ÿ
folder_path_cache._cache_name = 'folder_path_cache'

# æ™ºèƒ½åˆ†ç»„ç¼“å­˜ï¼ˆä¸­ç­‰å®¹é‡ï¼ŒçŸ­æœŸæœ‰æ•ˆï¼‰
grouping_cache = LRUCache(max_size=200, ttl=300)  # 5åˆ†é’Ÿ
grouping_cache._cache_name = 'grouping_cache'

# æ–‡ä»¶åˆ®å‰Šç»“æœç¼“å­˜ï¼ˆå¤§å®¹é‡ï¼ŒçŸ­æœŸæœ‰æ•ˆï¼‰
scraping_cache = LRUCache(max_size=1000, ttl=600)  # 10åˆ†é’Ÿ
scraping_cache._cache_name = 'scraping_cache'

# ç›®å½•å†…å®¹ç¼“å­˜ï¼ˆä¸­ç­‰å®¹é‡ï¼ŒæçŸ­æœŸæœ‰æ•ˆï¼‰
folder_content_cache = LRUCache(max_size=300, ttl=180)  # 3åˆ†é’Ÿ
folder_content_cache._cache_name = 'folder_content_cache'

# ä¿ç•™åŸæœ‰çš„å¸¸é‡å®šä¹‰ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼ˆè°ƒæ•´ä¸ºæ›´çŸ­çš„ç¼“å­˜æ—¶é—´ï¼‰
GROUPING_CACHE_DURATION = 300  # 5åˆ†é’Ÿ
SCRAPING_CACHE_DURATION = 600  # 10åˆ†é’Ÿ
FOLDER_CONTENT_CACHE_DURATION = 180  # 3åˆ†é’Ÿ


def cleanup_all_caches():
    """
    æ¸…ç†æ‰€æœ‰ç¼“å­˜ä¸­çš„è¿‡æœŸæ¡ç›®

    Returns:
        dict: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    stats = {}

    try:
        stats['folder_path_cache'] = folder_path_cache.cleanup_expired()
        stats['grouping_cache'] = grouping_cache.cleanup_expired()
        stats['scraping_cache'] = scraping_cache.cleanup_expired()
        stats['folder_content_cache'] = folder_content_cache.cleanup_expired()

        total_cleaned = sum(stats.values())
        if total_cleaned > 0:
            logging.info(f"ğŸ§¹ æ¸…ç†äº† {total_cleaned} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®: {stats}")

        return stats
    except Exception as e:
        logging.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return {}


def clear_operation_related_caches(folder_id=None, operation_type="unknown"):
    """
    æ¸…ç†ä¸æ“ä½œç›¸å…³çš„ç¼“å­˜

    Args:
        folder_id: æ“ä½œçš„æ–‡ä»¶å¤¹ID
        operation_type: æ“ä½œç±»å‹ï¼ˆscraping, renaming, groupingç­‰ï¼‰
    """
    try:
        cleared_count = 0

        if operation_type in ["scraping", "renaming"]:
            # åˆ®å‰Šå’Œé‡å‘½åæ“ä½œéœ€è¦æ¸…ç†åˆ®å‰Šç¼“å­˜
            old_size = scraping_cache.size()
            scraping_cache.clear()
            cleared_count += old_size
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ®å‰Šç¼“å­˜: {old_size} é¡¹")

        if operation_type in ["renaming", "grouping"]:
            # é‡å‘½åå’Œåˆ†ç»„æ“ä½œéœ€è¦æ¸…ç†åˆ†ç»„ç¼“å­˜
            old_size = grouping_cache.size()
            grouping_cache.clear()
            cleared_count += old_size
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ†ç»„ç¼“å­˜: {old_size} é¡¹")

        if folder_id:
            # æ¸…ç†ç‰¹å®šæ–‡ä»¶å¤¹çš„å†…å®¹ç¼“å­˜
            folder_content_cache.delete(f"folder_{folder_id}")
            cleared_count += 1
            logging.info(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶å¤¹ {folder_id} çš„å†…å®¹ç¼“å­˜")

        if operation_type == "major_change":
            # é‡å¤§å˜æ›´æ—¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
            stats = cleanup_all_caches()
            cleared_count += sum(stats.values())

        if cleared_count > 0:
            logging.info(f"ğŸ§¹ æ“ä½œ {operation_type} è§¦å‘ç¼“å­˜æ¸…ç†ï¼Œå…±æ¸…ç† {cleared_count} é¡¹")

        return cleared_count
    except Exception as e:
        logging.error(f"âŒ æ“ä½œç›¸å…³ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return 0


def start_cache_cleanup_task():
    """å¯åŠ¨ç¼“å­˜æ¸…ç†åå°ä»»åŠ¡"""
    def cache_cleanup_worker():
        while True:
            try:
                time.sleep(180)  # æ¯3åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
                cleanup_all_caches()
            except Exception as e:
                logging.error(f"âŒ ç¼“å­˜æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

    cleanup_thread = threading.Thread(target=cache_cleanup_worker, daemon=True)
    cleanup_thread.start()
    logging.info("ğŸ§¹ ç¼“å­˜æ¸…ç†åå°ä»»åŠ¡å·²å¯åŠ¨ï¼ˆæ¯3åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡ï¼‰")

# ğŸš¦ è¯·æ±‚é™æµæ§åˆ¶å…¨å±€å˜é‡ï¼ˆå·²ä¼˜åŒ–ä¸ºä¸ä»»åŠ¡é˜Ÿåˆ—é…åˆï¼‰
folder_request_tracker = {}
FOLDER_REQUEST_LIMIT_DURATION = 30  # é™æµæ—¶é—´çª—å£ï¼š30ç§’ï¼ˆå‡å°‘ï¼Œå› ä¸ºæœ‰ä»»åŠ¡é˜Ÿåˆ—ä¿æŠ¤ï¼‰
MAX_REQUESTS_PER_FOLDER = 1  # æ¯ä¸ªæ–‡ä»¶å¤¹åœ¨æ—¶é—´çª—å£å†…çš„æœ€å¤§è¯·æ±‚æ•°

# ğŸš€ ä»»åŠ¡é˜Ÿåˆ—é…ç½®
TASK_QUEUE_MAX_SIZE = 10  # æœ€å¤§é˜Ÿåˆ—å¤§å°
TASK_TIMEOUT_SECONDS = 300  # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰

# ================================
# é‡è¯•å’Œè¶…æ—¶é…ç½®å…¨å±€å˜é‡ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
# ================================

# APIé‡è¯•é…ç½®
API_MAX_RETRIES = 3  # APIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
API_RETRY_DELAY = 2  # APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# AI APIé…ç½®
AI_API_TIMEOUT = 60  # AI APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
AI_MAX_RETRIES = 3  # AIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
AI_RETRY_DELAY = 2  # AIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# TMDB APIé…ç½®
TMDB_API_TIMEOUT = 60  # TMDB APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TMDB_MAX_RETRIES = 3  # TMDB APIæœ€å¤§é‡è¯•æ¬¡æ•°
TMDB_RETRY_DELAY = 2  # TMDB APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# 123äº‘ç›˜APIé…ç½®
CLOUD_API_MAX_RETRIES = 3  # 123äº‘ç›˜APIæœ€å¤§é‡è¯•æ¬¡æ•°
CLOUD_API_RETRY_DELAY = 2  # 123äº‘ç›˜APIé‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# æ™ºèƒ½åˆ†ç»„é‡è¯•é…ç½®
GROUPING_MAX_RETRIES = 3  # æ™ºèƒ½åˆ†ç»„æœ€å¤§é‡è¯•æ¬¡æ•°
GROUPING_RETRY_DELAY = 2  # æ™ºèƒ½åˆ†ç»„é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# ä»»åŠ¡é˜Ÿåˆ—è¶…æ—¶é…ç½®
TASK_QUEUE_GET_TIMEOUT = 1.0  # ä»»åŠ¡é˜Ÿåˆ—è·å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# ================================
# å…¨å±€ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿ
# ================================

import queue
import threading
from enum import Enum
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
import uuid

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"      # ç­‰å¾…ä¸­
    RUNNING = "running"      # æ‰§è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"        # å¤±è´¥
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ
    TIMEOUT = "timeout"      # è¶…æ—¶

@dataclass
class GroupingTask:
    """æ™ºèƒ½åˆ†ç»„ä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    folder_id: str
    folder_name: str
    status: TaskStatus = TaskStatus.PENDING
    created_at: float = field(default_factory=time.time)
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    progress: float = 0.0  # è¿›åº¦ç™¾åˆ†æ¯” 0-100

    def get_duration(self) -> Optional[float]:
        """è·å–ä»»åŠ¡æ‰§è¡Œæ—¶é•¿"""
        if self.started_at and self.completed_at:
            return self.completed_at - self.started_at
        elif self.started_at:
            return time.time() - self.started_at
        return None

class GroupingTaskManager:
    """æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self, max_queue_size: int = 10, task_timeout: int = 300):
        self.task_queue = queue.Queue(maxsize=max_queue_size)
        self.active_tasks: Dict[str, GroupingTask] = {}
        self.completed_tasks: Dict[str, GroupingTask] = {}
        self.max_completed_tasks = 50  # æœ€å¤šä¿ç•™50ä¸ªå·²å®Œæˆä»»åŠ¡
        self.task_timeout = task_timeout  # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.lock = threading.RLock()
        self.worker_thread = None
        self.is_running = False
        self._start_worker()

    def _start_worker(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        if not self.is_running:
            self.is_running = True
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logging.info("ğŸš€ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨å·²å¯åŠ¨")

    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
                task = self.task_queue.get(timeout=TASK_QUEUE_GET_TIMEOUT)
                if task is None:  # åœæ­¢ä¿¡å·
                    break

                self._execute_task(task)
                self.task_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"âŒ ä»»åŠ¡ç®¡ç†å™¨å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")

    def _execute_task(self, task: GroupingTask):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        with self.lock:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = TaskStatus.RUNNING
            task.started_at = time.time()
            self.active_tasks[task.task_id] = task

        logging.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œæ™ºèƒ½åˆ†ç»„ä»»åŠ¡: {task.task_id} (æ–‡ä»¶å¤¹: {task.folder_name})")

        # åˆ›å»ºè¶…æ—¶æ£€æŸ¥çº¿ç¨‹
        timeout_thread = threading.Thread(target=self._check_task_timeout, args=(task,), daemon=True)
        timeout_thread.start()

        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
            if task.status == TaskStatus.CANCELLED:
                return

            # æ‰§è¡Œå®é™…çš„åˆ†ç»„ä»»åŠ¡
            result = self._perform_grouping_analysis(task)

            with self.lock:
                if task.status not in [TaskStatus.CANCELLED, TaskStatus.TIMEOUT]:
                    task.status = TaskStatus.COMPLETED
                    task.completed_at = time.time()
                    task.result = result
                    task.progress = 100.0

                    # ç§»åŠ¨åˆ°å·²å®Œæˆä»»åŠ¡
                    self._move_to_completed(task)

                    logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å®Œæˆ: {task.task_id} (è€—æ—¶: {task.get_duration():.2f}ç§’)")

        except Exception as e:
            with self.lock:
                if task.status not in [TaskStatus.CANCELLED, TaskStatus.TIMEOUT]:
                    task.status = TaskStatus.FAILED
                    task.completed_at = time.time()
                    task.error = str(e)
                    self._move_to_completed(task)

                    logging.error(f"âŒ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")

    def _check_task_timeout(self, task: GroupingTask):
        """æ£€æŸ¥ä»»åŠ¡è¶…æ—¶"""
        time.sleep(self.task_timeout)

        with self.lock:
            if task.task_id in self.active_tasks and task.status == TaskStatus.RUNNING:
                # ä»»åŠ¡è¶…æ—¶
                task.status = TaskStatus.TIMEOUT
                task.completed_at = time.time()
                task.error = f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ (è¶…è¿‡ {self.task_timeout} ç§’)"
                self._move_to_completed(task)

                logging.warning(f"â° æ™ºèƒ½åˆ†ç»„ä»»åŠ¡è¶…æ—¶: {task.task_id} (è¶…æ—¶æ—¶é—´: {self.task_timeout}ç§’)")

    def _perform_grouping_analysis(self, task: GroupingTask) -> Dict[str, Any]:
        """æ‰§è¡Œå®é™…çš„åˆ†ç»„åˆ†æ"""
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        # è®¾ç½®å½“å‰ä»»åŠ¡IDï¼Œä»¥ä¾¿å…¨å±€å–æ¶ˆæœºåˆ¶èƒ½å¤Ÿå·¥ä½œ
        global current_task_id, current_task_cancelled
        current_task_id = task.task_id
        current_task_cancelled = False  # é‡ç½®å–æ¶ˆæ ‡å¿—

        # è¿™é‡Œè°ƒç”¨ç°æœ‰çš„åˆ†ç»„åˆ†æå‡½æ•°
        video_files = []
        get_video_files_recursively(task.folder_id, video_files)

        # å†æ¬¡æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        if not video_files:
            return {
                'success': False,
                'error': 'æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶',
                'movie_info': [],
                'count': 0,
                'size': '0GB'
            }

        # æ›´æ–°è¿›åº¦
        task.progress = 30.0

        # è°ƒç”¨åˆ†ç»„åˆ†æ
        def progress_callback(message):
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            if task.status == TaskStatus.CANCELLED:
                raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

            # è¯¦ç»†çš„è¿›åº¦æ›´æ–°é€»è¾‘
            if 'å¼€å§‹æ™ºèƒ½æ–‡ä»¶åˆ†ç»„åˆ†æ' in message:
                task.progress = 40.0
                logging.info(f"ğŸ¯ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'æŒ‰å­æ–‡ä»¶å¤¹åˆ†ç»„' in message:
                task.progress = 45.0
                logging.info(f"ğŸ“ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'å¤„ç†å­æ–‡ä»¶å¤¹' in message:
                task.progress = min(50.0 + (task.progress - 50.0) * 0.1, 85.0)
                logging.info(f"ğŸ”„ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'AIåˆ†ç»„è€—æ—¶' in message:
                task.progress = min(task.progress + 5.0, 85.0)
                logging.info(f"â±ï¸ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'åˆ†ç»„åˆ†æå®Œæˆ' in message:
                task.progress = 90.0
                logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        grouping_result = get_folder_grouping_analysis_internal(video_files, task.folder_id, progress_callback)

        # æœ€åæ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        return {
            'success': True,
            'movie_info': grouping_result.get('movie_info', []),
            'video_files': video_files,
            'count': len(video_files),
            'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB"
        }

    def _move_to_completed(self, task: GroupingTask):
        """å°†ä»»åŠ¡ç§»åŠ¨åˆ°å·²å®Œæˆåˆ—è¡¨"""
        if task.task_id in self.active_tasks:
            del self.active_tasks[task.task_id]

        self.completed_tasks[task.task_id] = task

        # æ¸…ç†è¿‡å¤šçš„å·²å®Œæˆä»»åŠ¡
        if len(self.completed_tasks) > self.max_completed_tasks:
            # åˆ é™¤æœ€æ—§çš„ä»»åŠ¡
            oldest_task_id = min(self.completed_tasks.keys(),
                                key=lambda tid: self.completed_tasks[tid].completed_at or 0)
            del self.completed_tasks[oldest_task_id]

    def submit_task(self, folder_id: str, folder_name: str) -> str:
        """æäº¤æ–°çš„åˆ†ç»„ä»»åŠ¡"""
        task_id = str(uuid.uuid4())
        task = GroupingTask(
            task_id=task_id,
            folder_id=str(folder_id),
            folder_name=folder_name
        )

        try:
            with self.lock:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶å¤¹çš„ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­æˆ–æ‰§è¡Œä¸­
                for existing_task in list(self.active_tasks.values()):
                    if existing_task.folder_id == str(folder_id) and existing_task.status in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                        raise ValueError(f"æ–‡ä»¶å¤¹ {folder_name} å·²æœ‰åˆ†ç»„ä»»åŠ¡åœ¨è¿›è¡Œä¸­")

                # æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰ç›¸åŒæ–‡ä»¶å¤¹çš„ä»»åŠ¡
                temp_queue = []
                while not self.task_queue.empty():
                    try:
                        queued_task = self.task_queue.get_nowait()
                        if queued_task.folder_id == str(folder_id):
                            raise ValueError(f"æ–‡ä»¶å¤¹ {folder_name} å·²æœ‰åˆ†ç»„ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­")
                        temp_queue.append(queued_task)
                    except queue.Empty:
                        break

                # é‡æ–°æ”¾å›é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
                for queued_task in temp_queue:
                    self.task_queue.put_nowait(queued_task)

                # æ·»åŠ æ–°ä»»åŠ¡åˆ°é˜Ÿåˆ—å’Œæ´»åŠ¨ä»»åŠ¡åˆ—è¡¨
                self.task_queue.put_nowait(task)
                self.active_tasks[task_id] = task  # ç«‹å³æ·»åŠ åˆ°æ´»åŠ¨ä»»åŠ¡åˆ—è¡¨
                logging.info(f"ğŸ“ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤: {task_id} (æ–‡ä»¶å¤¹: {folder_name})")

                return task_id

        except queue.Full:
            raise ValueError("ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åå†è¯•")

    def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        with self.lock:
            if task_id in self.active_tasks:
                task = self.active_tasks[task_id]
                if task.status in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                    task.status = TaskStatus.CANCELLED
                    task.completed_at = time.time()
                    self._move_to_completed(task)
                    logging.info(f"ğŸ›‘ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
                    return True
            return False

    def get_task_status(self, task_id: str) -> Optional[GroupingTask]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        with self.lock:
            if task_id in self.active_tasks:
                return self.active_tasks[task_id]
            elif task_id in self.completed_tasks:
                return self.completed_tasks[task_id]
            return None

    def get_queue_info(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—ä¿¡æ¯"""
        with self.lock:
            return {
                'queue_size': self.task_queue.qsize(),
                'active_tasks': len(self.active_tasks),
                'completed_tasks': len(self.completed_tasks),
                'is_running': self.is_running
            }

    def cleanup_old_tasks(self, max_age_hours: int = 24):
        """æ¸…ç†æ—§ä»»åŠ¡"""
        cutoff_time = time.time() - (max_age_hours * 3600)
        with self.lock:
            to_remove = []
            for task_id, task in self.completed_tasks.items():
                if (task.completed_at or task.created_at) < cutoff_time:
                    to_remove.append(task_id)

            for task_id in to_remove:
                del self.completed_tasks[task_id]

            if to_remove:
                logging.info(f"ğŸ§¹ æ¸…ç†äº† {len(to_remove)} ä¸ªæ—§çš„åˆ†ç»„ä»»åŠ¡")

    def get_health_status(self) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡ç®¡ç†å™¨å¥åº·çŠ¶æ€"""
        with self.lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
            long_running_tasks = []
            current_time = time.time()

            for task in self.active_tasks.values():
                if task.started_at and (current_time - task.started_at) > (self.task_timeout * 0.8):
                    long_running_tasks.append({
                        'task_id': task.task_id,
                        'folder_name': task.folder_name,
                        'running_time': current_time - task.started_at
                    })

            return {
                'is_healthy': len(long_running_tasks) == 0 and self.is_running,
                'worker_running': self.is_running,
                'queue_size': self.task_queue.qsize(),
                'active_tasks_count': len(self.active_tasks),
                'completed_tasks_count': len(self.completed_tasks),
                'long_running_tasks': long_running_tasks,
                'max_queue_size': self.task_queue.maxsize,
                'task_timeout': self.task_timeout
            }

    def restart_worker_if_needed(self):
        """å¦‚æœå·¥ä½œçº¿ç¨‹åœæ­¢ï¼Œé‡æ–°å¯åŠ¨å®ƒ"""
        if not self.is_running or not self.worker_thread or not self.worker_thread.is_alive():
            logging.warning("ğŸ”„ æ£€æµ‹åˆ°å·¥ä½œçº¿ç¨‹åœæ­¢ï¼Œæ­£åœ¨é‡æ–°å¯åŠ¨...")
            self.is_running = False
            self._start_worker()

    def force_cleanup_stuck_tasks(self):
        """å¼ºåˆ¶æ¸…ç†å¡ä½çš„ä»»åŠ¡"""
        current_time = time.time()
        stuck_tasks = []

        with self.lock:
            for task_id, task in list(self.active_tasks.items()):
                if task.started_at and (current_time - task.started_at) > (self.task_timeout * 1.5):
                    # ä»»åŠ¡è¿è¡Œæ—¶é—´è¶…è¿‡è¶…æ—¶æ—¶é—´çš„1.5å€ï¼Œè®¤ä¸ºæ˜¯å¡ä½äº†
                    task.status = TaskStatus.TIMEOUT
                    task.completed_at = current_time
                    task.error = f"ä»»åŠ¡è¢«å¼ºåˆ¶æ¸…ç† (è¿è¡Œæ—¶é—´: {current_time - task.started_at:.1f}ç§’)"
                    self._move_to_completed(task)
                    stuck_tasks.append(task_id)

        if stuck_tasks:
            logging.warning(f"ğŸ§¹ å¼ºåˆ¶æ¸…ç†äº† {len(stuck_tasks)} ä¸ªå¡ä½çš„ä»»åŠ¡: {stuck_tasks}")

        return len(stuck_tasks)

# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
try:
    grouping_task_manager = GroupingTaskManager(
        max_queue_size=TASK_QUEUE_MAX_SIZE,
        task_timeout=TASK_TIMEOUT_SECONDS
    )
    logging.info("âœ… æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨å·²æˆåŠŸåˆå§‹åŒ–")
except Exception as e:
    logging.error(f"âŒ ä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    grouping_task_manager = None

# ================================
# ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤åŠŸèƒ½
# ================================

def start_task_manager_maintenance():
    """å¯åŠ¨ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤çº¿ç¨‹"""
    def maintenance_worker():
        while True:
            try:
                # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ç»´æŠ¤
                time.sleep(300)

                # æ£€æŸ¥å¹¶é‡å¯å·¥ä½œçº¿ç¨‹
                grouping_task_manager.restart_worker_if_needed()

                # æ¸…ç†æ—§ä»»åŠ¡ï¼ˆæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰
                if int(time.time()) % 3600 < 300:  # åœ¨æ¯å°æ—¶çš„å‰5åˆ†é’Ÿå†…æ‰§è¡Œ
                    grouping_task_manager.cleanup_old_tasks(24)

                # å¼ºåˆ¶æ¸…ç†å¡ä½çš„ä»»åŠ¡
                stuck_count = grouping_task_manager.force_cleanup_stuck_tasks()
                if stuck_count > 0:
                    logging.warning(f"ğŸ§¹ ç»´æŠ¤ä»»åŠ¡æ¸…ç†äº† {stuck_count} ä¸ªå¡ä½çš„ä»»åŠ¡")

                # è®°å½•å¥åº·çŠ¶æ€
                health = grouping_task_manager.get_health_status()
                if not health['is_healthy']:
                    logging.warning(f"âš ï¸ ä»»åŠ¡ç®¡ç†å™¨å¥åº·çŠ¶æ€å¼‚å¸¸: {health}")

            except Exception as e:
                logging.error(f"âŒ ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤å¤±è´¥: {e}")

    # å¯åŠ¨ç»´æŠ¤çº¿ç¨‹
    maintenance_thread = threading.Thread(target=maintenance_worker, daemon=True)
    maintenance_thread.start()
    logging.info("ğŸ”§ ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤çº¿ç¨‹å·²å¯åŠ¨")

# å¯åŠ¨ç»´æŠ¤çº¿ç¨‹
start_task_manager_maintenance()

# ================================
# ä»»åŠ¡ç®¡ç†APIç«¯ç‚¹
# ================================

@app.route('/api/grouping_task/submit', methods=['POST'])
def submit_grouping_task():
    """æäº¤æ™ºèƒ½åˆ†ç»„ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
    try:
        # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨æ˜¯å¦å¯ç”¨
        if grouping_task_manager is None:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€'})

        folder_id_str = request.form.get('folder_id', '0')
        folder_name = request.form.get('folder_name', 'æœªçŸ¥æ–‡ä»¶å¤¹')

        # éªŒè¯æ–‡ä»¶å¤¹ID
        folder_id, error_msg = validate_folder_id(folder_id_str)
        if error_msg:
            return jsonify({'success': False, 'error': error_msg})

        # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
        task_id = grouping_task_manager.submit_task(folder_id, folder_name)

        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': f'æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤åˆ°é˜Ÿåˆ— (ä»»åŠ¡ID: {task_id})'
        })

    except ValueError as e:
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.error(f"æäº¤æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'æäº¤ä»»åŠ¡å¤±è´¥: {str(e)}'})

@app.route('/api/grouping_task/status/<task_id>', methods=['GET'])
def get_grouping_task_status(task_id):
    """è·å–æ™ºèƒ½åˆ†ç»„ä»»åŠ¡çŠ¶æ€"""
    try:
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logging.info(f"ğŸ” å‰ç«¯è½®è¯¢ä»»åŠ¡çŠ¶æ€: {task_id}")

        task = grouping_task_manager.get_task_status(task_id)
        if not task:
            logging.warning(f"âš ï¸ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'})

        logging.info(f"ğŸ“Š è¿”å›ä»»åŠ¡çŠ¶æ€: {task.status.value}, è¿›åº¦: {task.progress}%")
        return jsonify({
            'success': True,
            'task': {
                'task_id': task.task_id,
                'folder_id': task.folder_id,
                'folder_name': task.folder_name,
                'status': task.status.value,
                'progress': task.progress,
                'created_at': task.created_at,
                'started_at': task.started_at,
                'completed_at': task.completed_at,
                'duration': task.get_duration(),
                'result': task.result,
                'error': task.error
            }
        })

    except Exception as e:
        logging.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}'})

@app.route('/api/grouping_task/queue_info', methods=['GET'])
def get_grouping_queue_info():
    """è·å–æ™ºèƒ½åˆ†ç»„ä»»åŠ¡é˜Ÿåˆ—ä¿¡æ¯"""
    try:
        queue_info = grouping_task_manager.get_queue_info()

        # è·å–å½“å‰æ´»è·ƒä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
        active_tasks = []
        with grouping_task_manager.lock:
            for task in grouping_task_manager.active_tasks.values():
                active_tasks.append({
                    'task_id': task.task_id,
                    'folder_name': task.folder_name,
                    'status': task.status.value,
                    'progress': task.progress,
                    'duration': task.get_duration()
                })

        return jsonify({
            'success': True,
            'queue_info': queue_info,
            'active_tasks': active_tasks
        })

    except Exception as e:
        logging.error(f"è·å–é˜Ÿåˆ—ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–é˜Ÿåˆ—ä¿¡æ¯å¤±è´¥: {str(e)}'})

@app.route('/api/grouping_task/cancel/<task_id>', methods=['POST'])
def cancel_grouping_task(task_id):
    """å–æ¶ˆæ™ºèƒ½åˆ†ç»„ä»»åŠ¡"""
    try:
        success = grouping_task_manager.cancel_task(task_id)
        if success:
            return jsonify({'success': True, 'message': f'ä»»åŠ¡ {task_id} å·²å–æ¶ˆ'})
        else:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ'})

    except Exception as e:
        logging.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}'})

@app.route('/api/grouping_task/health', methods=['GET'])
def get_grouping_task_health():
    """è·å–ä»»åŠ¡ç®¡ç†å™¨å¥åº·çŠ¶æ€"""
    try:
        health_status = grouping_task_manager.get_health_status()
        return jsonify({
            'success': True,
            'health': health_status
        })
    except Exception as e:
        logging.error(f"è·å–å¥åº·çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}'})

@app.route('/api/grouping_task/maintenance', methods=['POST'])
def perform_grouping_task_maintenance():
    """æ‰§è¡Œä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤æ“ä½œ"""
    try:
        action = request.form.get('action', '')

        if action == 'cleanup_old_tasks':
            max_age_hours = int(request.form.get('max_age_hours', 24))
            grouping_task_manager.cleanup_old_tasks(max_age_hours)
            return jsonify({'success': True, 'message': f'å·²æ¸…ç†è¶…è¿‡ {max_age_hours} å°æ—¶çš„æ—§ä»»åŠ¡'})

        elif action == 'restart_worker':
            grouping_task_manager.restart_worker_if_needed()
            return jsonify({'success': True, 'message': 'å·¥ä½œçº¿ç¨‹å·²é‡å¯'})

        elif action == 'force_cleanup_stuck':
            cleaned_count = grouping_task_manager.force_cleanup_stuck_tasks()
            return jsonify({'success': True, 'message': f'å¼ºåˆ¶æ¸…ç†äº† {cleaned_count} ä¸ªå¡ä½çš„ä»»åŠ¡'})

        else:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„ç»´æŠ¤æ“ä½œ'})

    except Exception as e:
        logging.error(f"æ‰§è¡Œç»´æŠ¤æ“ä½œå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'æ‰§è¡Œç»´æŠ¤æ“ä½œå¤±è´¥: {str(e)}'})

def check_task_cancelled():
    """æ£€æŸ¥å½“å‰ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ"""
    # ä½¿ç”¨app_stateæ£€æŸ¥å–æ¶ˆçŠ¶æ€
    app_state.check_task_cancelled()

    # æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—ä¸­çš„å–æ¶ˆçŠ¶æ€
    if app_state.current_task_id and grouping_task_manager:
        task = grouping_task_manager.get_task_status(app_state.current_task_id)
        if task and task.status == TaskStatus.CANCELLED:
            logging.info(f"ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ (ä»»åŠ¡é˜Ÿåˆ—): {app_state.current_task_id}")
            raise TaskCancelledException("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")

def cancel_current_task():
    """å–æ¶ˆå½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    app_state.cancel_task()

    # åŒæ—¶å–æ¶ˆä»»åŠ¡é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
    if app_state.current_task_id:
        cancelled = grouping_task_manager.cancel_task(app_state.current_task_id)
        if cancelled:
            logging.info(f"ğŸ›‘ ç”¨æˆ·è¯·æ±‚å–æ¶ˆå½“å‰ä»»åŠ¡: {app_state.current_task_id} (ä»»åŠ¡é˜Ÿåˆ—)")
        else:
            logging.warning(f"âš ï¸ ä»»åŠ¡é˜Ÿåˆ—ä¸­æœªæ‰¾åˆ°ä»»åŠ¡: {app_state.current_task_id}")

    logging.info("ğŸ›‘ ç”¨æˆ·è¯·æ±‚å–æ¶ˆå½“å‰ä»»åŠ¡ (å…¨å±€æ ‡å¿—)")

def start_new_task(task_id=None):
    """å¼€å§‹æ–°ä»»åŠ¡"""
    task_id = task_id or str(int(time.time()))
    app_state.start_task(task_id)

    # æ¸…ç†è·¯å¾„ç¼“å­˜ï¼ˆé¿å…å†…å­˜æ³„æ¼ï¼‰
    if folder_path_cache.size() > 1000:  # ç¼“å­˜è¿‡å¤šæ—¶æ¸…ç†
        folder_path_cache.clear()
        logging.info("ğŸ§¹ æ¸…ç†è·¯å¾„ç¼“å­˜")

    # æ¸…ç†ç›®å½•å†…å®¹ç¼“å­˜ï¼ˆé¿å…å†…å­˜æ³„æ¼ï¼‰
    if folder_content_cache.size() > 500:  # ç›®å½•ç¼“å­˜è¿‡å¤šæ—¶æ¸…ç†
        folder_content_cache.clear()
        logging.info("ğŸ§¹ æ¸…ç†ç›®å½•å†…å®¹ç¼“å­˜")

    # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    cleanup_expired_folder_cache()

def reset_task_state():
    """é‡ç½®ä»»åŠ¡çŠ¶æ€ï¼ˆç”¨äºæ™®é€šæ“ä½œï¼‰"""
    app_state.current_task_id = None
    app_state.task_cancelled = False

# ================================
# å·¥å…·å‡½æ•°å’Œè¾…åŠ©æ–¹æ³•
# ================================

def validate_folder_id(folder_id):
    """
    éªŒè¯å¹¶è½¬æ¢æ–‡ä»¶å¤¹ID

    Args:
        folder_id: å¾…éªŒè¯çš„æ–‡ä»¶å¤¹IDï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰

    Returns:
        tuple: (è½¬æ¢åçš„æ•´æ•°ID, é”™è¯¯ä¿¡æ¯)
               å¦‚æœéªŒè¯æˆåŠŸï¼Œè¿”å› (int_id, None)
               å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å› (None, error_message)
    """
    if not folder_id or folder_id == 'null' or folder_id == 'undefined':
        return None, 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'

    try:
        return int(folder_id), None
    except (ValueError, TypeError):
        return None, 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'


def decode_jwt_token(token):
    """
    è§£æJWT tokenè·å–è¿‡æœŸæ—¶é—´

    Args:
        token (str): JWTè®¿é—®ä»¤ç‰Œ

    Returns:
        dict or None: åŒ…å«è¿‡æœŸæ—¶é—´ç­‰ä¿¡æ¯çš„å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
    """
    try:
        if not token:
            return None

        # JWT tokenç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼Œç”¨.åˆ†éš”ï¼šheader.payload.signature
        parts = token.split('.')
        if len(parts) != 3:
            logging.warning("âš ï¸ è®¿é—®ä»¤ç‰Œæ ¼å¼ä¸æ­£ç¡®ï¼Œä¸æ˜¯æœ‰æ•ˆçš„JWT token")
            return None

        # è§£æpayloadéƒ¨åˆ†ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰
        payload = parts[1]

        # JWTä½¿ç”¨base64urlç¼–ç ï¼Œéœ€è¦è¡¥é½padding
        missing_padding = len(payload) % 4
        if missing_padding:
            payload += '=' * (4 - missing_padding)

        # è§£ç payload
        decoded_bytes = base64.urlsafe_b64decode(payload)
        payload_data = json.loads(decoded_bytes.decode('utf-8'))

        return payload_data

    except Exception as e:
        logging.error(f"âŒ è§£æJWT tokenå¤±è´¥: {e}")
        return None


def is_access_token_expired(token=None):
    """
    æ£€æŸ¥è®¿é—®ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ

    Args:
        token (str, optional): è¦æ£€æŸ¥çš„è®¿é—®ä»¤ç‰Œï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å…¨å±€token

    Returns:
        bool: Trueè¡¨ç¤ºå·²è¿‡æœŸæˆ–æ— æ•ˆï¼ŒFalseè¡¨ç¤ºä»ç„¶æœ‰æ•ˆ
    """
    global access_token, access_token_expires_at

    check_token = token or access_token
    if not check_token:
        logging.warning("âš ï¸ æ²¡æœ‰è®¿é—®ä»¤ç‰Œå¯ä¾›æ£€æŸ¥")
        return True

    try:
        # è§£æJWT token
        payload = decode_jwt_token(check_token)
        if not payload:
            logging.warning("âš ï¸ æ— æ³•è§£æè®¿é—®ä»¤ç‰Œ")
            return True

        # è·å–è¿‡æœŸæ—¶é—´ï¼ˆexpå­—æ®µï¼ŒUnixæ—¶é—´æˆ³ï¼‰
        exp_timestamp = payload.get('exp')
        if not exp_timestamp:
            logging.warning("âš ï¸ è®¿é—®ä»¤ç‰Œä¸­æ²¡æœ‰è¿‡æœŸæ—¶é—´ä¿¡æ¯")
            return True

        # è½¬æ¢ä¸ºdatetimeå¯¹è±¡
        expires_at = datetime.datetime.fromtimestamp(exp_timestamp)
        current_time = datetime.datetime.now()

        # æ›´æ–°å…¨å±€è¿‡æœŸæ—¶é—´å˜é‡
        if token is None:  # åªæœ‰æ£€æŸ¥å…¨å±€tokenæ—¶æ‰æ›´æ–°
            access_token_expires_at = expires_at

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆæå‰5åˆ†é’Ÿåˆ¤æ–­ä¸ºè¿‡æœŸï¼Œç•™å‡ºåˆ·æ–°æ—¶é—´ï¼‰
        buffer_time = datetime.timedelta(minutes=5)
        is_expired = current_time >= (expires_at - buffer_time)

        if is_expired:
            logging.warning(f"âš ï¸ è®¿é—®ä»¤ç‰Œå·²è¿‡æœŸæˆ–å³å°†è¿‡æœŸã€‚è¿‡æœŸæ—¶é—´: {expires_at}, å½“å‰æ—¶é—´: {current_time}")
        else:
            # logging.info(f"âœ… è®¿é—®ä»¤ç‰Œæœ‰æ•ˆï¼Œå‰©ä½™æ—¶é—´: {expires_at - current_time}")
            pass

        return is_expired

    except Exception as e:
        logging.error(f"âŒ æ£€æŸ¥è®¿é—®ä»¤ç‰Œè¿‡æœŸæ—¶é—´å¤±è´¥: {e}")
        return True


def refresh_access_token_if_needed():
    """
    å¦‚æœè®¿é—®ä»¤ç‰Œè¿‡æœŸï¼Œè‡ªåŠ¨åˆ·æ–°è®¿é—®ä»¤ç‰Œ

    Returns:
        bool: Trueè¡¨ç¤ºä»¤ç‰Œæœ‰æ•ˆæˆ–åˆ·æ–°æˆåŠŸï¼ŒFalseè¡¨ç¤ºåˆ·æ–°å¤±è´¥
    """
    global access_token

    # æ£€æŸ¥å½“å‰ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ
    if not is_access_token_expired():
        return True  # ä»¤ç‰Œä»ç„¶æœ‰æ•ˆ

    logging.info("ğŸ”„ è®¿é—®ä»¤ç‰Œå·²è¿‡æœŸï¼Œå°è¯•åˆ·æ–°...")

    # å°è¯•è·å–æ–°çš„è®¿é—®ä»¤ç‰Œ
    if CLIENT_ID and CLIENT_SECRET:
        new_token = get_access_token_from_api(CLIENT_ID, CLIENT_SECRET)
        if new_token:
            # æ›´æ–°å…¨å±€å˜é‡
            access_token = new_token
            API_HEADERS["Authorization"] = f"Bearer {new_token}"

            # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆWindowså…¼å®¹æ€§ï¼šæŒ‡å®šUTF-8ç¼–ç ï¼‰
            try:
                with open("123_access_token.txt", "w", encoding='utf-8') as f:
                    f.write(new_token)
                logging.info("âœ… è®¿é—®ä»¤ç‰Œåˆ·æ–°æˆåŠŸå¹¶å·²ä¿å­˜")
                return True
            except Exception as e:
                logging.error(f"âŒ ä¿å­˜æ–°è®¿é—®ä»¤ç‰Œå¤±è´¥: {e}")
                return True  # ä»¤ç‰Œå·²æ›´æ–°ï¼Œåªæ˜¯ä¿å­˜å¤±è´¥
        else:
            logging.error("âŒ æ— æ³•è·å–æ–°çš„è®¿é—®ä»¤ç‰Œ")
            return False
    else:
        logging.error("âŒ ç¼ºå°‘CLIENT_IDæˆ–CLIENT_SECRETï¼Œæ— æ³•åˆ·æ–°è®¿é—®ä»¤ç‰Œ")
        return False


def ensure_valid_access_token(func):
    """
    è£…é¥°å™¨ï¼šç¡®ä¿APIè°ƒç”¨å‰è®¿é—®ä»¤ç‰Œæœ‰æ•ˆ

    åœ¨è°ƒç”¨éœ€è¦è®¿é—®ä»¤ç‰Œçš„APIå‡½æ•°å‰ï¼Œè‡ªåŠ¨æ£€æŸ¥å¹¶åˆ·æ–°è¿‡æœŸçš„ä»¤ç‰Œ
    å¦‚æœAPIè°ƒç”¨è¿”å›ä»¤ç‰Œè¶…é™é”™è¯¯ï¼Œä¼šè‡ªåŠ¨åˆ·æ–°ä»¤ç‰Œå¹¶é‡è¯•
    """
    def wrapper(*args, **kwargs):
        global access_token
        max_retries = API_MAX_RETRIES  # ä½¿ç”¨å…¨å±€é…ç½®

        for attempt in range(max_retries):
            try:
                # ç¬¬ä¸€æ¬¡è°ƒç”¨å‰æ£€æŸ¥å¹¶åˆ·æ–°è®¿é—®ä»¤ç‰Œ
                if attempt == 0:
                    if not refresh_access_token_if_needed():
                        logging.error("âŒ è®¿é—®ä»¤ç‰Œæ— æ•ˆä¸”æ— æ³•åˆ·æ–°ï¼ŒAPIè°ƒç”¨å¯èƒ½å¤±è´¥")

                # è°ƒç”¨åŸå‡½æ•°
                return func(*args, **kwargs)

            except TokenLimitExceededError as e:
                logging.warning(f"âš ï¸ è®¿é—®ä»¤ç‰Œä½¿ç”¨æ¬¡æ•°è¶…é™ (å°è¯• {attempt + 1}/{max_retries}): {e}")

                if attempt < max_retries - 1:
                    # å¼ºåˆ¶åˆ·æ–°è®¿é—®ä»¤ç‰Œ
                    logging.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°è®¿é—®ä»¤ç‰Œ...")
                    if CLIENT_ID and CLIENT_SECRET:
                        new_token = get_access_token_from_api(CLIENT_ID, CLIENT_SECRET)
                        if new_token:
                            access_token = new_token
                            API_HEADERS["Authorization"] = f"Bearer {new_token}"

                            # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆWindowså…¼å®¹æ€§ï¼šæŒ‡å®šUTF-8ç¼–ç ï¼‰
                            try:
                                with open("123_access_token.txt", "w", encoding='utf-8') as f:
                                    f.write(new_token)
                                logging.info("âœ… è®¿é—®ä»¤ç‰Œå¼ºåˆ¶åˆ·æ–°æˆåŠŸ")
                            except Exception as save_e:
                                logging.error(f"âŒ ä¿å­˜æ–°è®¿é—®ä»¤ç‰Œå¤±è´¥: {save_e}")
                        else:
                            logging.error("âŒ æ— æ³•è·å–æ–°çš„è®¿é—®ä»¤ç‰Œ")
                            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                    else:
                        logging.error("âŒ ç¼ºå°‘CLIENT_IDæˆ–CLIENT_SECRETï¼Œæ— æ³•åˆ·æ–°è®¿é—®ä»¤ç‰Œ")
                        raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise e

            except AccessTokenError as e:
                logging.warning(f"âš ï¸ è®¿é—®ä»¤ç‰Œè®¤è¯é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")

                if attempt < max_retries - 1:
                    # å¼ºåˆ¶åˆ·æ–°è®¿é—®ä»¤ç‰Œ
                    logging.info("ğŸ”„ æ£€æµ‹åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°è®¿é—®ä»¤ç‰Œ...")
                    if CLIENT_ID and CLIENT_SECRET:
                        new_token = get_access_token_from_api(CLIENT_ID, CLIENT_SECRET)
                        if new_token:
                            access_token = new_token
                            API_HEADERS["Authorization"] = f"Bearer {new_token}"

                            # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆWindowså…¼å®¹æ€§ï¼šæŒ‡å®šUTF-8ç¼–ç ï¼‰
                            try:
                                with open("123_access_token.txt", "w", encoding='utf-8') as f:
                                    f.write(new_token)
                                logging.info("âœ… è®¿é—®ä»¤ç‰Œå¼ºåˆ¶åˆ·æ–°æˆåŠŸï¼Œå°†é‡è¯•APIè°ƒç”¨")
                            except Exception as save_e:
                                logging.error(f"âŒ ä¿å­˜æ–°è®¿é—®ä»¤ç‰Œå¤±è´¥: {save_e}")
                        else:
                            logging.error("âŒ æ— æ³•è·å–æ–°çš„è®¿é—®ä»¤ç‰Œ")
                            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                    else:
                        logging.error("âŒ ç¼ºå°‘CLIENT_IDæˆ–CLIENT_SECRETï¼Œæ— æ³•åˆ·æ–°è®¿é—®ä»¤ç‰Œ")
                        raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    logging.error(f"âŒ è®¿é—®ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œå·²å°è¯• {max_retries} æ¬¡")
                    raise e

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè°ƒç”¨åŸå‡½æ•°ï¼ˆè®©å®ƒå¤„ç†é”™è¯¯ï¼‰
        return func(*args, **kwargs)

    wrapper.__name__ = func.__name__
    wrapper.__doc__ = func.__doc__
    return wrapper


def call_ai_api(prompt, model=None, temperature=0.1):
    """
    è°ƒç”¨AI APIè¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰

    Args:
        prompt (str): å‘é€ç»™AIçš„æç¤ºè¯
        model (str, optional): ä½¿ç”¨çš„AIæ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        temperature (float): ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œ0.0-1.0ä¹‹é—´

    Returns:
        str or None: AIç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # æ£€æŸ¥å¿…è¦çš„é…ç½®
        if not AI_API_KEY:
            logging.error("âŒ AI APIå¯†é’¥æœªé…ç½®")
            return None

        if not AI_API_URL:
            logging.error("âŒ AI APIæœåŠ¡åœ°å€æœªé…ç½®")
            return None

        if not model:
            logging.error("âŒ æ¨¡å‹åç§°æœªæŒ‡å®š")
            return None

        logging.info(f"ğŸŒ è°ƒç”¨AI API: {AI_API_URL}")
        logging.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
        logging.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

        headers = {
            "Authorization": f"Bearer {AI_API_KEY}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature
            # "max_tokens": max_tokens
        }

        # ä½¿ç”¨å…¨å±€é…ç½®çš„è¶…æ—¶æ—¶é—´
        response = requests.post(AI_API_URL, headers=headers, json=payload, timeout=AI_API_TIMEOUT)

        logging.info(f"ğŸ“Š APIå“åº”çŠ¶æ€ç : {response.status_code}")

        response.raise_for_status()
        data = response.json()

        # æ£€æŸ¥å“åº”æ ¼å¼
        if "choices" not in data:
            logging.error(f"âŒ APIå“åº”æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘choiceså­—æ®µ: {data}")
            return None

        if not data["choices"] or len(data["choices"]) == 0:
            logging.error(f"âŒ APIå“åº”choicesä¸ºç©º: {data}")
            return None

        if "message" not in data["choices"][0]:
            logging.error(f"âŒ APIå“åº”ç¼ºå°‘messageå­—æ®µ: {data['choices'][0]}")
            return None

        if "content" not in data["choices"][0]["message"]:
            logging.error(f"âŒ APIå“åº”ç¼ºå°‘contentå­—æ®µ: {data['choices'][0]['message']}")
            return None

        content = data["choices"][0]["message"]["content"]
        logging.info(f"âœ… AI APIè°ƒç”¨æˆåŠŸï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

        return content

    except requests.exceptions.Timeout as e:
        logging.error(f"âŒ AI APIè°ƒç”¨è¶…æ—¶: {e}")
        return None
    except requests.exceptions.ConnectionError as e:
        logging.error(f"âŒ AI APIè¿æ¥å¤±è´¥: {e}")
        return None
    except requests.exceptions.HTTPError as e:
        logging.error(f"âŒ AI API HTTPé”™è¯¯: {e}, å“åº”å†…å®¹: {e.response.text if e.response else 'N/A'}")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ AI APIè¯·æ±‚å¼‚å¸¸: {e}")
        return None
    except KeyError as e:
        logging.error(f"âŒ AI APIå“åº”è§£æå¤±è´¥ï¼Œç¼ºå°‘å­—æ®µ: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ AI APIè°ƒç”¨æœªçŸ¥é”™è¯¯: {e}")
        return None

def parse_json_from_ai_response(response_content):
    """
    ä»AIå“åº”ä¸­è§£æJSONæ•°æ®

    AIçš„å“åº”å¯èƒ½åŒ…å«é¢å¤–çš„æ–‡æœ¬ï¼Œæ­¤å‡½æ•°å°è¯•æå–å…¶ä¸­çš„JSONéƒ¨åˆ†

    Args:
        response_content (str): AIè¿”å›çš„åŸå§‹å“åº”å†…å®¹

    Returns:
        dict or None: è§£ææˆåŠŸè¿”å›å­—å…¸å¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
    """
    if not response_content:
        return None

    # æ–¹æ³•1: å°è¯•æ‰¾åˆ°æ‰€æœ‰JSONå—ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
    json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_content, re.DOTALL)

    for json_str in json_matches:
        try:
            parsed_json = json.loads(json_str)
            # éªŒè¯JSONæ˜¯å¦åŒ…å«å¿…è¦çš„å­—æ®µ
            if isinstance(parsed_json, dict) and 'suggested_name' in parsed_json:
                logging.info(f"âœ… æˆåŠŸè§£æJSON (æ–¹æ³•1): {json_str[:100]}...")
                return parsed_json
        except json.JSONDecodeError:
            continue

    # æ–¹æ³•2: ä½¿ç”¨è´ªå©ªåŒ¹é…æå–æœ€å¤§çš„JSONå—
    json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
    if json_match:
        json_str = json_match.group()

        # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        # ç§»é™¤é‡å¤çš„JSONå—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if '```json' in json_str:
            # æå–ç¬¬ä¸€ä¸ª```jsonå—ä¸­çš„å†…å®¹
            json_blocks = re.findall(r'```json\s*(\{.*?\})\s*```', json_str, re.DOTALL)
            if json_blocks:
                json_str = json_blocks[0]

        try:
            parsed_json = json.loads(json_str)
            logging.info(f"âœ… æˆåŠŸè§£æJSON (æ–¹æ³•2): {json_str[:100]}...")
            return parsed_json
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æå¤±è´¥: {e}")
            logging.debug(f"åŸå§‹JSONå­—ç¬¦ä¸²: {json_str[:500]}...")

            # æ–¹æ³•3: å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
            try:
                # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹
                if json_str.count('{') > json_str.count('}'):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                    brace_count = 0
                    end_pos = 0
                    for i, char in enumerate(json_str):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break

                    if end_pos > 0:
                        json_str = json_str[:end_pos]
                        parsed_json = json.loads(json_str)
                        logging.info(f"âœ… æˆåŠŸè§£æJSON (æ–¹æ³•3): {json_str[:100]}...")
                        return parsed_json
            except json.JSONDecodeError:
                pass
    else:
        logging.error(f"å“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼: {response_content[:200]}...")

    return None

def match_files_with_ai(group_name, video_files, used_file_ids, target_count):
    """
    ä½¿ç”¨AIè¿›è¡Œæ™ºèƒ½æ–‡ä»¶åŒ¹é…

    æ ¹æ®åˆ†ç»„åç§°ï¼Œè®©AIæ™ºèƒ½åˆ¤æ–­å“ªäº›æ–‡ä»¶åº”è¯¥å±äºè¯¥åˆ†ç»„

    Args:
        group_name (str): ç›®æ ‡åˆ†ç»„çš„åç§°
        video_files (list): å¯ç”¨çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        used_file_ids (set): å·²ç»è¢«ä½¿ç”¨çš„æ–‡ä»¶IDé›†åˆ
        target_count (int): ç›®æ ‡åŒ¹é…çš„æ–‡ä»¶æ•°é‡

    Returns:
        list: åŒ¹é…åˆ°çš„æ–‡ä»¶IDåˆ—è¡¨
    """
    try:
        # å‡†å¤‡å¯ç”¨çš„æ–‡ä»¶åˆ—è¡¨
        available_files = [f for f in video_files if f['fileId'] not in used_file_ids]

        if not available_files:
            return []

        # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼Œé¿å…promptè¿‡é•¿
        max_files_for_ai = 50
        if len(available_files) > max_files_for_ai:
            available_files = available_files[:max_files_for_ai]

        # æ„å»ºæ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²
        file_list = []
        for i, file_info in enumerate(available_files):
            file_list.append(f"{i}: {file_info['filename']}")

        files_text = "\n".join(file_list)

        # æ„å»ºAIåŒ¹é…prompt
        match_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ï¼Œæ‰¾å‡ºå±äºåˆ†ç»„"{group_name}"çš„æ–‡ä»¶ã€‚

        æ–‡ä»¶åˆ—è¡¨ï¼š
        {files_text}

        è¯·æ ¹æ®æ–‡ä»¶ååˆ¤æ–­å“ªäº›æ–‡ä»¶å±äºåˆ†ç»„"{group_name}"ã€‚è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
        1. æ–‡ä»¶åä¸­çš„å…³é”®è¯åŒ¹é…
        2. å¹´ä»½ã€å­£æ•°ã€é›†æ•°ç­‰ä¿¡æ¯
        3. è¯­è¨€å’Œæ ¼å¼æ ‡è¯†
        4. ç³»åˆ—åç§°çš„ç›¸ä¼¼æ€§

        ç›®æ ‡æ–‡ä»¶æ•°é‡ï¼šçº¦{target_count}ä¸ªæ–‡ä»¶

        è¯·è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«åŒ¹é…çš„æ–‡ä»¶ç´¢å¼•ï¼š
        {{
            "matched_files": [0, 1, 2, ...],
            "reason": "åŒ¹é…åŸå› è¯´æ˜"
        }}

        åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
        """

        # è°ƒç”¨AIè¿›è¡ŒåŒ¹é…
        response_content = call_ai_api(match_prompt, app_config.get('GROUPING_MODEL', ''))

        if response_content:
            result = parse_json_from_ai_response(response_content)
            if result:
                matched_indices = result.get('matched_files', [])
                reason = result.get('reason', 'æ— è¯´æ˜')

                logging.info(f"ğŸ¤– AIåŒ¹é…åˆ†ç»„'{group_name}': {len(matched_indices)}ä¸ªæ–‡ä»¶, åŸå› : {reason}")

                # è½¬æ¢ç´¢å¼•ä¸ºæ–‡ä»¶ID
                matched_file_ids = []
                for idx in matched_indices:
                    if isinstance(idx, int) and 0 <= idx < len(available_files):
                        matched_file_ids.append(available_files[idx]['fileId'])

                return matched_file_ids

    except Exception as e:
        logging.error(f"AIæ–‡ä»¶åŒ¹é…å¤±è´¥: {e}")

    return []



def get_filenames_by_ids(file_ids, video_files):
    """
    æ ¹æ®æ–‡ä»¶IDåˆ—è¡¨è·å–å¯¹åº”çš„æ–‡ä»¶ååˆ—è¡¨

    Args:
        file_ids (list): æ–‡ä»¶IDåˆ—è¡¨
        video_files (list): è§†é¢‘æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«fileIdå’Œfilenameå­—æ®µ

    Returns:
        list: å¯¹åº”çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œä¿æŒä¸file_idsç›¸åŒçš„é¡ºåº
    """
    file_names = []
    for file_id in file_ids:
        for video_file in video_files:
            if video_file['fileId'] == file_id:
                file_names.append(video_file['filename'])
                break
    return file_names

def validate_and_process_groups(movie_info_raw):
    """éªŒè¯å’Œå¤„ç†åˆ†ç»„ä¿¡æ¯"""
    if not movie_info_raw:
        return []

    # å¤„ç†å¯èƒ½çš„åµŒå¥—åˆ—è¡¨ç»“æ„
    if isinstance(movie_info_raw, list):
        if len(movie_info_raw) > 0 and isinstance(movie_info_raw[0], list):
            # å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå–ç¬¬ä¸€å±‚
            movie_info = movie_info_raw[0]
        else:
            movie_info = movie_info_raw
    else:
        movie_info = [movie_info_raw] if movie_info_raw else []

    # éªŒè¯åˆ†ç»„ä¿¡æ¯çš„æœ‰æ•ˆæ€§
    if movie_info and isinstance(movie_info, list) and len(movie_info) > 0:
        valid_groups = []
        for group in movie_info:
            if (isinstance(group, dict) and
                'group_name' in group and
                group['group_name'] and
                ('fileIds' in group or 'files' in group)):
                # æ£€æŸ¥fileIdsæ˜¯å¦æœ‰æ•ˆ
                file_ids = group.get('fileIds', []) or group.get('files', [])
                if file_ids and isinstance(file_ids, list) and len(file_ids) > 0:
                    valid_groups.append(group)
        return valid_groups

    return []

def process_group_file_matching(valid_groups, video_files):
    """å¤„ç†åˆ†ç»„çš„æ–‡ä»¶åŒ¹é… - ä¼˜åŒ–ç‰ˆï¼šç›´æ¥è¿”å›åŸå§‹åˆ†ç»„ç»“æœï¼Œå‡å°‘APIè°ƒç”¨"""
    if not valid_groups:
        return []

    # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨AIåˆ†ç»„çš„åŸå§‹ç»“æœï¼Œé¿å…é¢å¤–çš„APIè°ƒç”¨
    logging.info(f"âš¡ è·³è¿‡æ‰¹é‡æ–‡ä»¶åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ†ç»„ç»“æœ: {len(valid_groups)} ä¸ªåˆ†ç»„")
    logging.info(f"ğŸ“Š å¯ç”¨è§†é¢‘æ–‡ä»¶æ•°é‡: {len(video_files)} ä¸ª")



    # ç›´æ¥è¿”å›åŸå§‹åˆ†ç»„ç»“æœï¼Œä¸è¿›è¡Œé¢å¤–çš„AIåŒ¹é…
    return valid_groups

def enhance_groups_with_filenames(corrected_groups, video_files):
    """ä¸ºåˆ†ç»„æ·»åŠ æ–‡ä»¶åä¿¡æ¯"""
    enhanced_groups = []
    for group in corrected_groups:
        enhanced_group = group.copy()
        file_ids = enhanced_group.get('fileIds', [])

        # è·å–å¯¹åº”çš„æ–‡ä»¶ååˆ—è¡¨
        file_names = get_filenames_by_ids(file_ids, video_files)
        enhanced_group['file_names'] = file_names
        enhanced_groups.append(enhanced_group)

        # è°ƒè¯•æ—¥å¿—
        logging.info(f"åˆ†ç»„ '{enhanced_group['group_name']}': {len(file_ids)} ä¸ªæ–‡ä»¶ID, {len(file_names)} ä¸ªæ–‡ä»¶å")
        if len(file_names) > 0:
            logging.info(f"  æ–‡ä»¶åç¤ºä¾‹: {file_names[0] if file_names else 'æ— '}")

    return enhanced_groups


# è¿™äº›å…¨å±€å˜é‡å·²ç»ç§»åŠ¨åˆ°ä¸Šé¢çš„å…¨å±€å˜é‡å£°æ˜åŒºåŸŸ

def load_application_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½åº”ç”¨ç¨‹åºé…ç½®

    å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®å¹¶åˆ›å»ºé…ç½®æ–‡ä»¶ã€‚
    åŠ è½½å®Œæˆåæ›´æ–°æ‰€æœ‰ç›¸å…³çš„å…¨å±€å˜é‡ã€‚

    Global Variables Updated:
        app_config: ä¸»é…ç½®å­—å…¸
        QPS_LIMIT, CHUNK_SIZE, MAX_WORKERS: æ€§èƒ½é…ç½®
        CLIENT_ID, CLIENT_SECRET: 123äº‘ç›˜APIé…ç½®
        TMDB_API_KEY, GEMINI_API_KEY, GEMINI_API_URL: ç¬¬ä¸‰æ–¹APIé…ç½®
        MODEL, GROUPING_MODEL, LANGUAGE: AIå’Œæœ¬åœ°åŒ–é…ç½®
    """
    global app_config, QPS_LIMIT, CHUNK_SIZE, MAX_WORKERS, CLIENT_ID, CLIENT_SECRET
    global TMDB_API_KEY, AI_API_KEY, AI_API_URL, MODEL, GROUPING_MODEL, LANGUAGE
    global API_MAX_RETRIES, API_RETRY_DELAY, AI_API_TIMEOUT, AI_MAX_RETRIES, AI_RETRY_DELAY
    global TMDB_API_TIMEOUT, TMDB_MAX_RETRIES, TMDB_RETRY_DELAY, CLOUD_API_MAX_RETRIES, CLOUD_API_RETRY_DELAY
    global GROUPING_MAX_RETRIES, GROUPING_RETRY_DELAY, TASK_QUEUE_GET_TIMEOUT
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                loaded_config = json.load(f)
                app_config.update(loaded_config)
            logging.info(f"é…ç½®å·²ä» {CONFIG_FILE} åŠ è½½ã€‚")
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶ {CONFIG_FILE} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    else:
        logging.info(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºå¹¶ä¿å­˜é»˜è®¤é…ç½®ã€‚")
        save_application_config()

    # æ›´æ–°å…¨å±€å˜é‡
    global ENABLE_QUALITY_ASSESSMENT, ENABLE_SCRAPING_QUALITY_ASSESSMENT

    QPS_LIMIT = app_config["QPS_LIMIT"]
    CHUNK_SIZE = app_config["CHUNK_SIZE"]
    MAX_WORKERS = app_config["MAX_WORKERS"]
    CLIENT_ID = app_config["CLIENT_ID"]
    CLIENT_SECRET = app_config["CLIENT_SECRET"]
    TMDB_API_KEY = app_config.get("TMDB_API_KEY", "")
    # å…¼å®¹æ—§é…ç½®ï¼šå¦‚æœæ–°é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ—§é…ç½®
    AI_API_KEY = app_config.get("AI_API_KEY", "") or app_config.get("GEMINI_API_KEY", "")
    AI_API_URL = app_config.get("AI_API_URL", "") or app_config.get("GEMINI_API_URL", "")
    MODEL = app_config.get("MODEL", "gpt-3.5-turbo")
    GROUPING_MODEL = app_config.get("GROUPING_MODEL", "gpt-3.5-turbo")
    LANGUAGE = app_config.get("LANGUAGE", "zh-CN")

    # æ›´æ–°é‡è¯•å’Œè¶…æ—¶é…ç½®
    API_MAX_RETRIES = app_config.get("API_MAX_RETRIES", 3)
    API_RETRY_DELAY = app_config.get("API_RETRY_DELAY", 2)
    AI_API_TIMEOUT = app_config.get("AI_API_TIMEOUT", 60)
    AI_MAX_RETRIES = app_config.get("AI_MAX_RETRIES", 3)
    AI_RETRY_DELAY = app_config.get("AI_RETRY_DELAY", 2)
    TMDB_API_TIMEOUT = app_config.get("TMDB_API_TIMEOUT", 60)
    TMDB_MAX_RETRIES = app_config.get("TMDB_MAX_RETRIES", 3)
    TMDB_RETRY_DELAY = app_config.get("TMDB_RETRY_DELAY", 2)
    CLOUD_API_MAX_RETRIES = app_config.get("CLOUD_API_MAX_RETRIES", 3)
    CLOUD_API_RETRY_DELAY = app_config.get("CLOUD_API_RETRY_DELAY", 2)
    GROUPING_MAX_RETRIES = app_config.get("GROUPING_MAX_RETRIES", 3)
    GROUPING_RETRY_DELAY = app_config.get("GROUPING_RETRY_DELAY", 2)
    TASK_QUEUE_GET_TIMEOUT = app_config.get("TASK_QUEUE_GET_TIMEOUT", 1.0)
    ENABLE_QUALITY_ASSESSMENT = app_config.get("ENABLE_QUALITY_ASSESSMENT", False)
    ENABLE_SCRAPING_QUALITY_ASSESSMENT = app_config.get("ENABLE_SCRAPING_QUALITY_ASSESSMENT", True)
    logging.info(f"âœ… é…ç½®åŠ è½½å®Œæˆã€‚QPS_LIMIT: {QPS_LIMIT}, CHUNK_SIZE: {CHUNK_SIZE}, MAX_WORKERS: {MAX_WORKERS}")
    logging.info(f"ğŸ”‘ APIé…ç½®çŠ¶æ€ - CLIENT_ID: {'å·²è®¾ç½®' if CLIENT_ID else 'æœªè®¾ç½®'}, CLIENT_SECRET: {'å·²è®¾ç½®' if CLIENT_SECRET else 'æœªè®¾ç½®'}")
    logging.info(f"ğŸ¬ TMDB_API_KEY: {'å·²è®¾ç½®' if TMDB_API_KEY else 'æœªè®¾ç½®'}, AI_API_KEY: {'å·²è®¾ç½®' if AI_API_KEY else 'æœªè®¾ç½®'}")
    logging.info(f"ğŸ¤– AIæ¨¡å‹é…ç½® - MODEL: {MODEL}, GROUPING_MODEL: {GROUPING_MODEL}, LANGUAGE: {LANGUAGE}")
    logging.info(f"ğŸŒ AI_API_URL: {'å·²è®¾ç½®' if AI_API_URL else 'æœªè®¾ç½®'}")

def save_application_config():
    """
    å°†å½“å‰é…ç½®ä¿å­˜åˆ°é…ç½®æ–‡ä»¶

    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(app_config, f, ensure_ascii=False, indent=4)
        logging.info(f"é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}")
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶ {CONFIG_FILE} å¤±è´¥: {e}")
        return False

# ================================
# è‡ªå®šä¹‰å¼‚å¸¸ç±»
# ================================

class ClientCredentialsError(Exception):
    """123äº‘ç›˜å®¢æˆ·ç«¯å‡­æ®é”™è¯¯å¼‚å¸¸"""
    def __init__(self, response_data):
        self.response_data = response_data
        super().__init__(f"é”™è¯¯çš„client_idæˆ–client_secretï¼Œè¯·æ£€æŸ¥åé‡è¯•\n{self.response_data}")


class AccessTokenError(Exception):
    """123äº‘ç›˜è®¿é—®ä»¤ç‰Œé”™è¯¯å¼‚å¸¸"""
    def __init__(self, response_data):
        self.response_data = response_data
        super().__init__(f"é”™è¯¯çš„access_tokenï¼Œè¯·æ£€æŸ¥åé‡è¯•\n{self.response_data}")


class TokenLimitExceededError(Exception):
    """è®¿é—®ä»¤ç‰Œä½¿ç”¨æ¬¡æ•°è¶…é™å¼‚å¸¸"""
    def __init__(self, response_data):
        self.response_data = response_data
        super().__init__(f"è®¿é—®ä»¤ç‰Œä½¿ç”¨æ¬¡æ•°å·²è¶…é™ï¼Œéœ€è¦åˆ·æ–°ä»¤ç‰Œ\n{self.response_data}")


class TaskCancelledException(Exception):
    """ä»»åŠ¡è¢«å–æ¶ˆå¼‚å¸¸"""
    pass


class APIRateLimitException(Exception):
    """APIé¢‘ç‡é™åˆ¶å¼‚å¸¸"""
    pass


class AccessTokenExpiredException(Exception):
    """è®¿é—®ä»¤ç‰Œè¿‡æœŸå¼‚å¸¸"""
    pass


class ConfigurationError(Exception):
    """é…ç½®é”™è¯¯å¼‚å¸¸"""
    pass


class NetworkError(Exception):
    """ç½‘ç»œé”™è¯¯å¼‚å¸¸"""
    pass


class FileSystemError(Exception):
    """æ–‡ä»¶ç³»ç»Ÿé”™è¯¯å¼‚å¸¸"""
    pass


class ValidationError(Exception):
    """æ•°æ®éªŒè¯é”™è¯¯å¼‚å¸¸"""
    pass


class AIServiceError(Exception):
    """AIæœåŠ¡é”™è¯¯å¼‚å¸¸"""
    pass


class CacheError(Exception):
    """ç¼“å­˜æ“ä½œé”™è¯¯å¼‚å¸¸"""
    pass


# ================================
# é…ç½®ç®¡ç†ç±»
# ================================

class ConfigManager:
    """
    ç»Ÿä¸€çš„é…ç½®ç®¡ç†ç±»

    Features:
    - é…ç½®éªŒè¯
    - ç±»å‹è½¬æ¢
    - é»˜è®¤å€¼å¤„ç†
    - é…ç½®æ›´æ–°é€šçŸ¥
    """

    # é…ç½®é¡¹å®šä¹‰å’ŒéªŒè¯è§„åˆ™
    CONFIG_SCHEMA = {
        # æ€§èƒ½é…ç½®
        'QPS_LIMIT': {'type': int, 'min': 1, 'max': 50, 'default': 8},
        'CHUNK_SIZE': {'type': int, 'min': 10, 'max': 200, 'default': 50},
        'MAX_WORKERS': {'type': int, 'min': 1, 'max': 20, 'default': 6},

        # APIé…ç½®
        'CLIENT_ID': {'type': str, 'required': False, 'default': ''},
        'CLIENT_SECRET': {'type': str, 'required': False, 'default': ''},
        'TMDB_API_KEY': {'type': str, 'required': False, 'default': ''},
        'AI_API_KEY': {'type': str, 'required': False, 'default': ''},
        'AI_API_URL': {'type': str, 'required': False, 'default': ''},

        # AIæ¨¡å‹é…ç½®
        'MODEL': {'type': str, 'default': ''},
        'GROUPING_MODEL': {'type': str, 'default': ''},
        'LANGUAGE': {'type': str, 'default': 'zh-CN'},

        # è¶…æ—¶é…ç½®
        'AI_API_TIMEOUT': {'type': int, 'min': 5, 'max': 300, 'default': 30},
        'TMDB_API_TIMEOUT': {'type': int, 'min': 5, 'max': 60, 'default': 10},

        # é‡è¯•é…ç½®
        'AI_MAX_RETRIES': {'type': int, 'min': 1, 'max': 10, 'default': 3},
        'TMDB_MAX_RETRIES': {'type': int, 'min': 1, 'max': 10, 'default': 3},

        # åŠŸèƒ½å¼€å…³
        'KILL_OCCUPIED_PORT_PROCESS': {'type': bool, 'default': True},
        'ENABLE_QUALITY_ASSESSMENT': {'type': bool, 'default': False},
        'ENABLE_SCRAPING_QUALITY_ASSESSMENT': {'type': bool, 'default': True},
    }

    def __init__(self, config_file='config.json'):
        self.config_file = config_file
        self.config = {}
        self.load_config()

    def validate_config_item(self, key, value):
        """éªŒè¯å•ä¸ªé…ç½®é¡¹"""
        if key not in self.CONFIG_SCHEMA:
            raise ValidationError(f"æœªçŸ¥çš„é…ç½®é¡¹: {key}")

        schema = self.CONFIG_SCHEMA[key]

        # ç±»å‹æ£€æŸ¥
        expected_type = schema['type']
        if not isinstance(value, expected_type):
            try:
                # å°è¯•ç±»å‹è½¬æ¢
                if expected_type == int:
                    value = int(value)
                elif expected_type == float:
                    value = float(value)
                elif expected_type == bool:
                    if isinstance(value, str):
                        value = value.lower() in ('true', '1', 'yes', 'on')
                    else:
                        value = bool(value)
                elif expected_type == str:
                    value = str(value)
            except (ValueError, TypeError):
                raise ValidationError(f"é…ç½®é¡¹ {key} ç±»å‹é”™è¯¯ï¼ŒæœŸæœ› {expected_type.__name__}ï¼Œå¾—åˆ° {type(value).__name__}")

        # èŒƒå›´æ£€æŸ¥
        if 'min' in schema and value < schema['min']:
            raise ValidationError(f"é…ç½®é¡¹ {key} å€¼ {value} å°äºæœ€å°å€¼ {schema['min']}")

        if 'max' in schema and value > schema['max']:
            raise ValidationError(f"é…ç½®é¡¹ {key} å€¼ {value} å¤§äºæœ€å¤§å€¼ {schema['max']}")

        # å¿…éœ€é¡¹æ£€æŸ¥
        if schema.get('required', False) and not value:
            raise ValidationError(f"é…ç½®é¡¹ {key} æ˜¯å¿…éœ€çš„ï¼Œä¸èƒ½ä¸ºç©º")

        return value

    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)

                # éªŒè¯å¹¶åº”ç”¨é…ç½®
                for key, value in loaded_config.items():
                    if key in self.CONFIG_SCHEMA:
                        try:
                            validated_value = self.validate_config_item(key, value)
                            self.config[key] = validated_value
                        except ValidationError as e:
                            logging.warning(f"é…ç½®é¡¹ {key} éªŒè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                            self.config[key] = self.CONFIG_SCHEMA[key]['default']
                    else:
                        # ä¿ç•™æœªçŸ¥é…ç½®é¡¹ï¼ˆå‘åå…¼å®¹ï¼‰
                        self.config[key] = value

                # è®¾ç½®ç¼ºå¤±é…ç½®é¡¹çš„é»˜è®¤å€¼
                for key, schema in self.CONFIG_SCHEMA.items():
                    if key not in self.config:
                        self.config[key] = schema['default']

                logging.info(f"é…ç½®å·²ä» {self.config_file} åŠ è½½å¹¶éªŒè¯")
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                for key, schema in self.CONFIG_SCHEMA.items():
                    self.config[key] = schema['default']

                logging.info(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self.save_config()

        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise ConfigurationError(f"é…ç½®åŠ è½½å¤±è´¥: {str(e)}")

    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=4)
            logging.info(f"é…ç½®å·²ä¿å­˜åˆ° {self.config_file}")
            return True
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise ConfigurationError(f"é…ç½®ä¿å­˜å¤±è´¥: {str(e)}")

    def get(self, key, default=None):
        """è·å–é…ç½®é¡¹"""
        return self.config.get(key, default)

    def set(self, key, value):
        """è®¾ç½®é…ç½®é¡¹"""
        validated_value = self.validate_config_item(key, value)
        self.config[key] = validated_value
        return validated_value

    def update(self, new_config):
        """æ‰¹é‡æ›´æ–°é…ç½®"""
        validated_config = {}
        for key, value in new_config.items():
            if key in self.CONFIG_SCHEMA:
                validated_config[key] = self.validate_config_item(key, value)
            else:
                # ä¿ç•™æœªçŸ¥é…ç½®é¡¹
                validated_config[key] = value

        self.config.update(validated_config)
        return validated_config

    def get_all(self):
        """è·å–æ‰€æœ‰é…ç½®"""
        return self.config.copy()

    def get_stats(self):
        """è·å–é…ç½®ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_items': len(self.config),
            'schema_items': len(self.CONFIG_SCHEMA),
            'custom_items': len(self.config) - len(self.CONFIG_SCHEMA),
            'config_file': self.config_file
        }


# ================================
# åº”ç”¨ç¨‹åºçŠ¶æ€ç®¡ç†ç±»
# ================================

class AppState:
    """
    åº”ç”¨ç¨‹åºçŠ¶æ€ç®¡ç†ç±»

    ç”¨äºç®¡ç†å…¨å±€çŠ¶æ€ï¼Œå‡å°‘å…¨å±€å˜é‡çš„ä½¿ç”¨
    """

    def __init__(self):
        # é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager()

        # APIç›¸å…³çŠ¶æ€
        self.access_token = None
        self.token_expiry = None

        # ä»»åŠ¡ç®¡ç†çŠ¶æ€
        self.current_task_id = None
        self.task_cancelled = False

        # QPSé™åˆ¶å™¨
        self.qps_limiter = None

        # æ—¥å¿—é˜Ÿåˆ—
        self.log_queue = deque(maxlen=5000)

        # åˆå§‹åŒ–çŠ¶æ€
        self._initialize_state()

    def _initialize_state(self):
        """åˆå§‹åŒ–åº”ç”¨ç¨‹åºçŠ¶æ€"""
        # QPSé™åˆ¶å™¨å°†åœ¨åé¢åˆå§‹åŒ–
        self.qps_limiter = None

        # åˆå§‹åŒ–è®¿é—®ä»¤ç‰Œ
        self.access_token = None  # å°†åœ¨åé¢åˆå§‹åŒ–

    def get_config(self, key, default=None):
        """è·å–é…ç½®é¡¹"""
        return self.config_manager.get(key, default)

    def set_config(self, key, value):
        """è®¾ç½®é…ç½®é¡¹"""
        return self.config_manager.set(key, value)

    def update_config(self, new_config):
        """æ‰¹é‡æ›´æ–°é…ç½®"""
        return self.config_manager.update(new_config)

    def save_config(self):
        """ä¿å­˜é…ç½®"""
        return self.config_manager.save_config()

    def start_task(self, task_id):
        """å¼€å§‹æ–°ä»»åŠ¡"""
        self.current_task_id = task_id
        self.task_cancelled = False
        logging.info(f"ğŸš€ å¼€å§‹æ–°ä»»åŠ¡: {task_id}")

    def cancel_task(self):
        """å–æ¶ˆå½“å‰ä»»åŠ¡"""
        if self.current_task_id:
            self.task_cancelled = True
            logging.info(f"ğŸ›‘ ä»»åŠ¡å·²å–æ¶ˆ: {self.current_task_id}")

    def check_task_cancelled(self):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ"""
        if self.task_cancelled:
            raise TaskCancelledException(f"ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ: {self.current_task_id}")

    def add_log(self, message):
        """æ·»åŠ æ—¥å¿—åˆ°é˜Ÿåˆ—"""
        self.log_queue.append({
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'message': message
        })

    def get_logs(self, limit=None):
        """è·å–æ—¥å¿—"""
        if limit:
            return list(self.log_queue)[-limit:]
        return list(self.log_queue)

    def clear_logs(self):
        """æ¸…ç©ºæ—¥å¿—"""
        self.log_queue.clear()

    def get_stats(self):
        """è·å–åº”ç”¨ç¨‹åºç»Ÿè®¡ä¿¡æ¯"""
        return {
            'current_task': self.current_task_id,
            'task_cancelled': self.task_cancelled,
            'log_count': len(self.log_queue),
            'config_stats': self.config_manager.get_stats(),
            'cache_stats': {
                'folder_path_cache': folder_path_cache.stats(),
                'grouping_cache': grouping_cache.stats(),
                'scraping_cache': scraping_cache.stats(),
                'folder_content_cache': folder_content_cache.stats()
            }
        }


# app_stateå°†åœ¨QPSLimiterå®šä¹‰ååˆ›å»º


# ================================
# APIè£…é¥°å™¨
# ================================

def api_error_handler(func):
    """APIé”™è¯¯å¤„ç†è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except TaskCancelledException:
            # ä»»åŠ¡å–æ¶ˆå¼‚å¸¸éœ€è¦é‡æ–°æŠ›å‡º
            raise
        except APIRateLimitException as e:
            logging.warning(f"âš ï¸ APIé¢‘ç‡é™åˆ¶: {e}")
            return jsonify({'success': False, 'error': 'APIè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•'})
        except AccessTokenExpiredException as e:
            logging.error(f"âŒ è®¿é—®ä»¤ç‰Œè¿‡æœŸ: {e}")
            return jsonify({'success': False, 'error': 'è®¿é—®ä»¤ç‰Œå·²è¿‡æœŸï¼Œè¯·é‡æ–°é…ç½®'})
        except ConfigurationError as e:
            logging.error(f"âŒ é…ç½®é”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'é…ç½®é”™è¯¯: {str(e)}'})
        except NetworkError as e:
            logging.error(f"âŒ ç½‘ç»œé”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'ç½‘ç»œè¿æ¥å¤±è´¥: {str(e)}'})
        except ValidationError as e:
            logging.error(f"âŒ æ•°æ®éªŒè¯é”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'æ•°æ®éªŒè¯å¤±è´¥: {str(e)}'})
        except AIServiceError as e:
            logging.error(f"âŒ AIæœåŠ¡é”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'AIæœåŠ¡ä¸å¯ç”¨: {str(e)}'})
        except FileSystemError as e:
            logging.error(f"âŒ æ–‡ä»¶ç³»ç»Ÿé”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'æ–‡ä»¶æ“ä½œå¤±è´¥: {str(e)}'})
        except Exception as e:
            logging.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return jsonify({'success': False, 'error': f'ç³»ç»Ÿå†…éƒ¨é”™è¯¯: {str(e)}'})

    return wrapper


# ================================
# æ€§èƒ½ç›‘æ§ç±»
# ================================

class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§ç±»

    ç”¨äºç›‘æ§APIè°ƒç”¨æ€§èƒ½ã€ç¼“å­˜å‘½ä¸­ç‡ç­‰å…³é”®æŒ‡æ ‡
    """

    def __init__(self):
        self.metrics = {
            'api_calls': {},  # APIè°ƒç”¨ç»Ÿè®¡
            'cache_hits': {},  # ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
            'response_times': {},  # å“åº”æ—¶é—´ç»Ÿè®¡
            'error_counts': {},  # é”™è¯¯è®¡æ•°
            'start_time': time.time()
        }
        self.lock = threading.Lock()

    def record_api_call(self, endpoint, duration, success=True):
        """è®°å½•APIè°ƒç”¨"""
        with self.lock:
            if endpoint not in self.metrics['api_calls']:
                self.metrics['api_calls'][endpoint] = {
                    'total_calls': 0,
                    'success_calls': 0,
                    'total_duration': 0,
                    'avg_duration': 0,
                    'min_duration': float('inf'),
                    'max_duration': 0
                }

            stats = self.metrics['api_calls'][endpoint]
            stats['total_calls'] += 1
            stats['total_duration'] += duration
            stats['avg_duration'] = stats['total_duration'] / stats['total_calls']
            stats['min_duration'] = min(stats['min_duration'], duration)
            stats['max_duration'] = max(stats['max_duration'], duration)

            if success:
                stats['success_calls'] += 1

    def record_cache_hit(self, cache_name, hit=True):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        with self.lock:
            if cache_name not in self.metrics['cache_hits']:
                self.metrics['cache_hits'][cache_name] = {
                    'hits': 0,
                    'misses': 0,
                    'hit_rate': 0
                }

            stats = self.metrics['cache_hits'][cache_name]
            if hit:
                stats['hits'] += 1
            else:
                stats['misses'] += 1

            total = stats['hits'] + stats['misses']
            stats['hit_rate'] = stats['hits'] / total if total > 0 else 0

    def record_error(self, error_type):
        """è®°å½•é”™è¯¯"""
        with self.lock:
            if error_type not in self.metrics['error_counts']:
                self.metrics['error_counts'][error_type] = 0
            self.metrics['error_counts'][error_type] += 1

    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            uptime = time.time() - self.metrics['start_time']
            return {
                'uptime_seconds': uptime,
                'uptime_formatted': self._format_duration(uptime),
                'api_calls': self.metrics['api_calls'].copy(),
                'cache_hits': self.metrics['cache_hits'].copy(),
                'error_counts': self.metrics['error_counts'].copy()
            }

    def _format_duration(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´é•¿åº¦"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        with self.lock:
            self.metrics = {
                'api_calls': {},
                'cache_hits': {},
                'response_times': {},
                'error_counts': {},
                'start_time': time.time()
            }


# åˆ›å»ºå…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹
performance_monitor = PerformanceMonitor()


def task_management_decorator(func):
    """ä»»åŠ¡ç®¡ç†è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            # å¼€å§‹æ–°ä»»åŠ¡
            task_id = f"{func.__name__}_{int(time.time())}"
            start_new_task(task_id)

            # æ¸…ç†æ“ä½œç›¸å…³ç¼“å­˜
            operation_type = getattr(func, '_operation_type', 'unknown')
            clear_operation_related_caches(operation_type=operation_type)

            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)

            return result
        except TaskCancelledException:
            logging.info(f"ğŸ›‘ ä»»åŠ¡ {func.__name__} è¢«ç”¨æˆ·å–æ¶ˆ")
            return jsonify({'success': False, 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True})
        except Exception as e:
            logging.error(f"âŒ ä»»åŠ¡ {func.__name__} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise

    return wrapper


def performance_monitor_decorator(endpoint_name=None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            success = True

            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                success = False
                performance_monitor.record_error(type(e).__name__)
                raise
            finally:
                duration = time.time() - start_time
                name = endpoint_name or func.__name__
                performance_monitor.record_api_call(name, duration, success)

        return wrapper
    return decorator


def validate_api_response(response):
    """
    éªŒè¯123äº‘ç›˜APIå“åº”çŠ¶æ€

    Args:
        response: requests.Responseå¯¹è±¡

    Returns:
        dict: APIå“åº”ä¸­çš„dataéƒ¨åˆ†

    Raises:
        TokenLimitExceededError: å½“è®¿é—®ä»¤ç‰Œä½¿ç”¨æ¬¡æ•°è¶…é™æ—¶
        AccessTokenError: å½“APIè¿”å›å…¶ä»–è®¤è¯é”™è¯¯æ—¶
        requests.HTTPError: å½“HTTPçŠ¶æ€ç ä¸æ˜¯200æ—¶
    """
    if response.status_code == 200:
        response_data = json.loads(response.text)
        if response_data["code"] == 0:
            return response_data["data"]
        elif response_data["code"] == 401:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»¤ç‰Œä½¿ç”¨æ¬¡æ•°è¶…é™
            message = response_data.get("message", "").lower()
            if "tokens number has exceeded" in message or "exceeded the limit" in message:
                raise TokenLimitExceededError(response_data)
            else:
                raise AccessTokenError(response_data)
        else:
            raise AccessTokenError(response_data)
    else:
        raise requests.HTTPError(response.text)

# è¿™äº›å¸¸é‡å·²ç»ç§»åŠ¨åˆ°ä¸Šé¢çš„å…¨å±€å˜é‡å£°æ˜åŒºåŸŸ


# ================================
# 123äº‘ç›˜APIæ ¸å¿ƒå‡½æ•°
# ================================

# ================================
# æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–
# ================================

class QueueHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—æ¶ˆæ¯æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ä¾›Webç•Œé¢æ˜¾ç¤ºï¼ˆWindowså…¼å®¹æ€§å¢å¼ºï¼‰"""
    def emit(self, record):
        try:
            log_entry = self.format(record)
            # ä½¿ç”¨å®‰å…¨ç¼–ç å¤„ç†ï¼Œé¿å…Windowsç³»ç»Ÿä¸­çš„å­—ç¬¦ç¼–ç é—®é¢˜
            safe_log_entry = safe_log_message(log_entry)
            log_queue.append(safe_log_entry)
        except Exception as e:
            # å¦‚æœæ—¥å¿—å¤„ç†å¤±è´¥ï¼Œæ·»åŠ ä¸€ä¸ªé”™è¯¯æ¶ˆæ¯è€Œä¸æ˜¯å´©æºƒ
            error_msg = f"[æ—¥å¿—å¤„ç†é”™è¯¯: {str(e)}]"
            log_queue.append(error_msg)


def initialize_logging_system():
    """
    åˆå§‹åŒ–åº”ç”¨ç¨‹åºæ—¥å¿—ç³»ç»Ÿ

    é…ç½®æ–‡ä»¶æ—¥å¿—ã€æ§åˆ¶å°æ—¥å¿—å’ŒWebç•Œé¢æ—¥å¿—é˜Ÿåˆ—
    """
    global root_logger, file_handler, console_handler, queue_handler

    # é…ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # æ¸…é™¤æ‰€æœ‰ç°æœ‰çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨ï¼ˆWindowså…¼å®¹æ€§ï¼šæ˜ç¡®æŒ‡å®šUTF-8ç¼–ç ï¼‰
    file_handler = RotatingFileHandler(
        'rename_log.log',
        maxBytes=1024 * 1024,
        backupCount=3,
        encoding='utf-8'  # æ˜ç¡®æŒ‡å®šUTF-8ç¼–ç ï¼Œè§£å†³Windowsä¸­æ–‡å­—ç¬¦é—®é¢˜
    )
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    root_logger.addHandler(file_handler)

    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆWindowså…¼å®¹æ€§ï¼šè®¾ç½®é”™è¯¯å¤„ç†ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    # åœ¨Windowsç³»ç»Ÿä¸­ï¼Œæ§åˆ¶å°å¯èƒ½ä¸æ”¯æŒæŸäº›Unicodeå­—ç¬¦ï¼Œè®¾ç½®é”™è¯¯å¤„ç†ç­–ç•¥
    if hasattr(console_handler.stream, 'reconfigure'):
        try:
            console_handler.stream.reconfigure(encoding='utf-8', errors='replace')
        except Exception:
            pass  # å¦‚æœé‡é…ç½®å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤è®¾ç½®
    root_logger.addHandler(console_handler)

    # æ·»åŠ è‡ªå®šä¹‰é˜Ÿåˆ—å¤„ç†å™¨
    queue_handler = QueueHandler()
    queue_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    root_logger.addHandler(queue_handler)

    # ç¡®ä¿ Flask è‡ªå·±çš„æ—¥å¿—ä¹Ÿé€šè¿‡ root_logger å¤„ç†
    app.logger.addHandler(queue_handler)

    # ç¦ç”¨ Werkzeug çš„è®¿é—®æ—¥å¿—
    logging.getLogger('werkzeug').setLevel(logging.ERROR)

# ================================
# QPSé™åˆ¶å™¨ç±»å’Œåˆå§‹åŒ–
# ================================

class QPSLimiter:
    """
    QPSï¼ˆæ¯ç§’æŸ¥è¯¢æ•°ï¼‰é™åˆ¶å™¨

    ç”¨äºæ§åˆ¶APIè¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¶…è¿‡æœåŠ¡ç«¯é™åˆ¶
    """
    def __init__(self, qps_limit):
        self.qps_limit = float(qps_limit)
        self.interval = 1.0 / self.qps_limit
        self.last_request_time = 0
        self.lock = threading.Lock()

    def acquire(self):
        """è·å–è¯·æ±‚è®¸å¯ï¼Œå¦‚æœéœ€è¦ä¼šé˜»å¡ç­‰å¾…"""
        with self.lock:
            current_time = time.time()
            elapsed_time = current_time - self.last_request_time
            if elapsed_time < self.interval:
                time.sleep(self.interval - elapsed_time)
            self.last_request_time = time.time()


# åˆ›å»ºå…¨å±€åº”ç”¨ç¨‹åºçŠ¶æ€å®ä¾‹ï¼ˆåœ¨QPSLimiterå®šä¹‰ä¹‹åï¼‰
app_state = AppState()


def limit_path_depth(file_path, max_depth=3):
    """
    é™åˆ¶æ–‡ä»¶è·¯å¾„æœ€å¤šæ˜¾ç¤ºæŒ‡å®šå±‚æ•°ï¼ˆä»æœ«å°¾å¼€å§‹è®¡ç®—ï¼‰

    Args:
        file_path (str): å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        max_depth (int): æœ€å¤§æ˜¾ç¤ºå±‚æ•°ï¼Œé»˜è®¤ä¸º3

    Returns:
        str: é™åˆ¶å±‚æ•°åçš„è·¯å¾„
    """
    if not file_path:
        return file_path

    path_parts = file_path.split('/')
    if len(path_parts) > max_depth:
        return '/'.join(path_parts[-max_depth:])
    else:
        return file_path


def initialize_qps_limiters():
    """
    åˆå§‹åŒ–å„ç§APIçš„QPSé™åˆ¶å™¨

    åŸºäº123panå®˜æ–¹APIé™åˆ¶åˆ›å»ºä¸“ç”¨é™åˆ¶å™¨ï¼š
    - api/v2/file/list: 5 QPS
    - api/v1/file/move: 1 QPS
    - api/v1/file/delete: 1 QPS
    - api/v1/file/rename: 1 QPS
    """
    global qps_limiter, v2_list_limiter, rename_limiter, move_limiter, delete_limiter

    qps_limiter = QPSLimiter(qps_limit=QPS_LIMIT)  # é€šç”¨é™åˆ¶å™¨ï¼Œä½¿ç”¨é…ç½®å€¼
    v2_list_limiter = QPSLimiter(qps_limit=5)     # api/v2/file/list: 4 QPS (å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§)
    rename_limiter = QPSLimiter(qps_limit=1)       # api/v1/file/rename: ä¿å®ˆä½¿ç”¨1 QPS
    move_limiter = QPSLimiter(qps_limit=1)        # api/v1/file/move: 1 QPS (æé«˜æ€§èƒ½)
    delete_limiter = QPSLimiter(qps_limit=1)       # api/v1/file/delete: 1 QPS (æé«˜æ€§èƒ½)


def get_access_token_from_api(client_id: str, client_secret: str):
    """
    ä»123äº‘ç›˜APIè·å–è®¿é—®ä»¤ç‰Œ

    Args:
        client_id (str): 123äº‘ç›˜å¼€æ”¾å¹³å°å®¢æˆ·ç«¯ID
        client_secret (str): 123äº‘ç›˜å¼€æ”¾å¹³å°å®¢æˆ·ç«¯å¯†é’¥

    Returns:
        str or None: è®¿é—®ä»¤ç‰Œï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    url = BASE_API_URL + "/api/v1/access_token"
    data = {"ClientID": client_id, "ClientSecret": client_secret}

    try:
        logging.info(f"ğŸ”‘ å°è¯•è·å–è®¿é—®ä»¤ç‰Œï¼ŒURL: {url}")
        logging.info(f"ğŸ”‘ å®¢æˆ·ç«¯ID: {client_id[:10]}...")
        # logging.info(f"ğŸ”‘ è¯·æ±‚æ•°æ®: {data}")
        r = requests.post(url, json=data, headers=API_HEADERS)
        logging.info(f"ğŸ”‘ HTTPçŠ¶æ€ç : {r.status_code}")
        # logging.info(f"ğŸ”‘ å“åº”å†…å®¹: {r.text}")

        rdata = json.loads(r.text)
        if r.status_code == 200:
            if rdata["code"] == 0:
                logging.info("âœ… æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œ")
                return rdata['data']['accessToken']
            else:
                logging.error(f"âŒ APIè¿”å›é”™è¯¯: {rdata}")
        else:
            logging.error(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {r.status_code}")
    except Exception as e:
        logging.error(f"âŒ è·å–è®¿é—®ä»¤ç‰Œæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

    return None


def initialize_access_token():
    """
    åˆå§‹åŒ–123äº‘ç›˜è®¿é—®ä»¤ç‰Œ

    ä»æœ¬åœ°æ–‡ä»¶è¯»å–æˆ–é‡æ–°è·å–è®¿é—®ä»¤ç‰Œï¼Œå¹¶æ›´æ–°å…¨å±€è¯·æ±‚å¤´
    æ”¯æŒè®¿é—®ä»¤ç‰Œè¿‡æœŸæ£€æŸ¥å’Œè‡ªåŠ¨åˆ·æ–°

    Returns:
        str: è®¿é—®ä»¤ç‰Œ
    """
    global access_token, access_token_expires_at
    token = ""

    if os.path.exists("123_access_token.txt"):
        # å¦‚æœå­˜åœ¨è®¿é—®ä»¤ç‰Œæ–‡ä»¶ï¼Œè¯»å–å¹¶æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆWindowså…¼å®¹æ€§ï¼šæŒ‡å®šUTF-8ç¼–ç ï¼‰
        with open("123_access_token.txt", "r", encoding='utf-8') as f:
            token = f.read().strip()
        logging.info("ğŸ“ ä»æ–‡ä»¶è¯»å–è®¿é—®ä»¤ç‰Œ")

        # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ
        if is_access_token_expired(token):
            logging.warning("âš ï¸ æœ¬åœ°è®¿é—®ä»¤ç‰Œå·²è¿‡æœŸï¼Œå°è¯•è·å–æ–°ä»¤ç‰Œ")
            token = ""  # æ¸…ç©ºè¿‡æœŸçš„ä»¤ç‰Œ
        else:
            logging.info("âœ… æœ¬åœ°è®¿é—®ä»¤ç‰Œä»ç„¶æœ‰æ•ˆ")

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä»¤ç‰Œï¼Œå°è¯•è·å–æ–°ä»¤ç‰Œ
    if not token and CLIENT_ID and CLIENT_SECRET:
        logging.info(f"ğŸ”‘ CLIENT_ID: {'å·²è®¾ç½®' if CLIENT_ID else 'æœªè®¾ç½®'}, CLIENT_SECRET: {'å·²è®¾ç½®' if CLIENT_SECRET else 'æœªè®¾ç½®'}")
        token = get_access_token_from_api(CLIENT_ID, CLIENT_SECRET)
        if token:
            # ä¿å­˜æ–°ä»¤ç‰Œåˆ°æ–‡ä»¶ï¼ˆWindowså…¼å®¹æ€§ï¼šæŒ‡å®šUTF-8ç¼–ç ï¼‰
            try:
                with open("123_access_token.txt", "w", encoding='utf-8') as f:
                    f.write(token)
                logging.info("âœ… æˆåŠŸè·å–å¹¶ä¿å­˜æ–°è®¿é—®ä»¤ç‰Œ")
            except Exception as e:
                logging.error(f"âŒ ä¿å­˜è®¿é—®ä»¤ç‰Œå¤±è´¥: {e}")
        else:
            logging.error("âŒ æ— æ³•è·å–è®¿é—®ä»¤ç‰Œï¼Œè¯·æ£€æŸ¥CLIENT_IDå’ŒCLIENT_SECRETé…ç½®")
    elif not token:
        # æ²¡æœ‰é…ç½®ä¿¡æ¯ï¼Œåº”ç”¨ç¨‹åºä»ç„¶å¯ä»¥å¯åŠ¨ï¼Œä½†åŠŸèƒ½å—é™
        logging.warning("âš ï¸ æœªé…ç½®CLIENT_IDå’ŒCLIENT_SECRETï¼Œè¯·åœ¨ç½‘é¡µé…ç½®é¡µé¢è®¾ç½®")

    if token:
        # æ›´æ–°å…¨å±€å˜é‡
        access_token = token
        API_HEADERS["Authorization"] = f"Bearer {token}"
        logging.info("ğŸ” è®¿é—®ä»¤ç‰Œå·²è®¾ç½®åˆ°è¯·æ±‚å¤´")

        # è§£æå¹¶è®°å½•ä»¤ç‰Œè¿‡æœŸæ—¶é—´
        payload = decode_jwt_token(token)
        if payload and payload.get('exp'):
            expires_at = datetime.datetime.fromtimestamp(payload['exp'])
            access_token_expires_at = expires_at
            logging.info(f"â° è®¿é—®ä»¤ç‰Œè¿‡æœŸæ—¶é—´: {expires_at}")
        else:
            logging.warning("âš ï¸ æ— æ³•è§£æè®¿é—®ä»¤ç‰Œè¿‡æœŸæ—¶é—´")
    else:
        logging.warning("âš ï¸ æœªè®¾ç½®è®¿é—®ä»¤ç‰Œï¼Œ123äº‘ç›˜åŠŸèƒ½å°†ä¸å¯ç”¨")

    return token

def find_existing_folder(name: str, parent_id: int):
    """
    åœ¨æŒ‡å®šçˆ¶ç›®å½•ä¸­æŸ¥æ‰¾åŒåæ–‡ä»¶å¤¹

    Args:
        name (str): æ–‡ä»¶å¤¹åç§°
        parent_id (int): çˆ¶æ–‡ä»¶å¤¹ID

    Returns:
        int or None: å¦‚æœæ‰¾åˆ°è¿”å›æ–‡ä»¶å¤¹IDï¼Œå¦åˆ™è¿”å›None
    """
    try:
        logging.info(f"ğŸ” åœ¨çˆ¶ç›®å½• {parent_id} ä¸­æŸ¥æ‰¾æ–‡ä»¶å¤¹ '{name}'")
        folder_list = get_file_list_from_cloud(parent_id, 100)

        if 'fileList' in folder_list:
            for item in folder_list['fileList']:
                if item['type'] == 1 and item['filename'] == name:
                    logging.info(f"âœ… æ‰¾åˆ°ç°æœ‰æ–‡ä»¶å¤¹: {name}ï¼Œæ–‡ä»¶å¤¹ID: {item['fileId']}")
                    return item['fileId']

        # å¦‚æœåœ¨å½“å‰é¡µæ²¡æ‰¾åˆ°ï¼Œè¿›è¡Œåˆ†é¡µæŸ¥æ‰¾
        last_file_id = folder_list.get('lastFileId', -1)
        while last_file_id != -1:
            try:
                next_page = get_file_list_from_cloud(parent_id, 100, last_file_id=last_file_id)
                if 'fileList' in next_page:
                    for item in next_page['fileList']:
                        if item['type'] == 1 and item['filename'] == name:
                            logging.info(f"âœ… åœ¨åˆ†é¡µä¸­æ‰¾åˆ°ç°æœ‰æ–‡ä»¶å¤¹: {name}ï¼Œæ–‡ä»¶å¤¹ID: {item['fileId']}")
                            return item['fileId']
                last_file_id = next_page.get('lastFileId', -1)
            except Exception as page_error:
                logging.error(f"åˆ†é¡µæŸ¥æ‰¾æ—¶å‡ºé”™: {page_error}")
                break

        logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ '{name}' åœ¨çˆ¶ç›®å½• {parent_id} ä¸­ä¸å­˜åœ¨")
        return None

    except Exception as e:
        logging.error(f"æŸ¥æ‰¾æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
        return None


@ensure_valid_access_token
def create_folder_in_cloud(name: str, parent_id: int):
    """
    åœ¨123äº‘ç›˜ä¸­åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œå¦‚æœæ–‡ä»¶å¤¹å·²å­˜åœ¨åˆ™è¿”å›ç°æœ‰æ–‡ä»¶å¤¹ID

    Args:
        name (str): æ–‡ä»¶å¤¹åç§°
        parent_id (int): çˆ¶æ–‡ä»¶å¤¹IDï¼Œæ ¹ç›®å½•ä¸º0

    Returns:
        dict: APIå“åº”æ•°æ®ï¼ŒåŒ…å«æ–‡ä»¶å¤¹ID

    APIè¿”å›æ•°æ®æ ¼å¼:
        {
            "data": {
                "dirID": number  # æ–‡ä»¶å¤¹IDï¼ˆæ–°å»ºæˆ–ç°æœ‰ï¼‰
            }
        }
    """
    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å·²å­˜åœ¨
    existing_folder_id = find_existing_folder(name, parent_id)
    if existing_folder_id:
        logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ '{name}' å·²å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶å¤¹ID: {existing_folder_id}")
        return {'data': {'dirID': existing_folder_id}}

    # æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å¤¹
    logging.info(f"ğŸ“ å‡†å¤‡åˆ›å»ºæ–°æ–‡ä»¶å¤¹: {name}ï¼Œçˆ¶ç›®å½•ID: {parent_id}")
    url = BASE_API_URL + "/upload/v1/file/mkdir"
    data = {"name": name, "parentID": parent_id}

    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            logging.info(f"åˆ›å»ºæ–‡ä»¶å¤¹è¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries}): {data}")
            # ä½¿ç”¨POSTæ–¹æ³•å’ŒJSONæ ¼å¼å‘é€è¯·æ±‚
            r = requests.post(url, json=data, headers=API_HEADERS)
            logging.info(f"HTTPå“åº”çŠ¶æ€ç : {r.status_code}")
            logging.info(f"HTTPå“åº”å†…å®¹: {r.text}")

            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            result = validate_api_response(r)
            logging.info(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {name}ï¼Œæ–°æ–‡ä»¶å¤¹ID: {result.get('dirID')}")
            return {'data': result}
        except (AccessTokenError, TokenLimitExceededError) as e:
            logging.error(f"è®¿é—®ä»¤ç‰Œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶å¤¹å·²å­˜åœ¨çš„é”™è¯¯ï¼ˆå¯èƒ½åœ¨æ£€æŸ¥å’Œåˆ›å»ºä¹‹é—´æœ‰å…¶ä»–è¿›ç¨‹åˆ›å»ºäº†åŒåæ–‡ä»¶å¤¹ï¼‰
            if isinstance(e, AccessTokenError):
                error_data = e.args[0] if e.args else {}
                if isinstance(error_data, dict) and error_data.get('message') == 'è¯¥ç›®å½•ä¸‹å·²ç»æœ‰åŒåæ–‡ä»¶å¤¹,æ— æ³•è¿›è¡Œåˆ›å»º':
                    logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ '{name}' åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­è¢«å…¶ä»–è¿›ç¨‹åˆ›å»ºï¼Œé‡æ–°æŸ¥æ‰¾")
                    existing_folder_id = find_existing_folder(name, parent_id)
                    if existing_folder_id:
                        return {'data': {'dirID': existing_folder_id}}
                    else:
                        logging.error(f"âŒ æ–‡ä»¶å¤¹ '{name}' åˆ›å»ºå¤±è´¥ä¸”æ— æ³•æ‰¾åˆ°ç°æœ‰æ–‡ä»¶å¤¹")
                        raise e

            # å…¶ä»–è®¿é—®ä»¤ç‰Œé”™è¯¯ï¼Œç»§ç»­é‡è¯•é€»è¾‘
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise
        except requests.exceptions.RequestException as e:
            logging.error(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise  # Re-raise the last exception if all retries fail


@ensure_valid_access_token
def get_file_list_from_cloud(parent_file_id: int, limit: int, search_data=None, search_mode=None, last_file_id=None):
    """
    ä»123äº‘ç›˜è·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰

    Args:
        parent_file_id (int): çˆ¶æ–‡ä»¶å¤¹IDï¼Œæ ¹ç›®å½•ä¸º0
        limit (int): æ¯é¡µè¿”å›çš„æ–‡ä»¶æ•°é‡é™åˆ¶
        search_data (str, optional): æœç´¢å…³é”®è¯
        search_mode (str, optional): æœç´¢æ¨¡å¼
        last_file_id (int, optional): ä¸Šä¸€é¡µæœ€åä¸€ä¸ªæ–‡ä»¶çš„IDï¼Œç”¨äºåˆ†é¡µ

    Returns:
        dict: APIå“åº”æ•°æ®ï¼ŒåŒ…å«æ–‡ä»¶åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯
    """
    v2_list_limiter.acquire()  # ä½¿ç”¨ä¸“ç”¨çš„v2_listé™æµå™¨ï¼ˆ15 QPSï¼‰
    # current_time = datetime.datetime.now()
    # formatted_time = current_time.strftime("%H:%M:%S")
    # print("v2_list:",formatted_time)
    url = BASE_API_URL + "/api/v2/file/list"
    data = {"parentFileId": parent_file_id, "limit": limit}
    if search_data:
        data["searchData"] = search_data
    if search_mode:
        data["searchMode"] = search_mode
    if last_file_id:
        data["lastFileID"] = last_file_id

    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            r = requests.get(url, data=data, headers=API_HEADERS)
            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            result = validate_api_response(r)

            # è·å–å½“å‰æ–‡ä»¶å¤¹çš„è·¯å¾„å‰ç¼€
            current_path_prefix = get_folder_full_path(parent_file_id)

            # ä¸ºæ¯ä¸ªæ–‡ä»¶å’Œæ–‡ä»¶å¤¹æ·»åŠ  file_name å­—æ®µï¼ˆå®Œæ•´è·¯å¾„ï¼Œé™åˆ¶æœ€å¤šå€’æ•°ä¸‰å±‚ï¼‰
            if "fileList" in result:
                for item in result["fileList"]:
                    full_path = os.path.join(current_path_prefix, item['filename']) if current_path_prefix else item['filename']
                    # é™åˆ¶è·¯å¾„æœ€å¤šæ˜¾ç¤ºå€’æ•°ä¸‰å±‚
                    item['file_name'] = limit_path_depth(full_path, 3)

            return result
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise  # Re-raise the last exception if all retries fail


def get_all_files_in_folder(folder_id, limit=100, check_cancellation=False):
    """
    è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆè‡ªåŠ¨å¤„ç†åˆ†é¡µï¼‰

    Args:
        folder_id (int): æ–‡ä»¶å¤¹ID
        limit (int): æ¯é¡µè¿”å›çš„æ–‡ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        check_cancellation (bool): æ˜¯å¦æ£€æŸ¥ä»»åŠ¡å–æ¶ˆçŠ¶æ€ï¼Œé»˜è®¤False

    Returns:
        list: åŒ…å«æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯çš„åˆ—è¡¨
    """
    # åªåœ¨æ˜ç¡®è¦æ±‚æ—¶æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
    if check_cancellation:
        check_task_cancelled()

    try:
        filelist = get_file_list_from_cloud(folder_id, limit=limit)
        all_files = filelist["fileList"]
        last_file_id = filelist["lastFileId"]

        while last_file_id != -1:
            # åªåœ¨æ˜ç¡®è¦æ±‚æ—¶æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            if check_cancellation:
                check_task_cancelled()

            # QPSæ§åˆ¶å·²ç»åœ¨QPSLimiterä¸­å®ç°ï¼Œæ— éœ€é¢å¤–å»¶è¿Ÿ

            next_page = get_file_list_from_cloud(folder_id, last_file_id=last_file_id, limit=limit)
            all_files.extend(next_page["fileList"])
            last_file_id = next_page["lastFileId"]

        return all_files
    except Exception as e:
        # å¦‚æœæ˜¯429é”™è¯¯æˆ–APIé¢‘ç‡é™åˆ¶ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        if "429" in str(e) or "æ“ä½œé¢‘ç¹" in str(e):
            logging.warning(f"âš ï¸ APIé¢‘ç‡é™åˆ¶ï¼Œè·³è¿‡æ–‡ä»¶å¤¹ {folder_id}: {e}")
            return []
        else:
            # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
            raise


def get_folder_full_path(folder_id):
    """
    è·å–æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰

    Args:
        folder_id (int): æ–‡ä»¶å¤¹IDï¼Œæ ¹ç›®å½•ä¸º0

    Returns:
        str: æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼Œæ ¹ç›®å½•è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    global folder_path_cache

    if folder_id == 0:
        return ""

    # æ£€æŸ¥ç¼“å­˜
    if folder_id in folder_path_cache:
        return folder_path_cache[folder_id]

    paths = []
    fid = folder_id
    uncached_folders = []

    # æ”¶é›†æœªç¼“å­˜çš„æ–‡ä»¶å¤¹ID
    while fid != 0 and fid not in folder_path_cache:
        uncached_folders.append(fid)
        try:
            folder_details = detail(fid)
            paths.append(folder_details['filename'])
            fid = int(folder_details["parentFileID"])
        except Exception as e:
            logging.warning(f"è·å–æ–‡ä»¶å¤¹ {fid} è·¯å¾„å¤±è´¥: {e}")
            break

    # å¦‚æœæ‰¾åˆ°äº†ç¼“å­˜çš„çˆ¶è·¯å¾„ï¼Œä½¿ç”¨å®ƒ
    if fid != 0 and fid in folder_path_cache:
        parent_path = folder_path_cache[fid]
        if parent_path:
            paths.append(parent_path)

    paths.reverse()
    full_path = "/".join(paths)

    # ç¼“å­˜æ‰€æœ‰è·¯å¾„ï¼ˆåŒ…æ‹¬ä¸­é—´è·¯å¾„ï¼‰
    current_path = ""
    for i, folder_id_to_cache in enumerate(reversed(uncached_folders)):
        if i == 0:
            current_path = paths[0] if paths else ""
        else:
            current_path = "/".join(paths[:i+1]) if len(paths) > i else current_path
        folder_path_cache[folder_id_to_cache] = current_path

    return full_path


def get_video_files_for_naming(folder_id, file_list, max_files=200, max_depth=3):
    """
    ä¸ºæ™ºèƒ½é‡å‘½ååŠŸèƒ½ä¼˜åŒ–çš„è§†é¢‘æ–‡ä»¶è·å–å‡½æ•°

    Args:
        folder_id (int): è¦æœç´¢çš„æ–‡ä»¶å¤¹ID
        file_list (list): ç”¨äºå­˜å‚¨æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶çš„åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
        max_files (int): æœ€å¤§æ–‡ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤200
        max_depth (int): æœ€å¤§æ‰«ææ·±åº¦ï¼Œé»˜è®¤3å±‚

    Note:
        æ­¤å‡½æ•°ä¸“é—¨ä¸ºæ™ºèƒ½é‡å‘½ååŠŸèƒ½ä¼˜åŒ–ï¼Œé™åˆ¶æ‰«ææ·±åº¦å’Œæ–‡ä»¶æ•°é‡ä»¥æé«˜æ€§èƒ½
    """
    logging.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½é‡å‘½åæ–‡ä»¶æ‰«æ - æœ€å¤§æ–‡ä»¶æ•°: {max_files}, æœ€å¤§æ·±åº¦: {max_depth}")

    def _scan_folder_limited(folder_id, current_path="", depth=0):
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        check_task_cancelled()

        # æ£€æŸ¥æ·±åº¦é™åˆ¶
        if depth >= max_depth:
            logging.info(f"â¹ï¸ è¾¾åˆ°æœ€å¤§æ‰«ææ·±åº¦ {max_depth}ï¼Œåœæ­¢æ‰«æ: {current_path}")
            return

        # æ£€æŸ¥æ–‡ä»¶æ•°é‡é™åˆ¶
        if len(file_list) >= max_files:
            logging.info(f"â¹ï¸ è¾¾åˆ°æœ€å¤§æ–‡ä»¶æ•°é‡ {max_files}ï¼Œåœæ­¢æ‰«æ")
            return

        if not current_path:
            current_path = get_folder_full_path(folder_id)
            logging.info(f"ğŸ” æ™ºèƒ½é‡å‘½åæ‰«æ - æ ¹è·¯å¾„: {current_path}")

        try:
            # è·å–æ–‡ä»¶å¤¹å†…å®¹
            all_files = get_all_files_in_folder(folder_id, limit=100, check_cancellation=True)
            logging.info(f"ğŸ“‚ æ™ºèƒ½é‡å‘½åæ‰«æ {folder_id} ({current_path}) - {len(all_files)} ä¸ªé¡¹ç›® (æ·±åº¦: {depth})")

            # åˆ†ç¦»è§†é¢‘æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
            video_files_in_folder = []
            subfolders = []

            for file_item in all_files:
                if len(file_list) >= max_files:
                    break

                if file_item['type'] == 0:  # æ–‡ä»¶
                    _, ext = os.path.splitext(file_item['filename'])
                    if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                        video_files_in_folder.append(file_item)
                elif file_item['type'] == 1:  # æ–‡ä»¶å¤¹
                    subfolders.append(file_item)

            # å¤„ç†å½“å‰æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘æ–‡ä»¶
            if video_files_in_folder:
                gb_in_bytes = 1024 ** 3
                for file_item in video_files_in_folder:
                    if len(file_list) >= max_files:
                        break

                    bytes_value = file_item['size']
                    gb_value = bytes_value / gb_in_bytes
                    full_file_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']
                    file_path = limit_path_depth(full_file_path, 3)

                    enhanced_file_item = file_item.copy()
                    enhanced_file_item['file_path'] = file_path
                    enhanced_file_item['size_gb'] = f"{gb_value:.1f}GB"

                    file_list.append(enhanced_file_item)

                logging.info(f"âœ… å‘ç° {len(video_files_in_folder)} ä¸ªè§†é¢‘æ–‡ä»¶: {current_path}")

            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹ï¼ˆå¦‚æœè¿˜æ²¡è¾¾åˆ°é™åˆ¶ï¼‰
            if depth < max_depth - 1 and len(file_list) < max_files and subfolders:
                # é™åˆ¶å­æ–‡ä»¶å¤¹æ•°é‡ï¼Œä¼˜å…ˆå¤„ç†å¯èƒ½åŒ…å«æ›´å¤šå†…å®¹çš„æ–‡ä»¶å¤¹
                limited_subfolders = subfolders[:20]  # æœ€å¤šå¤„ç†20ä¸ªå­æ–‡ä»¶å¤¹

                for subfolder in limited_subfolders:
                    if len(file_list) >= max_files:
                        break

                    subfolder_path = os.path.join(current_path, subfolder['filename']) if current_path else subfolder['filename']
                    _scan_folder_limited(subfolder['fileId'], subfolder_path, depth + 1)

                if len(subfolders) > 20:
                    logging.info(f"âš ï¸ å­æ–‡ä»¶å¤¹æ•°é‡ ({len(subfolders)}) è¶…è¿‡é™åˆ¶ï¼Œåªå¤„ç†å‰20ä¸ª")

        except Exception as e:
            logging.error(f"æ™ºèƒ½é‡å‘½åæ‰«ææ–‡ä»¶å¤¹ {folder_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # å¼€å§‹æ‰«æ
    _scan_folder_limited(folder_id)

    logging.info(f"ğŸ¯ æ™ºèƒ½é‡å‘½åæ–‡ä»¶æ‰«æå®Œæˆ - å…±æ‰¾åˆ° {len(file_list)} ä¸ªè§†é¢‘æ–‡ä»¶")


def get_video_files_recursively(folder_id, file_list, current_path="", depth=0, use_concurrent=True):
    """
    é€’å½’è·å–æŒ‡å®šæ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        folder_id (int): è¦æœç´¢çš„æ–‡ä»¶å¤¹ID
        file_list (list): ç”¨äºå­˜å‚¨æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶çš„åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
        current_path (str): å½“å‰æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        depth (int): é€’å½’æ·±åº¦ï¼Œç”¨äºæ§åˆ¶æ—¥å¿—è¾“å‡º
        use_concurrent (bool): æ˜¯å¦ä½¿ç”¨å¹¶å‘ä¼˜åŒ–ï¼Œé»˜è®¤True

    Note:
        æ­¤å‡½æ•°ä¼šä¿®æ”¹ä¼ å…¥çš„file_listå‚æ•°ï¼Œå°†æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶æ·»åŠ åˆ°å…¶ä¸­
    """
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
    check_task_cancelled()

    if not current_path:
        current_path = get_folder_full_path(folder_id)
        logging.info(f"ğŸ” è°ƒè¯• - ä½¿ç”¨get_folder_full_pathè·å–è·¯å¾„: {current_path}")
    else:
        logging.info(f"ğŸ” è°ƒè¯• - ä½¿ç”¨ä¼ é€’çš„è·¯å¾„: {current_path}")

    try:
        # ä½¿ç”¨æœ€å¤§å…è®¸çš„limitå€¼ï¼ˆ100ï¼‰ï¼Œå¦‚æœæœ‰æ›´å¤šæ–‡ä»¶ä¼šè‡ªåŠ¨åˆ†é¡µå¤„ç†
        # åœ¨AIåˆ†æä»»åŠ¡ä¸­å¯ç”¨å–æ¶ˆæ£€æŸ¥
        all_files = get_all_files_in_folder(folder_id, limit=100, check_cancellation=True)

        # è¾“å‡ºæ‰«æè¿›åº¦æ—¥å¿—
        if depth == 0:  # æ ¹çº§åˆ«æ€»æ˜¯è¾“å‡º
            logging.info(f"ğŸ“‚ æ‰«ææ–‡ä»¶å¤¹ {folder_id} ({current_path}) - {len(all_files)} ä¸ªé¡¹ç›®")
        elif len(all_files) > 50:  # å¤§æ–‡ä»¶å¤¹è¾“å‡ºè¿›åº¦
            logging.info(f"ğŸ“‚ æ‰«æå­æ–‡ä»¶å¤¹ {folder_id} ({current_path}) - {len(all_files)} ä¸ªé¡¹ç›®")
        elif depth % 3 == 0:  # æ¯3å±‚è¾“å‡ºä¸€æ¬¡è¿›åº¦
            logging.info(f"ğŸ“‚ æ‰«ææ–‡ä»¶å¤¹ {folder_id} ({current_path}) - {len(all_files)} ä¸ªé¡¹ç›®")

        # æ‰¹é‡å¤„ç†è§†é¢‘æ–‡ä»¶ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        video_files_in_folder = []
        subfolders = []

        for file_item in all_files:
            if file_item['type'] == 0:  # æ–‡ä»¶
                _, ext = os.path.splitext(file_item['filename'])
                if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                    video_files_in_folder.append(file_item)
            elif file_item['type'] == 1:  # æ–‡ä»¶å¤¹
                subfolders.append(file_item)

        # æ‰¹é‡å¤„ç†è§†é¢‘æ–‡ä»¶
        if video_files_in_folder:
            gb_in_bytes = 1024 ** 3
            for file_item in video_files_in_folder:
                bytes_value = file_item['size']
                gb_value = bytes_value / gb_in_bytes
                # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                full_file_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']

                # é™åˆ¶è·¯å¾„æœ€å¤šæ˜¾ç¤ºå€’æ•°ä¸‰å±‚
                file_path = limit_path_depth(full_file_path, 3)

                # åˆ›å»ºå¢å¼ºçš„æ–‡ä»¶é¡¹ï¼Œä¿ç•™åŸæœ‰ä¿¡æ¯å¹¶æ·»åŠ è®¡ç®—å­—æ®µ
                enhanced_file_item = file_item.copy()
                enhanced_file_item['file_path'] = file_path
                enhanced_file_item['size_gb'] = f"{gb_value:.1f}GB"

                file_list.append(enhanced_file_item)

            # è¾“å‡ºè§†é¢‘æ–‡ä»¶å‘ç°æ—¥å¿—
            if depth == 0:  # æ ¹ç›®å½•æ€»æ˜¯è¾“å‡º
                logging.info(f"âœ… å‘ç° {len(video_files_in_folder)} ä¸ªè§†é¢‘æ–‡ä»¶: {current_path}")
            elif len(video_files_in_folder) > 5:  # æœ‰è¾ƒå¤šè§†é¢‘æ–‡ä»¶æ—¶è¾“å‡º
                logging.info(f"âœ… å‘ç° {len(video_files_in_folder)} ä¸ªè§†é¢‘æ–‡ä»¶: {current_path}")
            elif len(video_files_in_folder) > 0 and depth <= 2:  # æµ…å±‚ç›®å½•æœ‰è§†é¢‘æ–‡ä»¶æ—¶è¾“å‡º
                logging.info(f"âœ… å‘ç° {len(video_files_in_folder)} ä¸ªè§†é¢‘æ–‡ä»¶: {current_path}")

        # å¤„ç†å­æ–‡ä»¶å¤¹ - æ ¹æ®æƒ…å†µé€‰æ‹©ä¸²è¡Œæˆ–å¹¶å‘
        if subfolders:
            if subfolders and (depth == 0 or len(subfolders) > 5):
                logging.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹: {current_path}")

            # å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†ï¼ˆé‡æ–°å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆè®¾ç½®ï¼‰
            should_use_concurrent = (
                use_concurrent and
                depth == 0 and  # åªåœ¨æ ¹çº§åˆ«ä½¿ç”¨å¹¶å‘
                len(subfolders) >= 20 and  # 20ä¸ªæ–‡ä»¶å¤¹ä»¥ä¸Šå¯ç”¨å¹¶å‘
                len(subfolders) <= 200  # 200ä¸ªæ–‡ä»¶å¤¹ä»¥ä¸‹ä½¿ç”¨å¹¶å‘
            )

            # è°ƒè¯•æ—¥å¿—
            
            logging.info(f"ğŸ” å¹¶å‘æ¡ä»¶æ£€æŸ¥: use_concurrent={use_concurrent}, depth={depth}, subfolders={len(subfolders)}, should_use_concurrent={should_use_concurrent}")

            if should_use_concurrent:
                logging.info(f"ğŸš€ å¯ç”¨å¹¶å‘å¤„ç†æ¨¡å¼")
                _process_subfolders_concurrent(subfolders, file_list, current_path, depth)
            else:
                logging.info(f"ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆé¿å…APIé¢‘ç‡é™åˆ¶ï¼‰")
                _process_subfolders_sequential(subfolders, file_list, current_path, depth)

        # è¾“å‡ºå®Œæˆç»Ÿè®¡ï¼ˆæ ¹ç›®å½•æˆ–å¤§æ–‡ä»¶å¤¹ï¼‰
        if depth == 0:
            total_videos = len([f for f in file_list if f.get('file_path', '').startswith(current_path or '')])
            logging.info(f"ğŸ æ–‡ä»¶å¤¹æ‰«æå®Œæˆ: {current_path} - å…±å‘ç° {total_videos} ä¸ªè§†é¢‘æ–‡ä»¶")
        elif len(subfolders) > 10:
            logging.info(f"âœ… å­æ–‡ä»¶å¤¹å¤„ç†å®Œæˆ: {current_path} - {len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹")

    except Exception as e:
        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
            raise  # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
        logging.error(f"å¤„ç†æ–‡ä»¶å¤¹ {folder_id} ({current_path}) æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

def _process_subfolders_sequential(subfolders, file_list, current_path, depth):
    """ä¸²è¡Œå¤„ç†å­æ–‡ä»¶å¤¹"""
    for i, file_item in enumerate(subfolders):
        try:
            # åœ¨å¤„ç†æ¯ä¸ªå­æ–‡ä»¶å¤¹å‰æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            check_task_cancelled()

            # è¾“å‡ºå¤„ç†è¿›åº¦ï¼ˆå¤§æ–‡ä»¶å¤¹æˆ–æ ¹ç›®å½•ï¼‰
            if depth == 0 and len(subfolders) > 10 and (i + 1) % 5 == 0:
                logging.info(f"ğŸ“ å¤„ç†è¿›åº¦: {i + 1}/{len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹")

            # æ·»åŠ é¢å¤–å»¶è¿Ÿä»¥è¿›ä¸€æ­¥å‡å°‘APIè°ƒç”¨é¢‘ç‡
            import time
            time.sleep(0.05)  # 50msé¢å¤–å»¶è¿Ÿ

            # æ„å»ºå­æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼ˆé¿å…é‡å¤APIè°ƒç”¨ï¼‰
            subfolder_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']
            # ç¼“å­˜å­æ–‡ä»¶å¤¹è·¯å¾„
            folder_path_cache[file_item['fileId']] = subfolder_path
            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            get_video_files_recursively(file_item['fileId'], file_list, subfolder_path, depth + 1, use_concurrent=False)
        except Exception as e:
            if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                raise  # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
            # å¦‚æœæ˜¯429é”™è¯¯ï¼Œè®°å½•ä½†ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶å¤¹
            if "429" in str(e) or "æ“ä½œé¢‘ç¹" in str(e):
                logging.warning(f"âš ï¸ APIé¢‘ç‡é™åˆ¶ï¼Œè·³è¿‡æ–‡ä»¶å¤¹: {file_item['filename']}")
                continue
            logging.error(f"å¤„ç†å­æ–‡ä»¶å¤¹ {file_item['filename']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue  # ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶å¤¹

def _process_subfolders_concurrent(subfolders, file_list, current_path, depth):
    """å¹¶å‘å¤„ç†å­æ–‡ä»¶å¤¹ï¼ˆä»…ç”¨äºæ ¹çº§åˆ«çš„å¤§æ–‡ä»¶å¤¹ï¼‰"""
    logging.info(f"ğŸš€ ä½¿ç”¨å¹¶å‘æ¨¡å¼å¤„ç† {len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹")

    # é¢„å…ˆç¼“å­˜æ‰€æœ‰å­æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä½¿ç”¨ç®€å•è·¯å¾„æ„å»ºï¼Œé¿å…APIè°ƒç”¨ï¼‰
    for file_item in subfolders:
        subfolder_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']
        folder_path_cache[file_item['fileId']] = subfolder_path

    # ä½¿ç”¨é€‚åº¦çš„å¹¶å‘çº¿ç¨‹æ•°ä»¥å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
    max_workers = min(5, len(subfolders))  # æé«˜åˆ°5ä¸ªçº¿ç¨‹
    completed_count = 0

    def process_single_subfolder(file_item):
        qps_limiter.acquire()
        """å¤„ç†å•ä¸ªå­æ–‡ä»¶å¤¹çš„å‡½æ•°"""
        try:
            check_task_cancelled()
            subfolder_path = folder_path_cache[file_item['fileId']]
            subfolder_files = []

            # QPSæ§åˆ¶å·²ç»åœ¨QPSLimiterä¸­å®ç°ï¼Œæ— éœ€é¢å¤–å»¶è¿Ÿ

            get_video_files_recursively(file_item['fileId'], subfolder_files, subfolder_path, depth + 1, use_concurrent=False)
            return subfolder_files
        except Exception as e:
            if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                raise
            # å¦‚æœæ˜¯429é”™è¯¯ï¼Œè®°å½•ä½†ä¸ä¸­æ–­æ•´ä¸ªå¤„ç†
            if "429" in str(e) or "æ“ä½œé¢‘ç¹" in str(e):
                logging.warning(f"âš ï¸ APIé¢‘ç‡é™åˆ¶: {file_item['filename']} - è·³è¿‡æ­¤æ–‡ä»¶å¤¹")
                return []
            logging.error(f"å¹¶å‘å¤„ç†å­æ–‡ä»¶å¤¹ {file_item['fileId']} å¤±è´¥: {e}")
            return []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # åˆ†æ‰¹æäº¤ä»»åŠ¡ä»¥å‡å°‘å¹¶å‘å‹åŠ›
        batch_size = 5
        for i in range(0, len(subfolders), batch_size):
            batch = subfolders[i:i + batch_size]
            future_to_folder = {executor.submit(process_single_subfolder, folder): folder for folder in batch}

            # æ”¶é›†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            for future in as_completed(future_to_folder):
                folder_item = future_to_folder[future]
                try:
                    subfolder_files = future.result()
                    file_list.extend(subfolder_files)
                    completed_count += 1

                    # è¾“å‡ºè¿›åº¦
                    if completed_count % 5 == 0 or completed_count == len(subfolders):
                        logging.info(f"ğŸ“ å¹¶å‘å¤„ç†è¿›åº¦: {completed_count}/{len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹å®Œæˆ")

                except Exception as e:
                    if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                        logging.info("ğŸ›‘ å¹¶å‘å¤„ç†è¢«ç”¨æˆ·å–æ¶ˆ")
                        raise
                    logging.error(f"å¤„ç†å­æ–‡ä»¶å¤¹ {folder_item['filename']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

            # QPSæ§åˆ¶å·²ç»åœ¨QPSLimiterä¸­å®ç°ï¼Œæ— éœ€æ‰¹æ¬¡é—´å»¶è¿Ÿ


@ensure_valid_access_token
def rename(rename_dict: dict, use_batch_qps=False):
    """
    é‡å‘½åæ–‡ä»¶
    :param rename_dict: é‡å‘½åå­—å…¸ {file_id: new_name}
    :param use_batch_qps: æ˜¯å¦ä½¿ç”¨æ‰¹é‡é‡å‘½åçš„QPSé™åˆ¶ï¼ˆ1 QPSï¼‰
    """

    rename_limiter.acquire()
    current_time = datetime.datetime.now()
    formatted_time = current_time.strftime("%H:%M:%S")
    print("rename:",formatted_time)
    logging.info(f"å¼€å§‹é‡å‘½åæ“ä½œï¼Œé‡å‘½åå­—å…¸: {rename_dict}ï¼Œä½¿ç”¨æ‰¹é‡QPS: {use_batch_qps}")

    url = BASE_API_URL + "/api/v1/file/rename"
    rename_list = []
    for i in rename_dict.keys():
        rename_list.append(f"{i}|{rename_dict[i]}")
    data = {"renameList": rename_list}

    logging.info(f"é‡å‘½åAPI URL: {url}")
    logging.info(f"é‡å‘½åæ•°æ®: {data}")
    logging.info(f"è¯·æ±‚å¤´: {API_HEADERS}")

    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            logging.info(f"å‘é€é‡å‘½åè¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries})")
            # ä½¿ç”¨JSONæ ¼å¼å‘é€è¯·æ±‚ï¼Œç¬¦åˆAPIè¦æ±‚
            r = requests.post(url, json=data, headers=API_HEADERS)
            logging.info(f"HTTPå“åº”çŠ¶æ€ç : {r.status_code}")
            logging.info(f"HTTPå“åº”å†…å®¹: {r.text}")

            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            result = validate_api_response(r)
            logging.info(f"é‡å‘½åAPIè¿”å›ç»“æœ: {result}")
            return result
        except (AccessTokenError, TokenLimitExceededError) as e:
            logging.error(f"è®¿é—®ä»¤ç‰Œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise
        except requests.exceptions.RequestException as e:
            logging.error(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise  # Re-raise the last exception if all retries fail


@ensure_valid_access_token
def detail(file_id):
    # ä½¿ç”¨é€šç”¨QPSé™åˆ¶å™¨æ§åˆ¶detail APIè°ƒç”¨é¢‘ç‡
    qps_limiter.acquire()

    # æ·»åŠ é¢å¤–å»¶è¿Ÿä»¥è¿›ä¸€æ­¥å‡å°‘APIè°ƒç”¨é¢‘ç‡
    import time
    time.sleep(0.1)  # 100msé¢å¤–å»¶è¿Ÿ

    # current_time = datetime.datetime.now()
    # formatted_time = current_time.strftime("%H:%M:%S")
    # print("detail:",formatted_time)
    url = BASE_API_URL + "/api/v1/file/detail"
    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            data = {"fileID": file_id}
            r = requests.get(url, data=data, headers=API_HEADERS)
            data = validate_api_response(r)
            if data["trashed"] == 1:
                data["trashed"] = True
            else:
                data["trashed"] = False
            if data["type"] == 1:
                data["type"] = "folder"
            else:
                data["type"] = "file"
            return data
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                raise  # Re-raise the last exception if all retries fail


def trash(file_id_list: list):
    delete_limiter.acquire()
    # current_time = datetime.datetime.now()
    # formatted_time = current_time.strftime("%H:%M:%S")
    # print("delete:",formatted_time)
    url = BASE_API_URL + "/api/v1/file/trash"
    data = {"fileIDs": file_id_list}
    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨JSONæ ¼å¼å‘é€è¯·æ±‚ï¼Œç¬¦åˆAPIè¦æ±‚
            r = requests.post(url, json=data, headers=API_HEADERS)
            logging.info(f"deleteAPI HTTPå“åº”çŠ¶æ€ç : {r.status_code}")
            logging.info(f"deleteAPI HTTPå“åº”å†…å®¹: {r.text}")
            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)

            # æ£€æŸ¥APIå“åº”
            response_data = json.loads(r.text)
            if response_data.get("code") == 0:
                logging.info(f"deleteæ“ä½œæˆåŠŸ: {response_data}")
                return {"success": True, "message": "deleteæˆåŠŸ"}
            else:
                error_message = response_data.get("message", "æœªçŸ¥é”™è¯¯")
                logging.error(f"deleteæ“ä½œå¤±è´¥: {response_data}")
                return {"success": False, "message": error_message, "response": response_data}

        except requests.exceptions.RequestException as e:
            logging.error(f"deleteè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                return {"success": False, "message": f"è¯·æ±‚å¤±è´¥: {str(e)}"}


def delete(file_id_list: list):
    delete_limiter.acquire()
    # current_time = datetime.datetime.now()
    # formatted_time = current_time.strftime("%H:%M:%S")
    # print("delete:",formatted_time)

    url = BASE_API_URL + "/api/v1/file/delete"
    data = {"fileIDs": file_id_list}
    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            trash(file_id_list)
            r = requests.post(url, json=data, headers=API_HEADERS)
            logging.info(f"deleteAPI HTTPå“åº”çŠ¶æ€ç : {r.status_code}")
            logging.info(f"deleteAPI HTTPå“åº”å†…å®¹: {r.text}")
            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)

            # æ£€æŸ¥APIå“åº”
            response_data = json.loads(r.text)
            if response_data.get("code") == 0:
                logging.info(f"deleteæ“ä½œæˆåŠŸ: {response_data}")
                return {"success": True, "message": "deleteæˆåŠŸ"}
            else:
                error_message = response_data.get("message", "æœªçŸ¥é”™è¯¯")
                logging.error(f"deleteæ“ä½œå¤±è´¥: {response_data}")
                return {"success": False, "message": error_message, "response": response_data}
        except requests.exceptions.RequestException as e:
            logging.error(f"deleteè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                return {"success": False, "message": f"è¯·æ±‚å¤±è´¥: {str(e)}"}


@ensure_valid_access_token
def move(file_id_list: list, to_parent_file_id: int):
    move_limiter.acquire()  # ä½¿ç”¨ä¸“ç”¨çš„moveé™æµå™¨ï¼ˆ10 QPSï¼‰
    current_time = datetime.datetime.now()
    formatted_time = current_time.strftime("%H:%M:%S")
    print("move:",formatted_time)
    url = BASE_API_URL + "/api/v1/file/move"
    data = {"fileIDs": file_id_list,"toParentFileID": to_parent_file_id}
    max_retries = CLOUD_API_MAX_RETRIES
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨JSONæ ¼å¼å‘é€è¯·æ±‚ï¼Œç¬¦åˆAPIè¦æ±‚
            r = requests.post(url, json=data, headers=API_HEADERS)
            logging.info(f"ç§»åŠ¨API HTTPå“åº”çŠ¶æ€ç : {r.status_code}")
            logging.info(f"ç§»åŠ¨API HTTPå“åº”å†…å®¹: {r.text}")
            r.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)

            # æ£€æŸ¥APIå“åº”
            response_data = json.loads(r.text)
            if response_data.get("code") == 0:
                logging.info(f"ç§»åŠ¨æ“ä½œæˆåŠŸ: {response_data}")
                return {"success": True, "message": "ç§»åŠ¨æˆåŠŸ"}
            else:
                error_message = response_data.get("message", "æœªçŸ¥é”™è¯¯")
                logging.error(f"ç§»åŠ¨æ“ä½œå¤±è´¥: {response_data}")
                return {"success": False, "message": error_message, "response": response_data}

        except requests.exceptions.RequestException as e:
            logging.error(f"ç§»åŠ¨è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(CLOUD_API_RETRY_DELAY)
            else:
                return {"success": False, "message": f"è¯·æ±‚å¤±è´¥: {str(e)}"}



def sanitize_filename(filename):
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç¡®ä¿Windowså’Œå…¶ä»–ç³»ç»Ÿå…¼å®¹æ€§

    Args:
        filename (str): åŸå§‹æ–‡ä»¶å

    Returns:
        str: æ¸…ç†åçš„æ–‡ä»¶å
    """
    # ç§»é™¤æˆ–æ›¿æ¢ä¸å…è®¸çš„å­—ç¬¦
    # Windowsæ–‡ä»¶åä¸å…è®¸çš„å­—ç¬¦: < > : " / \ | ? *
    # Linux/macOSæ–‡ä»¶åä¸å…è®¸çš„å­—ç¬¦: /
    # è¿™é‡Œæˆ‘ä»¬å¤„ç†æ‰€æœ‰å¸¸è§çš„ä¸å…è®¸å­—ç¬¦
    sanitized = re.sub(r'[<>:"/\\|?*]', '', filename)

    # Windowsä¿ç•™åç§°æ£€æŸ¥ï¼ˆCON, PRN, AUX, NUL, COM1-9, LPT1-9ï¼‰
    reserved_names = {
        'CON', 'PRN', 'AUX', 'NUL',
        'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9',
        'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'
    }

    name_without_ext = os.path.splitext(sanitized)[0].upper()
    if name_without_ext in reserved_names:
        sanitized = f"_{sanitized}"  # æ·»åŠ å‰ç¼€é¿å…ä¿ç•™åç§°å†²çª

    # ç§»é™¤æ–‡ä»¶åå¼€å¤´å’Œç»“å°¾çš„ç©ºæ ¼å’Œç‚¹ï¼ˆWindowsä¸å…è®¸ï¼‰
    sanitized = sanitized.strip(' .')

    # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
    if not sanitized:
        sanitized = "unnamed_file"

    # ç¡®ä¿æ–‡ä»¶åé•¿åº¦ä¸è¶…è¿‡255ä¸ªå­—ç¬¦ï¼ˆå¸¸è§æ–‡ä»¶ç³»ç»Ÿé™åˆ¶ï¼‰
    if len(sanitized) > 255:
        name, ext = os.path.splitext(sanitized)
        sanitized = name[:(255 - len(ext))] + ext

    return sanitized


def safe_encode_for_windows(text):
    """
    å®‰å…¨ç¼–ç æ–‡æœ¬ä»¥é¿å…Windowsç³»ç»Ÿä¸­çš„å­—ç¬¦ç¼–ç é—®é¢˜

    Args:
        text (str): éœ€è¦ç¼–ç çš„æ–‡æœ¬

    Returns:
        str: å®‰å…¨ç¼–ç åçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        text = str(text)

    try:
        # å°è¯•ç¼–ç ä¸ºUTF-8å¹¶è§£ç ï¼Œç¡®ä¿å­—ç¬¦ä¸²æ˜¯æœ‰æ•ˆçš„UTF-8
        text.encode('utf-8').decode('utf-8')
        return text
    except UnicodeEncodeError:
        # å¦‚æœç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†ç­–ç•¥
        return text.encode('utf-8', errors='replace').decode('utf-8')
    except UnicodeDecodeError:
        # å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†ç­–ç•¥
        return text.encode('utf-8', errors='ignore').decode('utf-8')


def safe_log_message(message):
    """
    å®‰å…¨å¤„ç†æ—¥å¿—æ¶ˆæ¯ï¼Œé¿å…Windowsç³»ç»Ÿä¸­çš„ç¼–ç é”™è¯¯

    Args:
        message (str): æ—¥å¿—æ¶ˆæ¯

    Returns:
        str: å®‰å…¨å¤„ç†åçš„æ—¥å¿—æ¶ˆæ¯
    """
    try:
        # ç¡®ä¿æ¶ˆæ¯æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(message, str):
            message = str(message)

        # ä½¿ç”¨å®‰å…¨ç¼–ç å¤„ç†
        safe_message = safe_encode_for_windows(message)

        # æ›¿æ¢å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
        # æŸäº›æ§åˆ¶å­—ç¬¦åœ¨Windowsæ§åˆ¶å°ä¸­å¯èƒ½å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
        safe_message = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '?', safe_message)

        return safe_message
    except Exception:
        # å¦‚æœæ‰€æœ‰å¤„ç†éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå®‰å…¨çš„é»˜è®¤æ¶ˆæ¯
        return "[æ—¥å¿—æ¶ˆæ¯ç¼–ç é”™è¯¯]"



# æ™ºèƒ½æ–‡ä»¶åˆ†ç»„è¾…åŠ©å‡½æ•°
def group_files_by_folder(files):
    """æŒ‰æ–‡ä»¶å¤¹è·¯å¾„åˆ†ç»„æ–‡ä»¶"""
    folder_groups = {}
    for video_file in files:
        file_path = video_file.get('file_path', video_file['filename'])
        # æå–å­æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå»æ‰æ–‡ä»¶åï¼‰
        folder_path = os.path.dirname(file_path)
        if folder_path not in folder_groups:
            folder_groups[folder_path] = []
        folder_groups[folder_path].append(video_file)
    return folder_groups

def split_files_into_batches(files, batch_size):
    """å°†æ–‡ä»¶åˆ—è¡¨æ‹†åˆ†ä¸ºæ‰¹æ¬¡"""
    return [files[i:i + batch_size] for i in range(0, len(files), batch_size)]



def merge_duplicate_named_groups(groups):
    """åˆå¹¶å…·æœ‰ç›¸åŒåç§°çš„åˆ†ç»„ï¼ˆè§£å†³æ‰¹å¤„ç†å¯¼è‡´çš„é‡å¤åˆ†ç»„é—®é¢˜ï¼‰"""
    if not groups or len(groups) < 2:
        return groups

    logging.info(f"ğŸ”„ å¼€å§‹åˆå¹¶é‡å¤å‘½åçš„åˆ†ç»„: {len(groups)} ä¸ªåˆ†ç»„")

    # æŒ‰åˆ†ç»„åç§°åˆ†ç±»
    groups_by_name = {}
    for group in groups:
        group_name = group.get('group_name', '')
        if not group_name:
            continue

        if group_name not in groups_by_name:
            groups_by_name[group_name] = []
        groups_by_name[group_name].append(group)

    # åˆå¹¶åŒååˆ†ç»„
    merged_groups = []
    for group_name, same_name_groups in groups_by_name.items():
        if len(same_name_groups) == 1:
            # åªæœ‰ä¸€ä¸ªåˆ†ç»„ï¼Œç›´æ¥æ·»åŠ 
            merged_groups.append(same_name_groups[0])
        else:
            # æœ‰å¤šä¸ªåŒååˆ†ç»„ï¼Œéœ€è¦åˆå¹¶
            logging.info(f"ğŸ”— åˆå¹¶é‡å¤åˆ†ç»„ '{group_name}': {len(same_name_groups)} ä¸ªåˆ†ç»„")

            # åˆå¹¶æ‰€æœ‰æ–‡ä»¶IDå’Œæ–‡ä»¶å
            all_file_ids = []
            all_file_names = []
            all_files = []
            folder_paths = set()

            for group in same_name_groups:
                file_ids = group.get('fileIds', [])
                file_names = group.get('file_names', [])
                files = group.get('files', [])
                folder_path = group.get('folder_path', '')

                all_file_ids.extend(file_ids)
                all_file_names.extend(file_names)
                all_files.extend(files)

                if folder_path:
                    folder_paths.add(folder_path)

            # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
            seen_ids = set()
            unique_file_ids = []
            for file_id in all_file_ids:
                if file_id not in seen_ids:
                    unique_file_ids.append(file_id)
                    seen_ids.add(file_id)

            seen_names = set()
            unique_file_names = []
            for file_name in all_file_names:
                if file_name not in seen_names:
                    unique_file_names.append(file_name)
                    seen_names.add(file_name)

            seen_files = set()
            unique_files = []
            for file_item in all_files:
                file_key = str(file_item)  # ç®€å•çš„å»é‡æ–¹å¼
                if file_key not in seen_files:
                    unique_files.append(file_item)
                    seen_files.add(file_key)

            # åˆ›å»ºåˆå¹¶åçš„åˆ†ç»„
            merged_group = {
                'group_name': group_name,
                'fileIds': unique_file_ids,
                'file_names': unique_file_names,
                'files': unique_files,
                'folder_path': '; '.join(folder_paths) if len(folder_paths) > 1 else list(folder_paths)[0] if folder_paths else '',
                'file_count': len(unique_file_ids)
            }

            merged_groups.append(merged_group)
            logging.info(f"âœ… åˆå¹¶å®Œæˆ '{group_name}': {len(unique_file_ids)} ä¸ªæ–‡ä»¶")

    logging.info(f"ğŸ¯ é‡å¤åˆ†ç»„åˆå¹¶å®Œæˆ: {len(groups)} â†’ {len(merged_groups)} ä¸ªåˆ†ç»„")
    return merged_groups

def merge_same_series_groups(groups):
    """ä½¿ç”¨AIæ™ºèƒ½åˆå¹¶åŒä¸€ç³»åˆ—çš„åˆ†ç»„ï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†"""
    if not groups or len(groups) < 2:
        return groups

    logging.info(f"ğŸ”„ å¼€å§‹AIæ™ºèƒ½åˆå¹¶åˆ†ç»„: è¾“å…¥ {len(groups)} ä¸ªåˆ†ç»„")

    # å¦‚æœåˆ†ç»„æ•°é‡å¤ªå¤šï¼Œå…ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•é¢„å¤„ç†
    if len(groups) > 20:
        logging.info(f"âš ï¸ åˆ†ç»„æ•°é‡è¿‡å¤š ({len(groups)} ä¸ª)ï¼Œå…ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•é¢„å¤„ç†")
        groups = merge_same_series_groups_traditional(groups)
        if len(groups) < 2:
            return groups

    # å‡†å¤‡åˆ†ç»„ä¿¡æ¯ä¾›AIåˆ†æ
    group_info = []
    for i, group in enumerate(groups):
        group_name = group.get('group_name', f'åˆ†ç»„{i+1}')
        file_count = len(group.get('fileIds', []))
        file_names = group.get('file_names', [])

        # å–å‰5ä¸ªæ–‡ä»¶åä½œä¸ºç¤ºä¾‹ï¼Œæä¾›æ›´å¤šä¿¡æ¯å¸®åŠ©AIåˆ¤æ–­
        sample_files = file_names[:5] if file_names else []

        # åˆ†æå†…å®¹ç±»å‹
        content_type = "unknown"
        movie_indicators = ["å‰§åœºç‰ˆ", "ç”µå½±", "Movie", "ä»£å·ï¼šç™½", "Code:", "å‰§åœº", "å¤§ç”µå½±"]
        tv_indicators = ["ç¬¬", "å­£", "é›†", "S0", "E0", "Season", "Episode", "EP"]

        # æ£€æŸ¥æ˜¯å¦ä¸ºç”µå½±/å‰§åœºç‰ˆ
        if any(indicator in name for indicator in movie_indicators for name in sample_files):
            content_type = "movie"
        # æ£€æŸ¥æ˜¯å¦ä¸ºç”µè§†å‰§é›†
        elif any(indicator in name for indicator in tv_indicators for name in sample_files):
            content_type = "tv_series"
        # æ ¹æ®åˆ†ç»„åç§°åˆ¤æ–­
        elif any(indicator in group_name for indicator in movie_indicators):
            content_type = "movie"
        elif any(indicator in group_name for indicator in tv_indicators):
            content_type = "tv_series"
        elif len(sample_files) > 0:
            # æ ¹æ®æ–‡ä»¶åæ¨¡å¼åˆ¤æ–­
            first_file = sample_files[0]
            if "(" in first_file and ")" in first_file and len(sample_files) < 5:
                # åŒ…å«å¹´ä»½ä¸”æ–‡ä»¶æ•°é‡å°‘çš„é€šå¸¸æ˜¯ç”µå½±
                content_type = "movie"
            elif len(sample_files) > 10:
                # æ–‡ä»¶æ•°é‡å¤šçš„é€šå¸¸æ˜¯ç”µè§†å‰§é›†
                content_type = "tv_series"

        group_info.append({
            'name': group_name,
            'file_count': file_count,
            'sample_files': sample_files,
            'content_type': content_type,
            'series_indicators': {
                'has_sequels': any(str(i) in name for i in range(2, 10) for name in sample_files),
                'has_seasons': any("å­£" in name or "Season" in name for name in sample_files),
                'has_episodes': any("é›†" in name or "Episode" in name for name in sample_files)
            }
        })

    # æ„å»ºAIåˆ†æçš„è¾“å…¥
    analysis_input = f"""è¯·åˆ†æä»¥ä¸‹åˆ†ç»„åˆ—è¡¨ï¼Œåˆ¤æ–­å“ªäº›åˆ†ç»„åº”è¯¥åˆå¹¶ï¼š

é‡è¦æé†’ï¼š
- ç»å¯¹ä¸è¦å°†ç”µå½±å’Œç”µè§†å‰§é›†åˆå¹¶ï¼ˆå³ä½¿æ˜¯åŒä¸€IPï¼‰
- ç»å¯¹ä¸è¦å°†å‰§åœºç‰ˆç”µå½±å’Œç”µè§†åŠ¨ç”»åˆå¹¶
- ç»å¯¹ä¸è¦å°†ä¸åŒçš„åŠ¨æ¼«/ç”µå½±ç³»åˆ—åˆå¹¶
- **ğŸš« ç»å¯¹ç¦æ­¢å°†ä¸åŒå­£çš„ç”µè§†å‰§åˆå¹¶ï¼æ¯å­£å¿…é¡»ä¿æŒç‹¬ç«‹ï¼**
- åªæœ‰çœŸæ­£å±äºåŒä¸€ç³»åˆ—ä¸”å†…å®¹ç±»å‹ç›¸åŒçš„æ‰èƒ½åˆå¹¶

**ğŸš« ä¸¥æ ¼ç¦æ­¢çš„åˆå¹¶è¡Œä¸ºï¼š**
- âŒ è€å‹è®° S01 + è€å‹è®° S02 â†’ è€å‹è®° S01-S02 (ç»å¯¹ç¦æ­¢ï¼)
- âŒ æƒåŠ›çš„æ¸¸æˆ S01 + æƒåŠ›çš„æ¸¸æˆ S02 â†’ æƒåŠ›çš„æ¸¸æˆ S01-S02 (ç»å¯¹ç¦æ­¢ï¼)
- âŒ ä»»ä½• "S01-S10" è¿™æ ·çš„è·¨å­£åˆå¹¶æ ¼å¼ (ç»å¯¹ç¦æ­¢ï¼)

**âœ… æ­£ç¡®çš„å¤„ç†æ–¹å¼ï¼š**
- âœ… è€å‹è®° S01 ä¿æŒç‹¬ç«‹
- âœ… è€å‹è®° S02 ä¿æŒç‹¬ç«‹
- âœ… æ¯ä¸ªå­£åº¦éƒ½æ˜¯ç‹¬ç«‹çš„åˆ†ç»„

ç‰¹åˆ«æ³¨æ„ï¼š
- "ä»£å·ï¼šç™½"ã€"å‰§åœºç‰ˆ"ç­‰æ˜¯ç”µå½±ï¼Œä¸èƒ½ä¸ç”µè§†å‰§é›†åˆå¹¶
- "S01"ã€"Season"ç­‰æ˜¯ç”µè§†å‰§é›†ï¼Œä¸èƒ½ä¸ç”µå½±åˆå¹¶
- åŒä¸€IPçš„ç”µå½±å’Œç”µè§†å‰§åº”è¯¥åˆ†å¼€ç®¡ç†

åˆ†ç»„åˆ—è¡¨: {json.dumps(group_info, ensure_ascii=False, indent=2)}

è¯·ä¸¥æ ¼æŒ‰ç…§è§„åˆ™åˆ†æï¼Œå¦‚æœä¸ç¡®å®šæ˜¯å¦åº”è¯¥åˆå¹¶ï¼Œè¯·é€‰æ‹©ä¸åˆå¹¶ã€‚"""

    try:
        # è°ƒç”¨AIè¿›è¡Œåˆ†ç»„åˆå¹¶åˆ†æ
        merge_result = extract_movie_info_from_filename_enhanced(analysis_input, GROUP_MERGE_PROMPT, GROUPING_MODEL)

        if not merge_result or not isinstance(merge_result, dict):
            logging.warning("AIåˆ†ç»„åˆå¹¶åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return merge_same_series_groups_traditional(groups)

        merges = merge_result.get('merges', [])
        if not merges:
            logging.info("ğŸ¤– AIåˆ¤æ–­æ— éœ€åˆå¹¶åˆ†ç»„")
            return groups

        # æ‰§è¡ŒAIå»ºè®®çš„åˆå¹¶
        merged_groups = []
        processed_groups = set()

        for merge in merges:
            merged_name = merge.get('merged_name', '')
            groups_to_merge = merge.get('groups_to_merge', [])
            reason = merge.get('reason', '')

            if len(groups_to_merge) < 2:
                continue

            # ğŸš« ä»£ç çº§åˆ«æ£€æŸ¥ï¼šé˜»æ­¢ä¸åŒå­£çš„åˆå¹¶
            if 'S' in merged_name and '-S' in merged_name:
                logging.warning(f"ğŸš« é˜»æ­¢è·¨å­£åˆå¹¶: {merged_name} - ä¸åŒå­£ä¸èƒ½åˆå¹¶ï¼")
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸åŒå­£çš„åˆ†ç»„
            season_numbers = set()
            for group_name in groups_to_merge:
                import re
                season_match = re.search(r'S(\d+)', group_name)
                if season_match:
                    season_numbers.add(season_match.group(1))

            if len(season_numbers) > 1:
                logging.warning(f"ğŸš« é˜»æ­¢è·¨å­£åˆå¹¶: {groups_to_merge} - åŒ…å«ä¸åŒå­£ {season_numbers}ï¼")
                continue

            logging.info(f"ğŸ¤– AIå»ºè®®åˆå¹¶: {groups_to_merge} -> {merged_name} (ç†ç”±: {reason})")

            # æ‰¾åˆ°è¦åˆå¹¶çš„åˆ†ç»„
            target_groups = []
            for group in groups:
                if group.get('group_name') in groups_to_merge:
                    target_groups.append(group)
                    processed_groups.add(group.get('group_name'))

            if len(target_groups) >= 2:
                # æ‰§è¡Œåˆå¹¶
                merged_group = merge_groups(target_groups, merged_name)
                merged_groups.append(merged_group)
                logging.info(f"âœ… æˆåŠŸåˆå¹¶: {merged_name} ({len(merged_group.get('fileIds', []))} ä¸ªæ–‡ä»¶)")

        # æ·»åŠ æœªè¢«åˆå¹¶çš„åˆ†ç»„
        for group in groups:
            if group.get('group_name') not in processed_groups:
                merged_groups.append(group)

        logging.info(f"ğŸ¯ AIæ™ºèƒ½åˆå¹¶å®Œæˆ: {len(groups)} -> {len(merged_groups)} ä¸ªåˆ†ç»„")
        return merged_groups

    except Exception as e:
        logging.error(f"AIåˆ†ç»„åˆå¹¶å‡ºé”™: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        return merge_same_series_groups_traditional(groups)


def merge_same_series_groups_traditional(groups):
    """ä¼ ç»Ÿçš„åŸºäºå­—ç¬¦ä¸²åŒ¹é…çš„åˆ†ç»„åˆå¹¶æ–¹æ³•"""
    if not groups:
        return groups

    logging.info(f"ğŸ”„ å¼€å§‹ä¼ ç»Ÿæ–¹æ³•åˆå¹¶åˆ†ç»„: è¾“å…¥ {len(groups)} ä¸ªåˆ†ç»„")

    # æŒ‰ç³»åˆ—åç§°åˆ†ç»„
    series_groups = {}
    for group in groups:
        group_name = group.get('group_name', '')
        if not group_name:
            continue

        # æå–ç³»åˆ—åŸºç¡€åç§°ï¼ˆå»æ‰æ•°å­—ã€å¹´ä»½ç­‰ï¼‰
        base_series_name = extract_series_base_name(group_name)

        if base_series_name not in series_groups:
            series_groups[base_series_name] = []
        series_groups[base_series_name].append(group)

    # åˆå¹¶åŒä¸€ç³»åˆ—çš„åˆ†ç»„
    merged_groups = []
    for base_name, group_list in series_groups.items():
        if len(group_list) == 1:
            # åªæœ‰ä¸€ä¸ªåˆ†ç»„ï¼Œç›´æ¥ä¿ç•™
            merged_groups.append(group_list[0])
        else:
            # å¤šä¸ªåˆ†ç»„ï¼Œéœ€è¦åˆå¹¶
            logging.info(f"ğŸ”— ä¼ ç»Ÿæ–¹æ³•åˆå¹¶ç³»åˆ— '{base_name}': {len(group_list)} ä¸ªåˆ†ç»„")
            merged_group = merge_groups(group_list, f"{base_name}ç³»åˆ—")
            merged_groups.append(merged_group)

    logging.info(f"ğŸ¯ ä¼ ç»Ÿæ–¹æ³•åˆå¹¶å®Œæˆ: è¾“å‡º {len(merged_groups)} ä¸ªåˆ†ç»„")
    return merged_groups


def merge_groups(group_list, merged_name):
    """åˆå¹¶å¤šä¸ªåˆ†ç»„ä¸ºä¸€ä¸ªåˆ†ç»„"""
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶IDå’Œæ–‡ä»¶å
    merged_file_ids = []
    merged_file_names = []
    folder_paths = []

    for group in group_list:
        merged_file_ids.extend(group.get('fileIds', []))
        merged_file_names.extend(group.get('file_names', []))
        folder_path = group.get('folder_path', '')
        if folder_path and folder_path not in folder_paths:
            folder_paths.append(folder_path)

    # å»é‡æ–‡ä»¶IDï¼ˆé˜²æ­¢é‡å¤ï¼‰
    unique_file_ids = list(dict.fromkeys(merged_file_ids))
    unique_file_names = list(dict.fromkeys(merged_file_names))

    # åˆ›å»ºåˆå¹¶åçš„åˆ†ç»„
    merged_group = {
        'group_name': merged_name,
        'fileIds': unique_file_ids,
        'file_names': unique_file_names,
        'folder_path': ' + '.join(folder_paths) if folder_paths else '',
        'merged_from': [g.get('group_name', '') for g in group_list]
    }

    return merged_group

def extract_series_base_name(group_name):
    """
    æå–ç³»åˆ—çš„åŸºç¡€åç§°ï¼Œç”¨äºè¯†åˆ«åŒä¸€ç³»åˆ—çš„ä¸åŒåˆ†ç»„

    Args:
        group_name (str): åˆ†ç»„åç§°

    Returns:
        str: æå–çš„åŸºç¡€ç³»åˆ—åç§°

    Examples:
        "å®å¯æ¢¦ç³»åˆ—" -> "å®å¯æ¢¦"
        "æµ·è´¼ç‹ç”µå½±ç³»åˆ—" -> "æµ·è´¼ç‹"
        "é¾™ç Zåˆé›†" -> "é¾™ç "
    """
    # ç§»é™¤å¸¸è§çš„ç³»åˆ—åç¼€
    series_suffixes = [
        'ç³»åˆ—', 'åˆé›†', 'å…¨é›†', 'ç”µå½±ç³»åˆ—', 'å‰§åœºç‰ˆç³»åˆ—', 'ç‰¹åˆ«ç¯‡',
        'Series', 'Collection', 'Complete', 'Movies', 'Films'
    ]
    base_name = group_name.strip()

    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…è¾ƒé•¿çš„åç¼€
    series_suffixes.sort(key=len, reverse=True)

    for suffix in series_suffixes:
        if base_name.endswith(suffix):
            base_name = base_name[:-len(suffix)].strip()
            break  # åªç§»é™¤ç¬¬ä¸€ä¸ªåŒ¹é…çš„åç¼€

    # ç§»é™¤å¸¸è§çš„ç³»åˆ—æ ‡è¯†ç¬¦å’Œæ•°å­—
    # ä¿ç•™æ ¸å¿ƒåç§°ï¼Œä¾‹å¦‚: "é¾™ç Z" -> "é¾™ç ", "æµ·è´¼ç‹" -> "æµ·è´¼ç‹"
    base_name = re.sub(r'\s*ç¬¬[0-9]+[éƒ¨å­£æœŸ]\s*', '', base_name)  # ç§»é™¤"ç¬¬Xéƒ¨/å­£/æœŸ"
    base_name = re.sub(r'\s*[0-9]+\s*', '', base_name)          # ç§»é™¤ç‹¬ç«‹æ•°å­—
    base_name = re.sub(r'\s*[Z]\s*', '', base_name)             # ç§»é™¤å¸¸è§çš„ç³»åˆ—æ ‡è¯†ç¬¦Z
    base_name = base_name.strip()

    # å¦‚æœå¤„ç†åä¸ºç©ºï¼Œè¿”å›åŸå§‹åç§°
    if not base_name:
        base_name = group_name.strip()

    logging.debug(f"ç³»åˆ—åç§°æå–: '{group_name}' -> '{base_name}'")
    return base_name

def process_files_for_grouping(files, source_name):
    """å¤„ç†æ–‡ä»¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„ - ä¼˜åŒ–ç‰ˆ"""
    if not files:
        return []

    logging.info(f"ğŸ”„ å¤„ç† '{source_name}': {len(files)} ä¸ªæ–‡ä»¶")

    try:
        check_task_cancelled()
    except:
        pass

    # æ‰¹æ¬¡å¤„ç†é€»è¾‘ - ä¼˜åŒ–APIè°ƒç”¨æ¬¡æ•°
    MAX_BATCH_SIZE = CHUNK_SIZE  # å¢åŠ æ‰¹å¤„ç†å¤§å°ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
    if len(files) > MAX_BATCH_SIZE:
        return _process_files_in_batches(files, MAX_BATCH_SIZE)
    else:
        return _process_single_batch(files)


def _process_files_in_batches(files, batch_size):
    """åˆ†æ‰¹å¤„ç†æ–‡ä»¶"""
    batches = split_files_into_batches(files, batch_size)
    logging.info(f"ğŸ“¦ åˆ†æ‰¹å¤„ç†: {len(batches)} æ‰¹")

    all_groups = []
    for i, batch_files in enumerate(batches):
        try:
            check_task_cancelled()
        except:
            pass

        logging.info(f"ğŸ“¦ å¤„ç†ç¬¬ {i+1}/{len(batches)} æ‰¹: {len(batch_files)} ä¸ªæ–‡ä»¶")
        batch_groups = _call_ai_for_grouping(batch_files)

        if batch_groups:
            all_groups.extend(batch_groups if isinstance(batch_groups, list) else [batch_groups])

        logging.info(f"âœ… ç¬¬ {i+1} æ‰¹å®Œæˆ: {len(batch_groups) if batch_groups else 0} ä¸ªåˆ†ç»„")

    return all_groups


def _process_single_batch(files):
    """å¤„ç†å•æ‰¹æ–‡ä»¶"""
    logging.info(f"ğŸ“Š å•æ‰¹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
    return _call_ai_for_grouping(files)


def _call_ai_for_grouping(files):
    """è°ƒç”¨AIè¿›è¡Œåˆ†ç»„å¹¶éªŒè¯ç»“æœ"""
    file_list = [{'fileId': f['fileId'], 'filename': f['filename']} for f in files]
    user_input = repr(file_list)

    logging.info(f"ğŸ¤– å¼€å§‹AIåˆ†ç»„åˆ†æ: {len(files)} ä¸ªæ–‡ä»¶")
    start_time = time.time()

    try:
        raw_result = extract_movie_info_from_filename_enhanced(user_input, MAGIC_PROMPT, GROUPING_MODEL)
        process_time = time.time() - start_time

        if raw_result:
            logging.info(f"â±ï¸ AIåˆ†ç»„è€—æ—¶: {process_time:.2f}ç§’ - æˆåŠŸ")
            # éªŒè¯å’Œå¢å¼ºåˆ†ç»„ç»“æœ
            return _validate_and_enhance_groups(raw_result, files, "AIåˆ†ç»„")
        else:
            logging.warning(f"â±ï¸ AIåˆ†ç»„è€—æ—¶: {process_time:.2f}ç§’ - æ— ç»“æœ")
            return []
    except Exception as e:
        process_time = time.time() - start_time
        logging.error(f"âŒ AIåˆ†ç»„å¤±è´¥: {process_time:.2f}ç§’ - {e}")
        return []


# å¤„ç†AIè¿”å›çš„åˆ†ç»„ç»“æœ - ç§»åˆ°ç‹¬ç«‹å‡½æ•°
def _validate_and_enhance_groups(raw_groups, files, source_name):
    """éªŒè¯å’Œå¢å¼ºAIåˆ†ç»„ç»“æœ"""
    movie_info = []
    if raw_groups:
        if isinstance(raw_groups, list):
            if len(raw_groups) > 0 and isinstance(raw_groups[0], list):
                movie_info = raw_groups[0]
            else:
                movie_info = raw_groups
        else:
            movie_info = [raw_groups] if raw_groups else []

    # éªŒè¯å’Œå¢å¼ºåˆ†ç»„ä¿¡æ¯
    enhanced_groups = []
    if movie_info and isinstance(movie_info, list):
        logging.info(f"ğŸ“‹ å¼€å§‹éªŒè¯ {len(movie_info)} ä¸ªåˆ†ç»„")
        for i, group in enumerate(movie_info):
            if isinstance(group, dict) and 'group_name' in group:
                enhanced_group = group.copy()
                group_name = group.get('group_name', '')
                ai_file_ids = group.get('fileIds', []) or group.get('files', [])

                # éªŒè¯åˆ†ç»„
                if len(ai_file_ids) < 2:
                    logging.info(f"â­ï¸ è·³è¿‡å•æ–‡ä»¶åˆ†ç»„ '{group_name}': åªæœ‰ {len(ai_file_ids)} ä¸ªæ–‡ä»¶")
                    continue

                if len(ai_file_ids) > 50:
                    logging.warning(f"ğŸš« æ‹’ç»è¶…å¤§åˆ†ç»„ '{group_name}': åŒ…å« {len(ai_file_ids)} ä¸ªæ–‡ä»¶")
                    continue

                # éªŒè¯æ–‡ä»¶åç›¸å…³æ€§
                file_names_for_validation = []
                for file_id in ai_file_ids:
                    for video_file in files:
                        if video_file['fileId'] == file_id:
                            file_names_for_validation.append(video_file['filename'])
                            break

                if len(file_names_for_validation) >= 2:
                    # æ£€æŸ¥æ–‡ä»¶åç›¸å…³æ€§
                    first_file = file_names_for_validation[0]
                    base_name = first_file.split('(')[0].strip() if '(' in first_file else first_file.split('.')[0].strip()

                    related_count = 0
                    for file_name in file_names_for_validation:
                        if base_name in file_name or any(str(i) in file_name for i in range(1, 10)):
                            related_count += 1

                    if related_count < len(file_names_for_validation) * 0.5:
                        logging.warning(f"ğŸš« æ‹’ç»å¯ç–‘åˆ†ç»„ '{group_name}': æ–‡ä»¶åç›¸å…³æ€§ä¸è¶³ ({related_count}/{len(file_names_for_validation)})")
                        continue

                # è·å–å®Œæ•´çš„æ–‡ä»¶ååˆ—è¡¨
                file_names = []
                logging.info(f"ğŸ” å¼€å§‹åŒ¹é…æ–‡ä»¶å: ai_file_ids={ai_file_ids[:3]}...")
                logging.info(f"ğŸ” å¯ç”¨æ–‡ä»¶æ•°é‡: {len(files)}, ç¤ºä¾‹æ–‡ä»¶ID: {[f.get('fileId', 'N/A') for f in files[:3]]}")

                for file_id in ai_file_ids:
                    found = False
                    for video_file in files:
                        if str(video_file['fileId']) == str(file_id):  # ç¡®ä¿ç±»å‹åŒ¹é…
                            file_names.append(video_file['filename'])
                            found = True
                            break
                    if not found:
                        logging.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶ID {file_id} å¯¹åº”çš„æ–‡ä»¶å")

                logging.info(f"ğŸ” åŒ¹é…ç»“æœ: æ‰¾åˆ° {len(file_names)} ä¸ªæ–‡ä»¶å: {file_names[:3]}...")

                # æ„å»ºå¢å¼ºçš„åˆ†ç»„ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®æ ¼å¼å…¼å®¹æ€§
                enhanced_group['fileIds'] = ai_file_ids
                enhanced_group['file_names'] = file_names
                enhanced_group['folder_path'] = source_name
                enhanced_group['group_name'] = group_name
                enhanced_group['files'] = file_names     # å‰ç«¯éœ€è¦æ–‡ä»¶ååˆ—è¡¨
                enhanced_groups.append(enhanced_group)

                logging.info(f"âœ… æˆåŠŸä¿ç•™åˆ†ç»„ '{group_name}': {len(ai_file_ids)} ä¸ªæ–‡ä»¶")
                logging.info(f"ğŸ“ åˆ†ç»„æ•°æ®: fileIds={ai_file_ids[:3]}..., file_names={file_names[:3]}...")
            else:
                logging.warning(f"â­ï¸ è·³è¿‡æ— æ•ˆåˆ†ç»„ (ç¬¬ {i+1} ä¸ª): ç¼ºå°‘å¿…è¦å­—æ®µ")

    # ç¡®ä¿æœ€ç»ˆç»“æœçš„æ•°æ®æ ¼å¼ä¸€è‡´æ€§
    for group in enhanced_groups:
        if 'file_names' in group and 'files' not in group:
            group['files'] = group['file_names']

    logging.info(f"ğŸ¯ éªŒè¯å®Œæˆ: ç”Ÿæˆ {len(enhanced_groups)} ä¸ªæœ‰æ•ˆåˆ†ç»„")

    # è°ƒè¯•ï¼šè¾“å‡ºæœ€ç»ˆåˆ†ç»„æ•°æ®ç»“æ„
    for i, group in enumerate(enhanced_groups[:2]):  # åªè¾“å‡ºå‰2ä¸ªåˆ†ç»„çš„è¯¦ç»†ä¿¡æ¯
        logging.info(f"ğŸ” åˆ†ç»„ {i+1} æ•°æ®ç»“æ„: {list(group.keys())}")
        if 'files' in group:
            logging.info(f"ğŸ” åˆ†ç»„ {i+1} fileså­—æ®µ: {group['files'][:3] if len(group['files']) > 3 else group['files']}")
        if 'file_names' in group:
            logging.info(f"ğŸ” åˆ†ç»„ {i+1} file_nameså­—æ®µ: {group['file_names'][:3] if len(group['file_names']) > 3 else group['file_names']}")

    return enhanced_groups

# ================================
# è´¨é‡è¯„ä¼°å’Œæ™ºèƒ½é‡è¯•æœºåˆ¶
# ================================

def log_extraction_summary(movie_info_list, user_input_content):
    """
    è®°å½•ç”µå½±ä¿¡æ¯æå–çš„è¯¦ç»†æ€»ç»“
    """
    if not movie_info_list:
        logging.error("ğŸ“‹ æå–æ€»ç»“: æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆä¿¡æ¯")
        return

    file_count = len(user_input_content.split('\n')) if user_input_content else 0
    extracted_count = len(movie_info_list) if isinstance(movie_info_list, list) else 0

    logging.info(f"ğŸ“‹ æå–æ€»ç»“: {extracted_count}/{file_count} ä¸ªæ–‡ä»¶æˆåŠŸæå–ä¿¡æ¯")

    if isinstance(movie_info_list, list):
        # ç»Ÿè®¡åª’ä½“ç±»å‹åˆ†å¸ƒ
        media_types = {}
        years = []

        for item in movie_info_list:
            if isinstance(item, dict):
                media_type = item.get('media_type', 'unknown')
                media_types[media_type] = media_types.get(media_type, 0) + 1

                year = item.get('year')
                if year:
                    try:
                        years.append(int(year))
                    except (ValueError, TypeError):
                        pass

        # è®°å½•åª’ä½“ç±»å‹åˆ†å¸ƒ
        if media_types:
            type_summary = ', '.join([f"{k}: {v}" for k, v in media_types.items()])
            logging.info(f"ğŸ“Š åª’ä½“ç±»å‹åˆ†å¸ƒ: {type_summary}")

        # è®°å½•å¹´ä»½èŒƒå›´
        if years:
            min_year, max_year = min(years), max(years)
            logging.info(f"ğŸ“… å¹´ä»½èŒƒå›´: {min_year} - {max_year}")


def log_tmdb_search_summary(results):
    """
    è®°å½•TMDBæœç´¢ç»“æœçš„è¯¦ç»†æ€»ç»“
    """
    if not results:
        return

    total = len(results)
    success = len([r for r in results if r.get('status') == 'success'])
    no_match = len([r for r in results if r.get('status') == 'no_match'])
    errors = len([r for r in results if r.get('status') == 'error'])

    logging.info(f"ğŸ¯ TMDBæœç´¢æ€»ç»“: æˆåŠŸ={success}, æ— åŒ¹é…={no_match}, é”™è¯¯={errors}, æ€»è®¡={total}")

    # ç»Ÿè®¡åŒ¹é…è´¨é‡åˆ†å¸ƒ
    quality_scores = []
    for result in results:
        if result.get('match_quality') and 'score' in result['match_quality']:
            quality_scores.append(result['match_quality']['score'])

    if quality_scores:
        avg_quality = sum(quality_scores) / len(quality_scores)
        high_quality = len([s for s in quality_scores if s >= 80])
        logging.info(f"ğŸ“Š åŒ¹é…è´¨é‡: å¹³å‡åˆ†={avg_quality:.1f}, é«˜è´¨é‡åŒ¹é…={high_quality}/{len(quality_scores)}")


def handle_extraction_error(error, attempt, max_attempts, strategy_name=""):
    """
    ç»Ÿä¸€çš„æå–é”™è¯¯å¤„ç†å‡½æ•°
    """
    error_msg = str(error)

    # æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„å¤„ç†å»ºè®®
    if "timeout" in error_msg.lower():
        suggestion = "å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ è¶…æ—¶æ—¶é—´"
    elif "json" in error_msg.lower():
        suggestion = "AIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æç¤ºè¯"
    elif "api" in error_msg.lower() or "401" in error_msg or "403" in error_msg:
        suggestion = "APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
    elif "429" in error_msg:
        suggestion = "APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œå»ºè®®é™ä½QPSè®¾ç½®"
    else:
        suggestion = "æœªçŸ¥é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥æ—¥å¿—è¯¦æƒ…"

    is_warning = attempt < max_attempts
    prefix = f"âš ï¸ {strategy_name}" if strategy_name else "âŒ"

    if is_warning:
        logging.warning(f"{prefix} å°è¯• {attempt}/{max_attempts} å¤±è´¥: {error_msg}")
        logging.warning(f"ğŸ’¡ å¤„ç†å»ºè®®: {suggestion}")
    else:
        logging.error(f"{prefix} å°è¯• {attempt}/{max_attempts} å¤±è´¥: {error_msg}")
        logging.error(f"ğŸ’¡ å¤„ç†å»ºè®®: {suggestion}")

    return suggestion

def validate_grouping_result(groups_data, file_list):
    """
    éªŒè¯åˆ†ç»„ç»“æœï¼Œè¿‡æ»¤æ‰æ˜æ˜¾é”™è¯¯çš„åˆ†ç»„

    Args:
        groups_data: åˆ†ç»„æ•°æ®ï¼Œæ ¼å¼ä¸º [{"group_name": "...", "fileIds": [...]}]
        file_list: åŸå§‹æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶åä¿¡æ¯

    Returns:
        list: éªŒè¯åçš„åˆ†ç»„æ•°æ®
    """
    if not groups_data or not isinstance(groups_data, list):
        return []

    validated_groups = []

    for group in groups_data:
        if not isinstance(group, dict) or 'group_name' not in group or 'fileIds' not in group:
            continue

        file_ids = group['fileIds']
        if len(file_ids) < 2:  # å•ä¸ªæ–‡ä»¶ä¸åˆ†ç»„
            continue

        # è·å–è¿™ä¸ªåˆ†ç»„ä¸­çš„æ–‡ä»¶å
        group_filenames = []
        for file_id in file_ids:
            for file_info in file_list:
                if file_info.get('id') == file_id:
                    group_filenames.append(file_info.get('name', ''))
                    break

        # éªŒè¯åˆ†ç»„çš„åˆç†æ€§
        if validate_group_similarity(group_filenames):
            validated_groups.append(group)
        else:
            logging.warning(f"ğŸš« è¿‡æ»¤æ‰ä¸åˆç†çš„åˆ†ç»„: {group['group_name']} - æ–‡ä»¶: {group_filenames}")

    return validated_groups

def validate_group_similarity(filenames):
    """
    éªŒè¯ä¸€ç»„æ–‡ä»¶åæ˜¯å¦çœŸçš„å±äºåŒä¸€ç³»åˆ—

    Args:
        filenames: æ–‡ä»¶ååˆ—è¡¨

    Returns:
        bool: æ˜¯å¦ä¸ºåˆç†çš„åˆ†ç»„
    """
    if len(filenames) < 2:
        return False

    # æå–ä¸»è¦å…³é”®è¯
    keywords_sets = []
    for filename in filenames:
        # ç§»é™¤å¹´ä»½ã€åˆ†è¾¨ç‡ã€ç¼–ç ç­‰ä¿¡æ¯ï¼Œæå–æ ¸å¿ƒå…³é”®è¯
        clean_name = re.sub(r'\(\d{4}\)', '', filename)  # ç§»é™¤å¹´ä»½
        clean_name = re.sub(r'\{[^}]*\}', '', clean_name)  # ç§»é™¤{}å†…å®¹
        clean_name = re.sub(r'\[[^\]]*\]', '', clean_name)  # ç§»é™¤[]å†…å®¹
        clean_name = re.sub(r'\.(mkv|mp4|avi|mov|wmv|flv|webm)$', '', clean_name, flags=re.IGNORECASE)  # ç§»é™¤æ‰©å±•å

        # åˆ†å‰²æˆå…³é”®è¯
        keywords = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', clean_name))
        keywords = {kw for kw in keywords if len(kw) > 1}  # è¿‡æ»¤å•å­—ç¬¦
        keywords_sets.append(keywords)

    # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„æ ¸å¿ƒå…³é”®è¯
    if not keywords_sets:
        return False

    # è®¡ç®—äº¤é›†
    common_keywords = keywords_sets[0]
    for keywords in keywords_sets[1:]:
        common_keywords = common_keywords.intersection(keywords)

    # å¦‚æœæœ‰è¶³å¤Ÿçš„å…±åŒå…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯åˆç†åˆ†ç»„
    return len(common_keywords) >= 1 and any(len(kw) >= 2 for kw in common_keywords)

def evaluate_grouping_quality(grouping_result):
    """
    è¯„ä¼°æ™ºèƒ½åˆ†ç»„ç»“æœçš„è´¨é‡

    Args:
        grouping_result: AIè¿”å›çš„åˆ†ç»„ç»“æœ

    Returns:
        dict: åŒ…å«è´¨é‡åˆ†æ•°å’Œè¯¦ç»†è¯„ä¼°ç»“æœ
    """
    if not grouping_result:
        return {"score": 0, "issues": ["æ— æ•ˆçš„åˆ†ç»„ç»“æœ"], "valid_count": 0, "total_count": 0, "valid_ratio": 0}

    # å¦‚æœæ˜¯å•ä¸ªåˆ†ç»„å¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(grouping_result, dict):
        grouping_result = [grouping_result]

    if not isinstance(grouping_result, list):
        return {"score": 0, "issues": ["åˆ†ç»„ç»“æœæ ¼å¼é”™è¯¯"], "valid_count": 0, "total_count": 0, "valid_ratio": 0}

    total_score = 0
    total_items = len(grouping_result)
    valid_count = 0
    issues = []

    for i, group in enumerate(grouping_result):
        item_score = 0
        item_issues = []

        if not isinstance(group, dict):
            item_issues.append("åˆ†ç»„é¡¹ä¸æ˜¯å­—å…¸æ ¼å¼")
            continue

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if 'group_name' in group and group['group_name']:
            group_name = str(group['group_name']).strip()
            if len(group_name) >= 2:
                item_score += 40  # åˆ†ç»„åç§°å 40%æƒé‡
            else:
                item_issues.append(f"åˆ†ç»„åç§°è¿‡çŸ­: {group_name}")
        else:
            item_issues.append("ç¼ºå°‘åˆ†ç»„åç§°")

        if 'fileIds' in group and group['fileIds']:
            file_ids = group['fileIds']
            if isinstance(file_ids, list) and len(file_ids) >= 2:
                item_score += 60  # æ–‡ä»¶IDåˆ—è¡¨å 60%æƒé‡
                # é¢å¤–æ£€æŸ¥ï¼šæ–‡ä»¶æ•°é‡åˆç†æ€§
                if len(file_ids) <= 50:  # åˆç†çš„åˆ†ç»„å¤§å°
                    item_score = min(100, item_score + 10)
            else:
                item_issues.append(f"æ–‡ä»¶IDåˆ—è¡¨æ— æ•ˆæˆ–å°‘äº2ä¸ªæ–‡ä»¶: {file_ids}")
        else:
            item_issues.append("ç¼ºå°‘æ–‡ä»¶IDåˆ—è¡¨")

        if item_score >= 70:  # è®¤ä¸ºæ˜¯æœ‰æ•ˆåˆ†ç»„
            valid_count += 1

        total_score += item_score
        if item_issues:
            issues.extend([f"åˆ†ç»„ {i+1}: {issue}" for issue in item_issues])

    average_score = total_score / total_items if total_items > 0 else 0

    return {
        "score": round(average_score, 2),
        "issues": issues,
        "valid_count": valid_count,
        "total_count": total_items,
        "valid_ratio": round(valid_count / total_items * 100, 2) if total_items > 0 else 0
    }


def evaluate_extraction_quality(movie_info_list):
    """
    è¯„ä¼°ç”µå½±ä¿¡æ¯æå–ç»“æœçš„è´¨é‡

    Args:
        movie_info_list: AIæå–çš„ç”µå½±ä¿¡æ¯åˆ—è¡¨

    Returns:
        dict: åŒ…å«è´¨é‡åˆ†æ•°å’Œè¯¦ç»†è¯„ä¼°ç»“æœ
    """
    if not movie_info_list or not isinstance(movie_info_list, list):
        return {"score": 0, "issues": ["æ— æ•ˆçš„æå–ç»“æœ"], "valid_count": 0}

    total_score = 0
    total_items = len(movie_info_list)
    valid_count = 0
    issues = []

    for i, item in enumerate(movie_info_list):
        if not isinstance(item, dict):
            issues.append(f"é¡¹ç›® {i+1}: ä¸æ˜¯æœ‰æ•ˆçš„å­—å…¸æ ¼å¼")
            continue

        item_score = 0
        item_issues = []

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['original_title', 'title', 'year', 'media_type']
        for field in required_fields:
            if field in item and item[field]:
                if field == 'year':
                    # éªŒè¯å¹´ä»½æ ¼å¼å’Œåˆç†æ€§
                    try:
                        year = int(str(item[field]))
                        if 1900 <= year <= 2030:
                            item_score += 25  # å¹´ä»½å 25%æƒé‡
                        else:
                            item_issues.append(f"å¹´ä»½ä¸åˆç†: {year}")
                    except (ValueError, TypeError):
                        item_issues.append(f"å¹´ä»½æ ¼å¼é”™è¯¯: {item[field]}")
                elif field == 'media_type':
                    # éªŒè¯åª’ä½“ç±»å‹
                    if item[field].lower() in ['movie', 'tv', 'tv_series', 'anime']:
                        item_score += 15  # åª’ä½“ç±»å‹å 15%æƒé‡
                    else:
                        item_issues.append(f"æœªçŸ¥åª’ä½“ç±»å‹: {item[field]}")
                elif field in ['original_title', 'title']:
                    # éªŒè¯æ ‡é¢˜é•¿åº¦å’Œå†…å®¹
                    title = str(item[field]).strip()
                    if len(title) >= 2 and not title.isdigit():
                        item_score += 30  # æ¯ä¸ªæ ‡é¢˜å 30%æƒé‡
                    else:
                        item_issues.append(f"{field}è¿‡çŸ­æˆ–æ— æ•ˆ: {title}")
            else:
                item_issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        # é¢å¤–è´¨é‡æ£€æŸ¥
        if item_score >= 80:  # åŸºç¡€åˆ†æ•°è¾¾æ ‡
            # æ£€æŸ¥æ ‡é¢˜ä¸€è‡´æ€§
            if 'original_title' in item and 'title' in item:
                orig_title = str(item['original_title']).strip().lower()
                title = str(item['title']).strip().lower()
                if orig_title and title and (orig_title == title or orig_title in title or title in orig_title):
                    item_score = min(100, item_score + 10)  # æ ‡é¢˜ä¸€è‡´æ€§åŠ åˆ†

        if item_score >= 70:  # è®¤ä¸ºæ˜¯æœ‰æ•ˆé¡¹ç›®
            valid_count += 1

        total_score += item_score
        if item_issues:
            issues.extend([f"é¡¹ç›® {i+1}: {issue}" for issue in item_issues])

    average_score = total_score / total_items if total_items > 0 else 0

    return {
        "score": round(average_score, 2),
        "issues": issues,
        "valid_count": valid_count,
        "total_count": total_items,
        "valid_ratio": round(valid_count / total_items * 100, 2) if total_items > 0 else 0
    }


def evaluate_tmdb_match_quality(movie_info, tmdb_result):
    """
    è¯„ä¼°TMDBæœç´¢ç»“æœçš„åŒ¹é…è´¨é‡

    Args:
        movie_info: æå–çš„ç”µå½±ä¿¡æ¯
        tmdb_result: TMDBæœç´¢ç»“æœ

    Returns:
        dict: åŒ…å«åŒ¹é…è´¨é‡åˆ†æ•°å’Œè¯¦ç»†ä¿¡æ¯
    """
    if not tmdb_result or not movie_info:
        return {"score": 0, "reasons": ["æ— æœç´¢ç»“æœæˆ–è¾“å…¥ä¿¡æ¯"]}

    score = 0
    reasons = []

    try:
        # å¹´ä»½åŒ¹é…æ£€æŸ¥ (40%æƒé‡) - æ›´ä¸¥æ ¼çš„å¹´ä»½åŒ¹é…
        movie_year = str(movie_info.get('year', ''))
        tmdb_date = tmdb_result.get('release_date') or tmdb_result.get('first_air_date', '')
        if tmdb_date and movie_year:
            tmdb_year = tmdb_date[:4]
            year_diff = abs(int(tmdb_year) - int(movie_year))
            if year_diff == 0:
                score += 40
                reasons.append(f"å¹´ä»½å®Œå…¨åŒ¹é…: {movie_year}")
            elif year_diff <= 1:
                score += 30
                reasons.append(f"å¹´ä»½æ¥è¿‘åŒ¹é…: {movie_year} vs {tmdb_year}")
            elif year_diff <= 3:
                score += 15  # é™ä½æ¨¡ç³ŠåŒ¹é…åˆ†æ•°
                reasons.append(f"å¹´ä»½æ¨¡ç³ŠåŒ¹é…: {movie_year} vs {tmdb_year}")
            elif year_diff <= 10:
                score -= 10  # å¹´ä»½å·®å¼‚è¾ƒå¤§æ—¶æ‰£åˆ†
                reasons.append(f"å¹´ä»½å·®å¼‚è¾ƒå¤§: {movie_year} vs {tmdb_year} (æ‰£åˆ†)")
            else:
                score -= 30  # å¹´ä»½å·®å¼‚è¿‡å¤§æ—¶ä¸¥é‡æ‰£åˆ†
                reasons.append(f"å¹´ä»½å·®å¼‚è¿‡å¤§: {movie_year} vs {tmdb_year} (ä¸¥é‡æ‰£åˆ†)")

        # æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥ (40%æƒé‡)
        movie_title = str(movie_info.get('title', '')).lower().strip()
        tmdb_title = str(tmdb_result.get('title') or tmdb_result.get('name', '')).lower().strip()

        if movie_title and tmdb_title:
            if movie_title == tmdb_title:
                score += 40
                reasons.append("æ ‡é¢˜å®Œå…¨åŒ¹é…")
            elif movie_title in tmdb_title or tmdb_title in movie_title:
                score += 30
                reasons.append("æ ‡é¢˜éƒ¨åˆ†åŒ¹é…")
            else:
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                movie_words = set(movie_title.split())
                tmdb_words = set(tmdb_title.split())
                common_words = movie_words.intersection(tmdb_words)
                if len(common_words) >= 2:
                    score += 20
                    reasons.append(f"æ ‡é¢˜å…³é”®è¯åŒ¹é…: {common_words}")
                elif len(common_words) >= 1:
                    score += 10
                    reasons.append(f"æ ‡é¢˜éƒ¨åˆ†å…³é”®è¯åŒ¹é…: {common_words}")
                else:
                    reasons.append("æ ‡é¢˜æ— æ˜æ˜¾åŒ¹é…")

        # åª’ä½“ç±»å‹åŒ¹é…æ£€æŸ¥ (20%æƒé‡)
        movie_type = str(movie_info.get('media_type', '')).lower()
        is_movie_result = 'release_date' in tmdb_result
        is_tv_result = 'first_air_date' in tmdb_result

        if movie_type == 'movie' and is_movie_result:
            score += 20
            reasons.append("åª’ä½“ç±»å‹åŒ¹é…: ç”µå½±")
        elif movie_type in ['tv', 'tv_series'] and is_tv_result:
            score += 20
            reasons.append("åª’ä½“ç±»å‹åŒ¹é…: ç”µè§†å‰§")
        elif movie_type and (is_movie_result or is_tv_result):
            score += 10
            reasons.append("åª’ä½“ç±»å‹éƒ¨åˆ†åŒ¹é…")

        # TMDB IDç›´æ¥åŒ¹é… (é¢å¤–åŠ åˆ†)
        if movie_info.get('tmdb_id') and str(movie_info['tmdb_id']) == str(tmdb_result.get('id', '')):
            score += 20
            reasons.append("TMDB IDç›´æ¥åŒ¹é…")

    except Exception as e:
        reasons.append(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")

    return {
        "score": min(100, score),  # æœ€é«˜100åˆ†
        "reasons": reasons
    }


def extract_movie_info_from_filename_enhanced(user_input_content, EXTRACTION_PROMPT, model=None, max_attempts=3, enable_quality_assessment=None):
    """
    å¢å¼ºç‰ˆç”µå½±ä¿¡æ¯æå–å‡½æ•°ï¼Œæ”¯æŒå¤šæ¬¡å°è¯•å’Œè´¨é‡è¯„ä¼°

    Args:
        user_input_content: è¾“å…¥çš„æ–‡ä»¶åå†…å®¹
        EXTRACTION_PROMPT: æå–æç¤ºè¯
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        max_attempts: æœ€å¤§å°è¯•æ¬¡æ•°
        enable_quality_assessment: æ˜¯å¦å¯ç”¨è´¨é‡è¯„ä¼°ï¼ŒNoneæ—¶æ ¹æ®æç¤ºè¯ç±»å‹è‡ªåŠ¨åˆ¤æ–­

    Returns:
        æœ€ä½³è´¨é‡çš„æå–ç»“æœ
    """
    logging.info(f"ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆç”µå½±ä¿¡æ¯æå–ï¼Œæœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤çš„MODELï¼›å¦‚æœæ˜¯MAGIC_PROMPTï¼Œä½¿ç”¨GROUPING_MODEL
    model = MODEL

    # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†ç»„æç¤ºè¯
    is_grouping_prompt = "åˆ†ç»„" in EXTRACTION_PROMPT or "group_name" in EXTRACTION_PROMPT or "fileIds" in EXTRACTION_PROMPT

    # æ ¹æ®å‚æ•°å’Œæç¤ºè¯ç±»å‹å†³å®šæ˜¯å¦å¯ç”¨è´¨é‡è¯„ä¼°
    if enable_quality_assessment is None:
        # è‡ªåŠ¨åˆ¤æ–­ï¼šåˆ†ç»„ä½¿ç”¨ENABLE_QUALITY_ASSESSMENTï¼Œåˆ®å‰Šä½¿ç”¨ENABLE_SCRAPING_QUALITY_ASSESSMENT
        use_quality_assessment = ENABLE_QUALITY_ASSESSMENT if is_grouping_prompt else ENABLE_SCRAPING_QUALITY_ASSESSMENT
    else:
        # ä½¿ç”¨æ˜ç¡®æŒ‡å®šçš„å‚æ•°
        use_quality_assessment = enable_quality_assessment

    best_result = None
    best_quality = {"score": 0}
    all_attempts = []

    # å®šä¹‰ä¸åŒçš„æå–ç­–ç•¥ - ä¼˜åŒ–ç‰ˆï¼šå‡å°‘ç­–ç•¥æ•°é‡
    if is_grouping_prompt:
        strategies = [
            {
                "name": "æ™ºèƒ½åˆ†ç»„",
                "prompt": EXTRACTION_PROMPT + "\n\n**é‡è¦æé†’**: å¿…é¡»è¿”å›å®Œæ•´çš„JSONæ ¼å¼ï¼ŒåŒ…å«group_nameå’Œå®Œæ•´çš„fileIdsæ•°ç»„ã€‚",
                "model": model,
                "temperature": 0.1
            }
        ]
    else:
        strategies = [
            {
                "name": "æ™ºèƒ½æå–",
                "prompt": EXTRACTION_PROMPT + "\n\n**é‡è¦æé†’**: å¿…é¡»è¿”å›å®Œæ•´çš„JSONæ ¼å¼ã€‚",
                "model": model,
                "temperature": 0.1  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„ç»“æœ
            }
        ]

    for attempt in range(max_attempts):
        strategy = strategies[min(attempt, len(strategies) - 1)]
        logging.info(f"ğŸ”„ å°è¯• {attempt + 1}/{max_attempts}: {strategy['name']}")

        try:
            result = _single_extraction_attempt(
                user_input_content,
                strategy['prompt'],
                strategy['model'],
                strategy.get('temperature', 0.1)
            )

            if result:
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œè´¨é‡è¯„ä¼°
                if use_quality_assessment:
                    # æ ¹æ®æç¤ºè¯ç±»å‹é€‰æ‹©åˆé€‚çš„è´¨é‡è¯„ä¼°å‡½æ•°
                    if is_grouping_prompt:
                        quality = evaluate_grouping_quality(result)
                        logging.info(f"ğŸ“Š {strategy['name']} æ™ºèƒ½åˆ†ç»„è´¨é‡è¯„ä¼°: åˆ†æ•°={quality.get('score', 0)}, æœ‰æ•ˆé¡¹={quality.get('valid_count', 0)}/{quality.get('total_count', 0)}")
                    else:
                        quality = evaluate_extraction_quality(result)
                        logging.info(f"ğŸ“Š {strategy['name']} åˆ®å‰Šè´¨é‡è¯„ä¼°: åˆ†æ•°={quality.get('score', 0)}, æœ‰æ•ˆé¡¹={quality.get('valid_count', 0)}/{quality.get('total_count', 0)}")
                else:
                    # ç¦ç”¨è´¨é‡è¯„ä¼°æ—¶ï¼Œä½¿ç”¨é»˜è®¤çš„é«˜è´¨é‡åˆ†æ•°
                    quality = {"score": 100, "valid_count": 1, "total_count": 1, "valid_ratio": 100}
                    if is_grouping_prompt:
                        logging.info(f"âš¡ {strategy['name']} æ™ºèƒ½åˆ†ç»„è´¨é‡è¯„ä¼°å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›ç»“æœ")
                    else:
                        logging.info(f"âš¡ {strategy['name']} åˆ®å‰Šè´¨é‡è¯„ä¼°å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›ç»“æœ")

                all_attempts.append({
                    "attempt": attempt + 1,
                    "strategy": strategy['name'],
                    "result": result,
                    "quality": quality
                })

                # å¦‚æœç¦ç”¨è´¨é‡è¯„ä¼°æˆ–è´¨é‡è¶³å¤Ÿå¥½ï¼Œå¯ä»¥æå‰è¿”å›
                if not use_quality_assessment or (quality.get('score', 0) >= 90 and quality.get('valid_ratio', 0) >= 80):
                    if use_quality_assessment:
                        logging.info(f"âœ… {strategy['name']} è¾¾åˆ°é«˜è´¨é‡æ ‡å‡†ï¼Œæå‰è¿”å›")
                    else:
                        logging.info(f"âš¡ {strategy['name']} è´¨é‡è¯„ä¼°å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªç»“æœ")
                    return result

                # æ›´æ–°æœ€ä½³ç»“æœ
                if quality.get('score', 0) > best_quality.get('score', 0):
                    best_result = result
                    best_quality = quality
                    if use_quality_assessment:
                        logging.info(f"ğŸ¯ æ›´æ–°æœ€ä½³ç»“æœ: {strategy['name']} (åˆ†æ•°: {quality['score']})")
                    else:
                        logging.info(f"ğŸ¯ æ›´æ–°æœ€ä½³ç»“æœ: {strategy['name']} (è´¨é‡è¯„ä¼°å·²ç¦ç”¨)")
            else:
                logging.warning(f"âŒ {strategy['name']} æœªèƒ½æå–åˆ°æœ‰æ•ˆç»“æœ")

        except Exception as e:
            logging.error(f"âŒ {strategy['name']} æ‰§è¡Œå¤±è´¥: {e}")
            continue

    # è®°å½•æ‰€æœ‰å°è¯•çš„æ€»ç»“
    if all_attempts:
        if use_quality_assessment:
            logging.info(f"ğŸ“‹ æå–å°è¯•æ€»ç»“:")
            for attempt_info in all_attempts:
                quality = attempt_info.get('quality', {})
                logging.info(f"  - {attempt_info['strategy']}: åˆ†æ•°={quality.get('score', 0)}, æœ‰æ•ˆç‡={quality.get('valid_ratio', 0)}%")
        else:
            logging.info(f"ğŸ“‹ æå–å°è¯•æ€»ç»“ (è´¨é‡è¯„ä¼°å·²ç¦ç”¨):")
            for attempt_info in all_attempts:
                logging.info(f"  - {attempt_info['strategy']}: å·²å®Œæˆ")

    if best_result:
        if use_quality_assessment:
            logging.info(f"ğŸ† è¿”å›æœ€ä½³ç»“æœï¼Œè´¨é‡åˆ†æ•°: {best_quality.get('score', 0)}")
            if best_quality.get('issues'):
                logging.warning(f"âš ï¸ è´¨é‡é—®é¢˜: {best_quality['issues'][:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
        else:
            logging.info(f"ğŸ† è¿”å›ç»“æœ (è´¨é‡è¯„ä¼°å·²ç¦ç”¨)")

        # è®°å½•æå–æ€»ç»“
        log_extraction_summary(best_result, user_input_content)
    else:
        logging.error(f"âŒ æ‰€æœ‰æå–å°è¯•éƒ½å¤±è´¥äº†")

    return best_result


def _single_extraction_attempt(user_input_content, prompt, model, temperature=0.1):
    """
    å•æ¬¡æå–å°è¯•çš„å†…éƒ¨å‡½æ•°ï¼ŒåŒ…å«JSONè§£æé‡è¯•æœºåˆ¶
    """
    max_retries = AI_MAX_RETRIES  # ä½¿ç”¨å…¨å±€é…ç½®
    retry_delay = AI_RETRY_DELAY  # ä½¿ç”¨å…¨å±€é…ç½®

    for retry in range(max_retries):
        try:
            logging.info(f"ğŸ”„ AIè°ƒç”¨å°è¯• {retry + 1}/{max_retries}")

            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{prompt}\n\n{user_input_content}"

            # ä½¿ç”¨æ”¹è¿›çš„APIè°ƒç”¨å‡½æ•°
            response_content = call_ai_api(full_prompt, model, temperature)

            if response_content:
                logging.info(f"âœ… AIå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_content)} å­—ç¬¦")
                # è§£æJSONå“åº”
                parsed_result = _parse_ai_response(response_content)
                if parsed_result:
                    logging.info(f"âœ… JSONè§£ææˆåŠŸ")
                    return parsed_result
                else:
                    logging.warning(f"âš ï¸ é‡è¯• {retry + 1}/{max_retries}: JSONè§£æå¤±è´¥")
            else:
                logging.warning(f"âš ï¸ é‡è¯• {retry + 1}/{max_retries}: AIå“åº”ä¸ºç©º")

            # å¦‚æœå¤±è´¥ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åé‡è¯•
            if retry < max_retries - 1:
                logging.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)

        except Exception as e:
            logging.warning(f"âŒ é‡è¯• {retry + 1}/{max_retries} å¤±è´¥: {e}")

    logging.error(f"âŒ AIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²å°è¯• {max_retries} æ¬¡")
    return None


def _parse_ai_response(input_string):
    """è§£æAIå“åº” - ç®€åŒ–ç‰ˆ"""
    # å°è¯•æå–JSONä»£ç å—
    logging.info(f"å°è¯•è§£æAIå“åº”: {input_string}")
    pattern = r'```json(.*?)```'
    match = re.search(pattern, input_string, re.DOTALL)

    if match:
        json_data = match.group(1).strip()
        try:
            return json.loads(json_data)
        except json.JSONDecodeError:
            pass
    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
    input_string = input_string.strip()
    if input_string.startswith(('[', '{')):
        try:
            return json.loads(input_string)
        except json.JSONDecodeError:
            pass

    return None


def search_movie_in_tmdb_enhanced(movie_info, max_strategies=5):
    """
    å¢å¼ºç‰ˆTMDBæœç´¢å‡½æ•°ï¼Œæ”¯æŒå¤šç§æœç´¢ç­–ç•¥å’Œè´¨é‡è¯„ä¼°

    Args:
        movie_info: ç”µå½±ä¿¡æ¯å­—å…¸
        max_strategies: æœ€å¤§æœç´¢ç­–ç•¥æ•°é‡

    Returns:
        æœ€ä½³åŒ¹é…çš„TMDBç»“æœ
    """
    logging.info(f"ğŸ” å¼€å§‹å¢å¼ºç‰ˆTMDBæœç´¢: {movie_info.get('title', 'Unknown')}")

    if not movie_info:
        logging.error("âŒ è¾“å…¥çš„ç”µå½±ä¿¡æ¯ä¸ºç©º")
        return None

    try:
        original_title = str(movie_info.get('original_title', ''))
        title = str(movie_info.get('title', ''))
        media_type = str(movie_info.get('media_type', 'movie')).lower()

        # ç¡®å®šæœç´¢ç±»å‹
        search_type = 'movie' if media_type == 'movie' else 'tv'

        # å®šä¹‰å¤šç§æœç´¢ç­–ç•¥
        search_strategies = [
            {
                "name": "ç²¾ç¡®åŒ¹é…",
                "query": original_title,
                "year_tolerance": 0,
                "priority": 1
            },
            {
                "name": "æ ‡é¢˜å˜ä½“åŒ¹é…",
                "query": title if title != original_title else original_title,
                "year_tolerance": 1,
                "priority": 2
            },
            {
                "name": "æ¸…ç†æ ‡é¢˜åŒ¹é…",
                "query": re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5\s]', ' ', original_title).strip(),
                "year_tolerance": 2,
                "priority": 3
            },
            {
                "name": "ç®€åŒ–æ ‡é¢˜åŒ¹é…",
                "query": _simplify_title(original_title),
                "year_tolerance": 3,
                "priority": 4
            },
            {
                "name": "å…³é”®è¯åŒ¹é…",
                "query": _extract_keywords(original_title),
                "year_tolerance": 5,
                "priority": 5
            }
        ]

        best_result = None
        best_quality = {"score": 0}
        all_results = []

        for i, strategy in enumerate(search_strategies[:max_strategies]):
            if not strategy["query"] or len(strategy["query"].strip()) < 2:
                logging.info(f"â­ï¸ è·³è¿‡ç­–ç•¥ '{strategy['name']}': æŸ¥è¯¢è¯å¤ªçŸ­")
                continue

            logging.info(f"ğŸ” ç­–ç•¥ {i+1}: {strategy['name']} - æŸ¥è¯¢: '{strategy['query']}'")

            try:
                # æ‰§è¡ŒTMDBæœç´¢
                search_results = _perform_tmdb_search(strategy["query"], search_type, LANGUAGE)

                if not search_results:
                    logging.info(f"âŒ ç­–ç•¥ '{strategy['name']}' æ— æœç´¢ç»“æœ")
                    continue

                # è¯„ä¼°æ¯ä¸ªæœç´¢ç»“æœ
                for result in search_results[:5]:  # åªè¯„ä¼°å‰5ä¸ªç»“æœ
                    quality = evaluate_tmdb_match_quality(movie_info, result)

                    # æ ¹æ®ç­–ç•¥ä¼˜å…ˆçº§è°ƒæ•´åˆ†æ•°
                    adjusted_score = quality["score"] * (1.0 - (strategy["priority"] - 1) * 0.1)

                    result_info = {
                        "result": result,
                        "quality": quality,
                        "adjusted_score": adjusted_score,
                        "strategy": strategy["name"],
                        "query": strategy["query"]
                    }

                    all_results.append(result_info)

                    # æ›´æ–°æœ€ä½³ç»“æœ
                    if adjusted_score > best_quality.get("score", 0):
                        best_result = result
                        best_quality = quality
                        best_quality["adjusted_score"] = adjusted_score
                        best_quality["strategy"] = strategy["name"]

                        logging.info(f"ğŸ¯ æ›´æ–°æœ€ä½³åŒ¹é…: {strategy['name']} - åˆ†æ•°: {adjusted_score:.1f}")

                # å¦‚æœæ‰¾åˆ°é«˜è´¨é‡åŒ¹é…ï¼Œå¯ä»¥æå‰è¿”å›
                if best_quality.get("adjusted_score", 0) >= 85:
                    logging.info(f"âœ… æ‰¾åˆ°é«˜è´¨é‡åŒ¹é…ï¼Œæå‰è¿”å›")
                    break

            except Exception as e:
                logging.error(f"âŒ ç­–ç•¥ '{strategy['name']}' æ‰§è¡Œå¤±è´¥: {e}")
                continue

        # è®°å½•æœç´¢ç»“æœæ€»ç»“
        if all_results:
            logging.info(f"ğŸ“Š TMDBæœç´¢æ€»ç»“ (å…± {len(all_results)} ä¸ªå€™é€‰ç»“æœ):")
            # æŒ‰åˆ†æ•°æ’åºæ˜¾ç¤ºå‰3ä¸ªç»“æœ
            sorted_results = sorted(all_results, key=lambda x: x["adjusted_score"], reverse=True)
            for i, result_info in enumerate(sorted_results[:3]):
                result = result_info["result"]
                title_display = result.get('title') or result.get('name', 'Unknown')
                year_display = (result.get('release_date') or result.get('first_air_date', ''))[:4]
                logging.info(f"  {i+1}. {title_display} ({year_display}) - åˆ†æ•°: {result_info['adjusted_score']:.1f} - ç­–ç•¥: {result_info['strategy']}")

        if best_result:
            title_display = best_result.get('title') or best_result.get('name', 'Unknown')
            year_display = (best_result.get('release_date') or best_result.get('first_air_date', ''))[:4]
            logging.info(f"ğŸ† æœ€ç»ˆé€‰æ‹©: {title_display} ({year_display}) - åˆ†æ•°: {best_quality.get('adjusted_score', 0):.1f}")
            logging.info(f"ğŸ” åŒ¹é…åŸå› : {', '.join(best_quality.get('reasons', []))}")
        else:
            logging.warning(f"âŒ æœªæ‰¾åˆ°åˆé€‚çš„TMDBåŒ¹é…ç»“æœ")

        return best_result

    except Exception as e:
        logging.error(f"âŒ TMDBæœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
        traceback.print_exc()
        return None


def _perform_tmdb_search(query, media_type_search, language):
    """
    æ‰§è¡Œå•æ¬¡TMDB APIæœç´¢
    """
    if media_type_search == 'movie':
        url = f"{TMDB_API_URL_BASE}/search/movie"
    elif media_type_search == 'tv':
        url = f"{TMDB_API_URL_BASE}/search/tv"
    else:
        return []

    params = {
        "api_key": TMDB_API_KEY,
        "query": query,
        "language": language,
    }

    max_retries = TMDB_MAX_RETRIES
    retry_delay = TMDB_RETRY_DELAY

    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, timeout=TMDB_API_TIMEOUT)
            response.raise_for_status()
            return response.json().get('results', [])
        except requests.RequestException as e:
            logging.warning(f"TMDB APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)

    return []


def _simplify_title(title):
    """
    ç®€åŒ–æ ‡é¢˜ï¼Œç§»é™¤å¸¸è§çš„ä¿®é¥°è¯å’Œæ ‡ç‚¹
    """
    if not title:
        return ""

    # ç§»é™¤å¸¸è§çš„ä¿®é¥°è¯
    simplified = re.sub(r'\b(the|a|an|and|or|of|in|on|at|to|for|with|by)\b', ' ', title.lower())
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸­æ–‡
    simplified = re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5\s]', ' ', simplified)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    simplified = re.sub(r'\s+', ' ', simplified).strip()

    return simplified


def _extract_keywords(title):
    """
    ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
    """
    if not title:
        return ""

    # ç§»é™¤å¸¸è§çš„æŠ€æœ¯æ ‡è®°
    cleaned = re.sub(r'\b(1080p|720p|4k|bluray|webrip|hdtv|x264|x265|h264|h265)\b', '', title.lower())
    # æå–ä¸»è¦è¯æ±‡
    words = re.findall(r'[a-zA-Z\u4e00-\u9fa5]{3,}', cleaned)
    # è¿”å›å‰3ä¸ªæœ€é•¿çš„è¯
    keywords = sorted(set(words), key=len, reverse=True)[:3]

    return ' '.join(keywords)


def extract_movie_name_and_info(chunk):
    """
    ä¼˜åŒ–ç‰ˆç”µå½±ä¿¡æ¯æå–å’ŒTMDBåŒ¹é…ä¸»å‡½æ•°
    å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œæé«˜å¤„ç†é€Ÿåº¦ï¼Œæ·»åŠ ç¼“å­˜æœºåˆ¶
    """
    qps_limiter.acquire()
    results = []

    # ä»æ–‡ä»¶é¡¹å¯¹è±¡ä¸­æå–ä¿¡æ¯
    fids = [item['fileId'] for item in chunk]
    names = [item['file_path'] for item in chunk]
    sizes = [item['size_gb'] for item in chunk]
    user_input_content = "\n".join(names)

    logging.info(f"ğŸ¬ å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(names)} ä¸ªæ–‡ä»¶")

    # ğŸš€ æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†
    cached_results = []
    uncached_names = []
    uncached_items = []

    current_time = time.time()

    for fid, name, size in zip(fids, names, sizes):
        cache_key = f"scrape_{hash(name)}"

        if cache_key in scraping_cache:
            cache_entry = scraping_cache[cache_key]
            if current_time - cache_entry['timestamp'] < SCRAPING_CACHE_DURATION:
                # ä½¿ç”¨ç¼“å­˜ç»“æœ
                cached_result = cache_entry['result'].copy()
                cached_result['fileId'] = fid  # æ›´æ–°æ–‡ä»¶ID
                cached_results.append(cached_result)
                logging.debug(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {os.path.basename(name)}")
                continue

        # éœ€è¦é‡æ–°å¤„ç†çš„æ–‡ä»¶
        uncached_names.append(name)
        uncached_items.append({'fileId': fid, 'file_path': name, 'size_gb': size})

    if cached_results:
        logging.info(f"ğŸ’¾ ä»ç¼“å­˜è·å¾— {len(cached_results)} ä¸ªç»“æœ")
        results.extend(cached_results)

    if not uncached_names:
        logging.info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰ç¼“å­˜ï¼Œæ— éœ€é‡æ–°å¤„ç†")
        return results

    logging.info(f"ğŸ”„ éœ€è¦é‡æ–°å¤„ç† {len(uncached_names)} ä¸ªæ–‡ä»¶")
    user_input_content = "\n".join(uncached_names)

    # ğŸš€ ä¼˜åŒ–ï¼šå‡å°‘é‡è¯•æ¬¡æ•°ä»3æ¬¡åˆ°2æ¬¡ï¼Œæé«˜é€Ÿåº¦
    movie_info = extract_movie_info_from_filename_enhanced(
        user_input_content,
        EXTRACTION_PROMPT,
        max_attempts=3  # ä»3å‡å°‘åˆ°2ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œé€Ÿåº¦
    )

    if not movie_info:
        logging.warning("âŒ æ²¡æœ‰ä»æ–‡ä»¶åä¸­æå–åˆ°ä»»ä½•ç”µå½±ä¿¡æ¯")
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºå¤±è´¥ç»“æœ
        for fid, original_filename, size in zip(fids, names, sizes):
            results.append({
                'fileId': fid,
                'original_name': os.path.basename(original_filename),
                'suggested_name': '',
                'size': size,
                'tmdb_info': None,
                'file_info': None,
                'status': 'extraction_failed'
            })
        return results

    logging.info(f"âœ… æˆåŠŸæå– {len(movie_info)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")

    # ğŸš€ å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
    def process_single_file(args):
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„TMDBæœç´¢å’Œå‘½å"""
        i, fid, file_info, size, original_filename = args
        file_basename = os.path.basename(original_filename)
        logging.info(f"ğŸ”„ å¤„ç†æ–‡ä»¶ {i+1}/{len(fids)}: {file_basename}")

        # ä¸º file_info æ·»åŠ  file_name å­—æ®µï¼Œç”¨äºåç»­å¤„ç†
        if isinstance(file_info, dict):
            file_info['file_name'] = original_filename
        else:
            logging.warning(f"âš ï¸ æ–‡ä»¶ {file_basename} çš„æå–ä¿¡æ¯æ ¼å¼å¼‚å¸¸")
            return {
                'fileId': fid,
                'original_name': file_basename,
                'suggested_name': '',
                'size': size,
                'tmdb_info': None,
                'file_info': file_info,
                'status': 'invalid_info'
            }

        try:
            # ğŸ¯ æ£€æŸ¥æ˜¯å¦åŒ…å«TMDB IDï¼Œå¦‚æœæœ‰åˆ™ä¼˜å…ˆéªŒè¯ä½†ä»è¿›è¡Œæœç´¢
            tmdb_id = file_info.get('tmdb_id', '')
            if tmdb_id and tmdb_id.isdigit():
                logging.info(f"ğŸ¯ å‘ç°TMDB ID: {tmdb_id}ï¼Œå°†è¿›è¡Œæœç´¢éªŒè¯")
                # ä»TMDB APIè·å–è¯¦ç»†ä¿¡æ¯è¿›è¡ŒéªŒè¯
                media_type = file_info.get('media_type', 'movie')

                try:
                    # æ ¹æ®åª’ä½“ç±»å‹è°ƒç”¨ç›¸åº”çš„TMDB API
                    if media_type in ['tv', 'tv_show', 'anime']:
                        url = f"{TMDB_API_URL_BASE}/tv/{tmdb_id}"
                    else:
                        url = f"{TMDB_API_URL_BASE}/movie/{tmdb_id}"

                    params = {
                        "api_key": TMDB_API_KEY,
                        "language": LANGUAGE,
                    }

                    response = requests.get(url, params=params, timeout=TMDB_API_TIMEOUT)
                    response.raise_for_status()
                    tmdb_candidate = response.json()

                    # éªŒè¯è¿™ä¸ªTMDB IDæ˜¯å¦ä¸æ–‡ä»¶ä¿¡æ¯åŒ¹é…
                    candidate_title = tmdb_candidate.get('name') or tmdb_candidate.get('title', '')
                    file_title = file_info.get('title', '')

                    # ç®€å•çš„æ ‡é¢˜åŒ¹é…éªŒè¯
                    if candidate_title and file_title:
                        title_similarity = len(set(candidate_title.lower().split()) & set(file_title.lower().split()))
                        if title_similarity >= 1:  # è‡³å°‘æœ‰ä¸€ä¸ªå…±åŒè¯æ±‡
                            logging.info(f"âœ… TMDB ID {tmdb_id} éªŒè¯é€šè¿‡: {candidate_title}")
                            tmdb_result = tmdb_candidate
                        else:
                            logging.warning(f"âš ï¸ TMDB ID {tmdb_id} éªŒè¯å¤±è´¥ï¼Œæ ‡é¢˜ä¸åŒ¹é…: '{candidate_title}' vs '{file_title}'ï¼Œå°†è¿›è¡Œæœç´¢")
                            tmdb_result = None
                    else:
                        logging.info(f"âœ… ä½¿ç”¨TMDB ID {tmdb_id}: {candidate_title}")
                        tmdb_result = tmdb_candidate

                except Exception as e:
                    logging.warning(f"âš ï¸ æ— æ³•éªŒè¯TMDB ID {tmdb_id}: {e}ï¼Œå°†è¿›è¡Œæœç´¢")
                    tmdb_result = None
            else:
                tmdb_result = None

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„TMDBç»“æœï¼Œè¿›è¡Œæœç´¢
            if not tmdb_result:
                # ä½¿ç”¨å¢å¼ºç‰ˆTMDBæœç´¢å‡½æ•°
                logging.info(f"ğŸ” å¼€å§‹TMDBæœç´¢: {file_info.get('title', 'Unknown')}")
                tmdb_result = search_movie_in_tmdb_enhanced(file_info, max_strategies=5)
            _, ext = os.path.splitext(original_filename)

            if tmdb_result:
                # è¯„ä¼°åŒ¹é…è´¨é‡
                match_quality = evaluate_tmdb_match_quality(file_info, tmdb_result)
                logging.info(f"ğŸ“Š TMDBåŒ¹é…è´¨é‡: {match_quality['score']:.1f}åˆ†")

                # æ ¹æ®åª’ä½“ç±»å‹ç¡®å®šå‘½åæ ¼å¼
                media_type = file_info.get('media_type', 'movie')
                suggested_name = ""

                if media_type in ['movie']:
                    # ç”µå½±å‘½åæ ¼å¼: Title (Year) {tmdb-id} size.ext
                    title = tmdb_result.get('title', 'Unknown')
                    release_date = tmdb_result.get('release_date', '0000-01-01')
                    tmdb_id = tmdb_result.get('id', 'unknown')
                    suggested_name = f"{title} ({release_date[:4]}) {{tmdb-{tmdb_id}}} {size}{ext}".replace('/', '')
                else:
                    # ç”µè§†å‰§å‘½åæ ¼å¼: Title (Year) S##E## {tmdb-id} size.ext
                    title = tmdb_result.get('name', 'Unknown')
                    first_air_date = tmdb_result.get('first_air_date', '0000-01-01')
                    tmdb_id = tmdb_result.get('id', 'unknown')

                    # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºTMDBç»“æœçš„è¯¦ç»†ä¿¡æ¯
                    logging.debug(f"ğŸ” TMDBç»“æœè¯¦æƒ…: name='{tmdb_result.get('name')}', first_air_date='{tmdb_result.get('first_air_date')}', id='{tmdb_result.get('id')}'")
                    logging.debug(f"ğŸ” æå–çš„æ ‡é¢˜ä¿¡æ¯: title='{title}', first_air_date='{first_air_date}', tmdb_id='{tmdb_id}'")

                    # ç¡®ä¿ season å’Œ episode æ˜¯æ•´æ•°ï¼Œå¦‚æœä¸º None æˆ–å…¶ä»–éæ•°å­—ç±»å‹ï¼Œåˆ™é»˜è®¤ä¸º 1
                    season = int(file_info.get('season', 1) or 1)
                    episode = int(file_info.get('episode', 1) or 1)

                    if file_info.get('episode') is not None:  # åªæœ‰å½“æœ‰å‰§é›†ä¿¡æ¯æ—¶æ‰ç”Ÿæˆå»ºè®®åç§°
                        suggested_name = f"{title} ({first_air_date[:4]}) S{season:02d}E{episode:02d} {{tmdb-{tmdb_id}}} {size}{ext}".replace('/', '')
                    else:
                        suggested_name = ""  # å¦‚æœæ²¡æœ‰å‰§é›†ä¿¡æ¯ï¼Œåˆ™ä¸ç”Ÿæˆå»ºè®®åç§°

                if suggested_name:
                    sanitized_output_string = sanitize_filename(suggested_name)
                    logging.info(f"âœ… æˆåŠŸç”Ÿæˆå»ºè®®åç§°: {file_basename} -> {sanitized_output_string}")

                    return {
                        'fileId': fid,
                        'original_name': file_basename,
                        'suggested_name': sanitized_output_string,
                        'size': size,
                        'tmdb_info': tmdb_result,
                        'file_info': file_info,
                        'match_quality': match_quality,
                        'status': 'success'
                    }
                else:
                    # æ²¡æœ‰ç”Ÿæˆå»ºè®®åç§°ï¼ˆé€šå¸¸æ˜¯ç”µè§†å‰§ç¼ºå°‘å‰§é›†ä¿¡æ¯ï¼‰
                    logging.warning(f"âš ï¸ æœªèƒ½ä¸º {file_basename} ç”Ÿæˆå»ºè®®åç§°ï¼ˆå¯èƒ½ç¼ºå°‘å‰§é›†ä¿¡æ¯ï¼‰")
                    return {
                        'fileId': fid,
                        'original_name': file_basename,
                        'suggested_name': '',
                        'size': size,
                        'tmdb_info': tmdb_result,
                        'file_info': file_info,
                        'match_quality': match_quality,
                        'status': 'no_episode_info'
                    }
            else:
                logging.warning(f"âŒ æœªæ‰¾åˆ° {file_basename} çš„TMDBåŒ¹é…ç»“æœ")
                return {
                    'fileId': fid,
                    'original_name': file_basename,
                    'suggested_name': '',
                    'size': size,
                    'tmdb_info': None,
                    'file_info': file_info,
                    'status': 'no_match'
                }

        except Exception as exc:
            logging.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_basename} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
            traceback.print_exc()
            return {
                'fileId': fid,
                'original_name': file_basename,
                'suggested_name': '',
                'size': size,
                'tmdb_info': None,
                'file_info': file_info,
                'status': 'error',
                'error': str(exc)
            }

    # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„å‚æ•°
    file_args = [(i, fid, file_info, size, original_filename)
                 for i, (fid, file_info, size, original_filename)
                 in enumerate(zip(fids, movie_info, sizes, names))]

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    from concurrent.futures import ThreadPoolExecutor, as_completed
    max_workers = min(len(file_args), MAX_WORKERS)  # ä½¿ç”¨é…ç½®çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°

    logging.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(file_args)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_args = {executor.submit(process_single_file, args): args for args in file_args}

        # æ”¶é›†ç»“æœå¹¶æ›´æ–°ç¼“å­˜
        for future in as_completed(future_to_args):
            try:
                result = future.result()
                results.append(result)

                # æ›´æ–°ç¼“å­˜ï¼ˆåªç¼“å­˜æˆåŠŸçš„ç»“æœï¼‰
                if result['status'] == 'success':
                    args = future_to_args[future]
                    original_filename = args[4]
                    cache_key = f"scrape_{hash(original_filename)}"
                    scraping_cache[cache_key] = {
                        'result': result.copy(),
                        'timestamp': current_time
                    }

            except Exception as exc:
                args = future_to_args[future]
                file_basename = os.path.basename(args[4])
                logging.error(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ {file_basename} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                results.append({
                    'fileId': args[1],
                    'original_name': file_basename,
                    'suggested_name': '',
                    'size': args[3],
                    'tmdb_info': None,
                    'file_info': args[2],
                    'status': 'error',
                    'error': str(exc)
                })

    # ç»Ÿè®¡å¤„ç†ç»“æœå¹¶è®°å½•è¯¦ç»†æ€»ç»“
    success_count = len([r for r in results if r['status'] == 'success'])
    total_count = len(results)
    success_rate = (success_count / total_count * 100) if total_count > 0 else 0

    logging.info(f"ğŸ¯ æ‰¹æ¬¡å¤„ç†å®Œæˆ: {success_count}/{total_count} æˆåŠŸ ({success_rate:.1f}%)")

    # è®°å½•TMDBæœç´¢æ€»ç»“
    log_tmdb_search_summary(results)

    # ğŸš€ ä¿å­˜æ–°å¤„ç†çš„ç»“æœåˆ°ç¼“å­˜
    current_time = time.time()
    for result in results:
        if result['fileId'] in [item['fileId'] for item in uncached_items]:
            # åªç¼“å­˜æ–°å¤„ç†çš„ç»“æœ
            original_name = result['original_name']
            cache_key = f"scrape_{hash(original_name)}"

            # åˆ›å»ºç¼“å­˜æ¡ç›®ï¼ˆä¸åŒ…å«fileIdï¼Œå› ä¸ºfileIdå¯èƒ½å˜åŒ–ï¼‰
            cache_result = result.copy()
            del cache_result['fileId']

            scraping_cache[cache_key] = {
                'result': cache_result,
                'timestamp': current_time
            }
            logging.debug(f"ğŸ’¾ ç¼“å­˜ç»“æœ: {original_name}")

    # LRUç¼“å­˜ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ¡ç›®ï¼Œæ— éœ€æ‰‹åŠ¨æ¸…ç†
    # è¿™é‡Œåªè®°å½•ä¸€ä¸‹ç¼“å­˜çŠ¶æ€
    cache_stats = scraping_cache.stats()
    logging.debug(f"ğŸ§¹ åˆ®å‰Šç¼“å­˜çŠ¶æ€: {cache_stats['size']}/{cache_stats['max_size']} é¡¹")

    return results



# ================================
# åº”ç”¨ç¨‹åºå¯åŠ¨åˆå§‹åŒ–
# ================================

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
initialize_logging_system()

# åŠ è½½åº”ç”¨é…ç½®
load_application_config()

# åˆå§‹åŒ–123äº‘ç›˜è®¿é—®ä»¤ç‰Œ
access_token = initialize_access_token()
if access_token:
    logging.info(f"âœ… 123äº‘ç›˜è®¿é—®ä»¤ç‰Œå·²åˆå§‹åŒ–: {access_token[:10]}***")
else:
    logging.info("âš ï¸ 123äº‘ç›˜è®¿é—®ä»¤ç‰Œæœªè®¾ç½®ï¼Œè¯·åœ¨é…ç½®é¡µé¢è®¾ç½®CLIENT_IDå’ŒCLIENT_SECRET")

# åˆå§‹åŒ–QPSé™åˆ¶å™¨
initialize_qps_limiters()

# Flask è·¯ç”±
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/logs')
def get_logs():
    # è¿”å›æ‰€æœ‰å½“å‰å­˜å‚¨çš„æ—¥å¿—
    return jsonify(list(log_queue))

@app.route('/config', methods=['GET'])
def get_config():
    """è¿”å›å½“å‰é…ç½®ã€‚"""
    return jsonify(app_config)

@app.route('/token_status', methods=['GET'])
def get_token_status():
    """è¿”å›è®¿é—®ä»¤ç‰ŒçŠ¶æ€ä¿¡æ¯"""
    global access_token, access_token_expires_at

    try:
        if not access_token:
            return jsonify({
                'has_token': False,
                'is_expired': True,
                'expires_at': None,
                'remaining_time': None,
                'message': 'æœªè®¾ç½®è®¿é—®ä»¤ç‰Œ'
            })

        # æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦è¿‡æœŸ
        is_expired = is_access_token_expired()

        # è®¡ç®—å‰©ä½™æ—¶é—´
        remaining_time = None
        expires_at_str = None
        if access_token_expires_at:
            expires_at_str = access_token_expires_at.strftime('%Y-%m-%d %H:%M:%S')
            if not is_expired:
                remaining_time = str(access_token_expires_at - datetime.datetime.now())

        return jsonify({
            'has_token': True,
            'is_expired': is_expired,
            'expires_at': expires_at_str,
            'remaining_time': remaining_time,
            'message': 'è®¿é—®ä»¤ç‰Œå·²è¿‡æœŸ' if is_expired else 'è®¿é—®ä»¤ç‰Œæœ‰æ•ˆ'
        })

    except Exception as e:
        logging.error(f"è·å–ä»¤ç‰ŒçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({
            'has_token': bool(access_token),
            'is_expired': True,
            'expires_at': None,
            'remaining_time': None,
            'message': f'æ£€æŸ¥ä»¤ç‰ŒçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        })

@app.route('/refresh_token', methods=['POST'])
def refresh_token():
    """æ‰‹åŠ¨åˆ·æ–°è®¿é—®ä»¤ç‰Œ"""
    try:
        success = refresh_access_token_if_needed()

        if success:
            # è·å–æ–°ä»¤ç‰Œçš„çŠ¶æ€ä¿¡æ¯
            expires_at_str = None
            remaining_time = None
            if access_token_expires_at:
                expires_at_str = access_token_expires_at.strftime('%Y-%m-%d %H:%M:%S')
                remaining_time = str(access_token_expires_at - datetime.datetime.now())

            return jsonify({
                'success': True,
                'message': 'è®¿é—®ä»¤ç‰Œåˆ·æ–°æˆåŠŸ',
                'expires_at': expires_at_str,
                'remaining_time': remaining_time
            })
        else:
            return jsonify({
                'success': False,
                'message': 'è®¿é—®ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œè¯·æ£€æŸ¥CLIENT_IDå’ŒCLIENT_SECRETé…ç½®'
            })

    except Exception as e:
        logging.error(f"æ‰‹åŠ¨åˆ·æ–°è®¿é—®ä»¤ç‰Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'message': f'åˆ·æ–°è®¿é—®ä»¤ç‰Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        })



@app.route('/test_ai_api', methods=['POST'])
def test_ai_api():
    """æµ‹è¯•AI APIè¿æ¥"""
    try:
        # è·å–å½“å‰é…ç½®ï¼ˆæ”¯æŒæ–°æ—§é…ç½®å­—æ®µï¼‰
        api_url = app_config.get("AI_API_URL", "") or app_config.get("GEMINI_API_URL", "")
        api_key = app_config.get("AI_API_KEY", "") or app_config.get("GEMINI_API_KEY", "")
        grouping_model = app_config.get("GROUPING_MODEL", "")

        # æ£€æŸ¥åŸºæœ¬é…ç½®
        if not api_url:
            return jsonify({
                'success': False,
                'error': 'GEMINI_API_URL æœªé…ç½®',
                'details': {
                    'api_url': api_url,
                    'model': grouping_model,
                    'api_key_status': 'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'
                }
            })

        if not api_key:
            return jsonify({
                'success': False,
                'error': 'GEMINI_API_KEY æœªé…ç½®',
                'details': {
                    'api_url': api_url,
                    'model': grouping_model,
                    'api_key_status': 'æœªè®¾ç½®'
                }
            })

        if not grouping_model:
            return jsonify({
                'success': False,
                'error': 'GROUPING_MODEL æœªé…ç½®',
                'details': {
                    'api_url': api_url,
                    'model': grouping_model,
                    'api_key_status': 'å·²è®¾ç½®'
                }
            })

        # æµ‹è¯•åŸºç¡€APIè¿æ¥
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        # ç®€å•çš„æµ‹è¯•è¯·æ±‚
        test_payload = {
            "model": grouping_model,
            "messages": [
                {
                    "role": "user",
                    "content": "è¯·å›å¤'AIè¿æ¥æµ‹è¯•æˆåŠŸ'"
                }
            ],
            "max_tokens": 50,
            "temperature": 0.1
        }

        logging.info(f"ğŸ§ª æµ‹è¯•AI APIè¿æ¥: {api_url}")
        logging.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {grouping_model}")

        response = requests.post(api_url, headers=headers, json=test_payload, timeout=AI_API_TIMEOUT)

        if response.status_code == 200:
            response_data = response.json()
            basic_response = response_data.get('choices', [{}])[0].get('message', {}).get('content', 'æ— å“åº”')

            # æµ‹è¯•æ™ºèƒ½åˆ†ç»„åŠŸèƒ½
            grouping_test_payload = {
                "model": grouping_model,
                "messages": [
                    {
                        "role": "user",
                        "content": """è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶åå¹¶è¿”å›JSONæ ¼å¼çš„åˆ†ç»„å»ºè®®ï¼š
æ–‡ä»¶åˆ—è¡¨ï¼š
1. å¤ä»‡è€…è”ç›Ÿ1.mkv
2. å¤ä»‡è€…è”ç›Ÿ2.mkv
3. é’¢é“ä¾ 1.mkv

è¯·è¿”å›JSONæ ¼å¼ï¼š
{
  "groups": [
    {
      "name": "å¤ä»‡è€…è”ç›Ÿç³»åˆ—",
      "files": ["å¤ä»‡è€…è”ç›Ÿ1.mkv", "å¤ä»‡è€…è”ç›Ÿ2.mkv"]
    }
  ]
}"""
                    }
                ],
                "max_tokens": 200,
                "temperature": 0.1
            }

            grouping_response = requests.post(api_url, headers=headers, json=grouping_test_payload, timeout=AI_API_TIMEOUT)
            grouping_result = "åˆ†ç»„æµ‹è¯•å¤±è´¥"

            if grouping_response.status_code == 200:
                grouping_data = grouping_response.json()
                grouping_result = grouping_data.get('choices', [{}])[0].get('message', {}).get('content', 'æ— å“åº”')

            return jsonify({
                'success': True,
                'details': {
                    'api_url': api_url,
                    'model': grouping_model,
                    'api_key_status': 'å·²è®¾ç½®',
                    'basic_response': basic_response,
                    'grouping_response': grouping_result
                }
            })
        else:
            error_text = response.text
            return jsonify({
                'success': False,
                'error': f'APIè¯·æ±‚å¤±è´¥ (HTTP {response.status_code}): {error_text}',
                'details': {
                    'api_url': api_url,
                    'model': grouping_model,
                    'api_key_status': 'å·²è®¾ç½®'
                }
            })

    except requests.exceptions.Timeout:
        return jsonify({
            'success': False,
            'error': 'APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡å™¨çŠ¶æ€',
            'details': {
                'api_url': app_config.get("GEMINI_API_URL", ""),
                'model': app_config.get("GROUPING_MODEL", ""),
                'api_key_status': 'å·²è®¾ç½®' if app_config.get("GEMINI_API_KEY") else 'æœªè®¾ç½®'
            }
        })
    except requests.exceptions.ConnectionError:
        return jsonify({
            'success': False,
            'error': 'APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥GEMINI_API_URLæ˜¯å¦æ­£ç¡®',
            'details': {
                'api_url': app_config.get("AI_API_URL", "") or app_config.get("GEMINI_API_URL", ""),
                'model': app_config.get("GROUPING_MODEL", ""),
                'api_key_status': 'å·²è®¾ç½®' if (app_config.get("AI_API_KEY") or app_config.get("GEMINI_API_KEY")) else 'æœªè®¾ç½®'
            }
        })
    except Exception as e:
        logging.error(f"âŒ AI APIæµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': f'æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}',
            'details': {
                'api_url': app_config.get("AI_API_URL", "") or app_config.get("GEMINI_API_URL", ""),
                'model': app_config.get("GROUPING_MODEL", ""),
                'api_key_status': 'å·²è®¾ç½®' if (app_config.get("AI_API_KEY") or app_config.get("GEMINI_API_KEY")) else 'æœªè®¾ç½®'
            }
        })


@app.route('/save_config', methods=['POST'])
def save_configuration():
    """ä¿å­˜ç”¨æˆ·æäº¤çš„é…ç½®ã€‚"""
    global app_config, QPS_LIMIT, CHUNK_SIZE, MAX_WORKERS, CLIENT_ID, CLIENT_SECRET
    global TMDB_API_KEY, AI_API_KEY, AI_API_URL, MODEL, GROUPING_MODEL, LANGUAGE
    global CURRENT_STORAGE_TYPE, STORAGE_CONFIG
    try:
        new_config_data = request.json
        logging.info(f"æ¥æ”¶åˆ°æ–°çš„é…ç½®æ•°æ®: {new_config_data}")

        # éªŒè¯å¹¶æ›´æ–°é…ç½®
        if "QPS_LIMIT" in new_config_data:
            app_config["QPS_LIMIT"] = int(new_config_data["QPS_LIMIT"])
        if "CHUNK_SIZE" in new_config_data:
            app_config["CHUNK_SIZE"] = int(new_config_data["CHUNK_SIZE"])
        if "MAX_WORKERS" in new_config_data:
            app_config["MAX_WORKERS"] = int(new_config_data["MAX_WORKERS"])
        if "CLIENT_ID" in new_config_data:
            app_config["CLIENT_ID"] = new_config_data["CLIENT_ID"]
        if "CLIENT_SECRET" in new_config_data:
            app_config["CLIENT_SECRET"] = new_config_data["CLIENT_SECRET"]

        if "TMDB_API_KEY" in new_config_data:
            app_config["TMDB_API_KEY"] = new_config_data["TMDB_API_KEY"]
        if "GEMINI_API_KEY" in new_config_data:
            app_config["GEMINI_API_KEY"] = new_config_data["GEMINI_API_KEY"]
        if "GEMINI_API_URL" in new_config_data:
            app_config["GEMINI_API_URL"] = new_config_data["GEMINI_API_URL"]
        if "MODEL" in new_config_data:
            app_config["MODEL"] = new_config_data["MODEL"]
        if "GROUPING_MODEL" in new_config_data:
            app_config["GROUPING_MODEL"] = new_config_data["GROUPING_MODEL"]
        if "LANGUAGE" in new_config_data:
            app_config["LANGUAGE"] = new_config_data["LANGUAGE"]

        # äº‘ç›˜é…ç½®
        if "STORAGE_TYPE" in new_config_data:
            app_config["STORAGE_TYPE"] = new_config_data["STORAGE_TYPE"]
        if "PAN115_COOKIE" in new_config_data:
            app_config["PAN115_COOKIE"] = new_config_data["PAN115_COOKIE"]

        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        save_application_config()

        # é‡æ–°åˆ›å»ºäº‘ç›˜æœåŠ¡å®ä¾‹
        CURRENT_STORAGE_TYPE = app_config.get("STORAGE_TYPE", "123pan")
        STORAGE_CONFIG = {
            "123pan": {
                "client_id": app_config.get("CLIENT_ID"),
                "client_secret": app_config.get("CLIENT_SECRET")
            },
            "115pan": {
                "cookie": app_config.get("PAN115_COOKIE")
            }
        }
        # cloud_service = create_cloud_service()  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œå½“å‰åªä½¿ç”¨123äº‘ç›˜

        # é‡æ–°åŠ è½½æ‰€æœ‰é…ç½®å’Œç›¸å…³å…¨å±€å˜é‡
        load_application_config()

        # ç¡®ä¿æ‰€æœ‰QPSé™åˆ¶å™¨éƒ½æ›´æ–°
        global qps_limiter, v2_list_limiter, rename_limiter, move_limiter, delete_limiter
        qps_limiter = QPSLimiter(qps_limit=app_config["QPS_LIMIT"])  # é€šç”¨é™åˆ¶å™¨ï¼Œä½¿ç”¨é…ç½®å€¼
        v2_list_limiter = QPSLimiter(qps_limit=4)     # api/v2/file/list: 4 QPS (å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§)
        rename_limiter = QPSLimiter(qps_limit=1)       # api/v1/file/rename: ä¿å®ˆä½¿ç”¨1 QPS
        move_limiter = QPSLimiter(qps_limit=3)        # api/v1/file/move: 3 QPS (æé«˜æ€§èƒ½)
        delete_limiter = QPSLimiter(qps_limit=2)       # api/v1/file/delete: 2 QPS (æé«˜æ€§èƒ½)

        logging.info("é…ç½®å·²æ›´æ–°å¹¶åº”ç”¨ã€‚")
        return jsonify({'success': True, 'message': 'é…ç½®ä¿å­˜æˆåŠŸå¹¶å·²åº”ç”¨ã€‚'})
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'ä¿å­˜é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'})



@app.route('/get_folder_content/<int:folder_id>', methods=['GET'])
def get_folder_content_by_id(folder_id):
    """é€šè¿‡GETæ–¹æ³•è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆç”¨äºæ–‡ä»¶å¤¹æµè§ˆå™¨ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    try:
        logging.info(f"ğŸ“‚ è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„å†…å®¹")

        # ğŸš€ æ£€æŸ¥ç¼“å­˜ï¼ˆä½¿ç”¨ä¸åŒçš„ç¼“å­˜é”®ä»¥åŒºåˆ†GETå’ŒPOSTè¯·æ±‚ï¼‰
        cache_key_suffix = "_folders_only"
        cached_content = get_cached_folder_content(f"{folder_id}{cache_key_suffix}")
        if cached_content is not None:
            logging.info(f"âš¡ ä½¿ç”¨ç¼“å­˜çš„ç›®å½•å†…å®¹ï¼ˆä»…æ–‡ä»¶å¤¹ï¼‰ï¼Œè·³è¿‡APIè°ƒç”¨")
            return jsonify(cached_content)

        limit = 100
        paths = []
        fid = folder_id
        if fid == 0:
            paths.append({"fileID": 0, "filename": "æ ¹ç›®å½•", "parentFileID": 0})
        else:
            while True:
                folder_details = detail(fid)
                fid = int(folder_details["parentFileID"])
                paths.append(folder_details)
                if fid == 0:
                    paths.append({"fileID": 0, "filename": "æ ¹ç›®å½•", "parentFileID": 0})
                    break

        paths.reverse()
        # logging.info(f"paths: {paths}")
        current_path_parts = [a['filename'] for a in paths[1:]]
        current_path_prefix = "/".join(current_path_parts)
        logging.info(f"current_path_prefix: {current_path_prefix}")
        folder_content = get_all_files_in_folder(folder_id, limit=limit)
        # logging.info(folder_content)

        # åªè¿”å›æ–‡ä»¶å¤¹ï¼Œä¸è¿”å›æ–‡ä»¶ï¼ˆç”¨äºæ–‡ä»¶å¤¹æµè§ˆå™¨ï¼‰
        folders = []
        for item in folder_content:
            if item['type'] == 1:  # åªå¤„ç†æ–‡ä»¶å¤¹
                folders.append({
                    'filename': item['filename'],
                    'fileId': item['fileId'],
                    'parentFileId': item['parentFileId'],
                    'file_name': item.get('file_name', os.path.join(current_path_prefix, item['filename']))
                })

        # æ„å»ºè·¯å¾„ä¿¡æ¯
        path_info = {
            'path_parts': [{'id': path['fileID'], 'name': path['filename']} for path in paths],
            'parent_id': paths[-2]['fileID'] if len(paths) > 1 else 0
        }

        logging.info(f"ğŸ“ æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹")

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'success': True,
            'folders': folders,
            'path_info': path_info,
            'current_folder_id': folder_id
        }

        # ğŸš€ ç¼“å­˜å“åº”æ•°æ®ï¼ˆä»…æ–‡ä»¶å¤¹ï¼‰
        cache_key = f"folder_{folder_id}_folders_only"
        cache_folder_content(cache_key, response_data)

        return jsonify(response_data)

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/get_folder_content', methods=['POST'])
def get_folder_content():
    """è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    try:
        # é‡ç½®ä»»åŠ¡çŠ¶æ€ï¼Œé¿å…ä¹‹å‰çš„å–æ¶ˆçŠ¶æ€å½±å“æ™®é€šæµè§ˆ
        reset_task_state()

        folder_id = request.form.get('folder_id', '0')
        limit = int(request.form.get('limit', 100))
        folder_id = int(folder_id)

        logging.info(f"ğŸ“‚ è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„å†…å®¹")

        # ğŸš€ æ£€æŸ¥ç¼“å­˜
        cached_content = get_cached_folder_content(folder_id)
        if cached_content is not None:
            logging.info(f"âš¡ ä½¿ç”¨ç¼“å­˜çš„ç›®å½•å†…å®¹ï¼Œè·³è¿‡APIè°ƒç”¨")
            return jsonify(cached_content)

        paths = []
        fid = folder_id
        if fid == 0:
            paths.append({"fileID": 0, "filename": "æ ¹ç›®å½•", "parentFileID": 0})
        else:
            while True:
                folder_details = detail(fid)
                # fileID = folder_details["fileID"]
                # filename = folder_details["filename"]
                fid = int(folder_details["parentFileID"])
                paths.append(folder_details)
                if fid == 0:
                    paths.append({"fileID": 0, "filename": "æ ¹ç›®å½•", "parentFileID": 0})
                    break 

        paths.reverse()
        # logging.info(f"paths: {paths}")
        current_path_parts = [a['filename'] for a in paths[1:]]
        current_path_prefix = "/".join(current_path_parts)
        # logging.info(f"ğŸ“‚ æ–‡ä»¶å¤¹è·¯å¾„ : {current_path_prefix}")
        folder_content = get_all_files_in_folder(folder_id, limit=limit)
        # logging.info(folder_content)



        all_files_and_folders = []
        for item in folder_content:
            if item['type'] == 1:  # æ–‡ä»¶å¤¹
                all_files_and_folders.append({
                    'name': item['filename'],
                    'is_dir': True,
                    'fileId': item['fileId'],
                    'parentFileId': item['parentFileId'],
                    'file_name': item.get('file_name', os.path.join(current_path_prefix, item['filename']))
                })

            elif item['type'] == 0:  # æ–‡ä»¶
                _, ext = os.path.splitext(item['filename'])
                if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                    bytes_value = item['size']
                    gb_in_bytes = 1024 ** 3
                    gb_value = bytes_value / gb_in_bytes
                    size_str = f"{gb_value:.1f}GB"

                    all_files_and_folders.append({
                        'name': item['filename'],
                        'is_dir': False,
                        'fileId': item['fileId'],
                        'parentFileId': item['parentFileId'],
                        'size': size_str,
                        'file_name': item.get('file_name', os.path.join(current_path_prefix, item['filename']))

                    })


        current_path = "/" + "/".join([part['filename'] for part in paths[1:]])
        if current_path == "/":
            current_path = "/æ ¹ç›®å½•"

        if len(paths) < 2:
            parent_folder_id = '0'
        else:
            parent_folder_id = str(paths[-2]['fileID'])

        logging.info(f"ğŸ“„ æ‰¾åˆ° {len(all_files_and_folders)} ä¸ªé¡¹ç›®")
        logging.info(f"ğŸ“ å½“å‰è·¯å¾„: {current_path}")
        logging.info(f"â¬†ï¸ çˆ¶æ–‡ä»¶å¤¹ID: {parent_folder_id}")

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            'success': True,
            'current_folder_id': str(folder_id),
            'parent_folder_id': parent_folder_id,
            'current_path': current_path,
            'files_and_folders': all_files_and_folders,
            'total_count': len(all_files_and_folders),
            'path_parts': [{'name': path['filename'], 'fileId': str(path['fileID'])} for path in paths]
        }

        # ğŸš€ ç¼“å­˜å“åº”æ•°æ®ï¼ˆåªç¼“å­˜æˆåŠŸçš„ç»“æœï¼‰
        cache_folder_content(folder_id, response_data)

        return jsonify(response_data)

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})


@app.route('/get_folder_info', methods=['GET'])
def get_folder_info():
    """è·å–å•ä¸ªæ–‡ä»¶å¤¹çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè½»é‡çº§ï¼Œç”¨äºç‚¹å‡»æŸ¥çœ‹ï¼‰"""
    try:
        folder_id = request.args.get('folder_id', '0')

        # éªŒè¯æ–‡ä»¶å¤¹ID
        folder_id, error_msg = validate_folder_id(folder_id)
        if error_msg:
            return jsonify({'success': False, 'error': error_msg})

        logging.info(f"ğŸ” è·å–æ–‡ä»¶å¤¹ {folder_id} çš„åŸºæœ¬ä¿¡æ¯")

        # åªè·å–ç›´æ¥å­é¡¹ï¼Œä¸é€’å½’ï¼Œæé«˜é€Ÿåº¦
        try:
            folder_content = get_file_list_from_cloud(folder_id, limit=100)
            if not folder_content or 'fileList' not in folder_content:
                return jsonify({'success': False, 'error': 'æ— æ³•è·å–æ–‡ä»¶å¤¹å†…å®¹'})

            files = folder_content['fileList']

            # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
            total_items = len(files)
            folder_count = sum(1 for item in files if item.get('type') == 1)
            file_count = sum(1 for item in files if item.get('type') == 0)
            video_count = 0
            total_size = 0

            # ç»Ÿè®¡è§†é¢‘æ–‡ä»¶å’Œå¤§å°
            for item in files:
                if item.get('type') == 0:  # æ–‡ä»¶
                    file_size = item.get('size', 0)
                    if isinstance(file_size, (int, float)):
                        total_size += file_size

                    # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                    filename = item.get('filename', '')
                    _, ext = os.path.splitext(filename)
                    if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                        video_count += 1

            # æ ¼å¼åŒ–å¤§å°
            if total_size >= 1024 ** 4:  # TB
                size_str = f"{total_size / (1024 ** 4):.1f}TB"
            elif total_size >= 1024 ** 3:  # GB
                size_str = f"{total_size / (1024 ** 3):.1f}GB"
            elif total_size >= 1024 ** 2:  # MB
                size_str = f"{total_size / (1024 ** 2):.1f}MB"
            elif total_size >= 1024:  # KB
                size_str = f"{total_size / 1024:.1f}KB"
            else:
                size_str = f"{int(total_size)}B"

            # è·å–æ–‡ä»¶å¤¹è·¯å¾„
            folder_path = get_folder_full_path(folder_id)

            result = {
                'success': True,
                'folder_id': folder_id,
                'folder_path': folder_path,
                'total_items': total_items,
                'folder_count': folder_count,
                'file_count': file_count,
                'video_count': video_count,
                'size': size_str,
                'total_size_bytes': int(total_size)
            }

            logging.info(f"ğŸ“Š æ–‡ä»¶å¤¹ {folder_id} ä¿¡æ¯: {total_items} é¡¹ç›® ({folder_count} æ–‡ä»¶å¤¹, {file_count} æ–‡ä»¶, {video_count} è§†é¢‘), å¤§å° {size_str}")
            return jsonify(result)

        except Exception as e:
            logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {e}")
            return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {str(e)}'})

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/get_folder_properties', methods=['POST'])
def get_folder_properties():
    """è·å–æ–‡ä»¶å¤¹åŸºæœ¬å±æ€§ï¼ˆæ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°ï¼‰- å¿«é€Ÿç‰ˆæœ¬"""
    try:
        folder_id = request.form.get('folder_id', '0')
        include_grouping = request.form.get('include_grouping', 'false').lower() == 'true'

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        try:
            folder_id = int(folder_id)
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'})

        logging.info(f"ğŸ” è·å–æ–‡ä»¶å¤¹ {folder_id} çš„åŸºæœ¬å±æ€§")

        if include_grouping:
            # ğŸš€ ä½¿ç”¨æ–°çš„ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿè¿›è¡Œæ™ºèƒ½åˆ†ç»„
            try:
                # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨æ˜¯å¦å¯ç”¨
                if grouping_task_manager is None:
                    return jsonify({
                        'success': False,
                        'error': 'ä»»åŠ¡ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€',
                        'task_queue_error': True
                    })

                # è·å–æ–‡ä»¶å¤¹åç§°
                folder_name = request.form.get('folder_name', f'æ–‡ä»¶å¤¹{folder_id}')

                # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
                task_id = grouping_task_manager.submit_task(folder_id, folder_name)

                return jsonify({
                    'success': True,
                    'use_task_queue': True,
                    'task_id': task_id,
                    'message': f'æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤åˆ°é˜Ÿåˆ— (ä»»åŠ¡ID: {task_id})'
                })

            except ValueError as e:
                # å¦‚æœæ˜¯é‡å¤ä»»åŠ¡æˆ–é˜Ÿåˆ—æ»¡çš„é”™è¯¯ï¼Œè¿”å›ç›¸åº”ä¿¡æ¯
                return jsonify({
                    'success': False,
                    'error': str(e),
                    'task_queue_error': True
                })
        else:
            # ğŸ” åªè·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯ï¼Œä¸è¿›è¡Œæ™ºèƒ½åˆ†ç»„
            video_files = []
            try:
                get_video_files_recursively(folder_id, video_files)
            except Exception as e:
                logging.error(f"é€’å½’è·å–è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}'})

            return jsonify({
                'success': True,
                'count': len(video_files),
                'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB",
                'video_files': video_files
            })

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å±æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})


@app.route('/get_folder_grouping_analysis', methods=['POST'])
def get_folder_grouping_analysis():
    """è·å–æ–‡ä»¶å¤¹çš„æ™ºèƒ½åˆ†ç»„åˆ†æ - è¯¦ç»†ç‰ˆæœ¬"""
    # ç”¨äºæ”¶é›†å¤„ç†è¿‡ç¨‹ä¿¡æ¯çš„åˆ—è¡¨
    process_logs = []

    def add_process_log(message, level='info'):
        """æ·»åŠ å¤„ç†è¿‡ç¨‹æ—¥å¿—"""
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        process_logs.append({
            'timestamp': timestamp,
            'level': level,
            'message': message
        })
        # åŒæ—¶è®°å½•åˆ°ç³»ç»Ÿæ—¥å¿—
        if level == 'error':
            logging.error(message)
        elif level == 'warning':
            logging.warning(message)
        else:
            logging.info(message)

    try:
        # å¼€å§‹æ–°ä»»åŠ¡
        task_id = f"folder_grouping_{int(time.time())}"
        start_new_task(task_id)
        add_process_log(f"ğŸš€ å¼€å§‹æ–°ä»»åŠ¡: {task_id}")

        folder_id = request.form.get('folder_id', '0')

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            add_process_log("âŒ æ— æ•ˆçš„æ–‡ä»¶å¤¹ID", 'error')
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID', 'process_logs': process_logs})

        try:
            folder_id = int(folder_id)
        except (ValueError, TypeError):
            add_process_log("âŒ æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—", 'error')
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—', 'process_logs': process_logs})

        add_process_log(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶å¤¹ {folder_id} çš„æ™ºèƒ½åˆ†ç»„")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        try:
            check_task_cancelled()
        except Exception as e:
            if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                add_process_log("ğŸ›‘ ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ", 'warning')
                return jsonify({'success': False, 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True, 'process_logs': process_logs})

        # é€’å½’è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶
        video_files = []
        try:
            add_process_log(f"ğŸ“‚ å¼€å§‹æ‰«ææ–‡ä»¶å¤¹ {folder_id} ä¸­çš„è§†é¢‘æ–‡ä»¶")
            get_video_files_recursively(folder_id, video_files)
            add_process_log(f"âœ… æ‰«æå®Œæˆï¼Œå‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        except Exception as e:
            add_process_log(f"âŒ é€’å½’è·å–è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", 'error')
            return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}', 'process_logs': process_logs})

        # è°ƒç”¨å†…éƒ¨åˆ†ç»„åˆ†æå‡½æ•°
        try:
            grouping_result = get_folder_grouping_analysis_internal(video_files, folder_id, add_process_log)
            grouping_result['process_logs'] = process_logs
            return jsonify(grouping_result)
        except Exception as e:
            add_process_log(f"âŒ åˆ†ç»„åˆ†æå¤±è´¥: {e}", 'error')
            return jsonify({'success': False, 'error': str(e), 'process_logs': process_logs})



    except Exception as e:
        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
            add_process_log("ğŸ›‘ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ", 'warning')
            return jsonify({'success': False, 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True, 'process_logs': process_logs})
        else:
            add_process_log(f"âŒ åˆ†ç»„åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}", 'error')
            return jsonify({'success': False, 'error': str(e), 'process_logs': process_logs})

def _calculate_total_size(video_files):
    """è®¡ç®—æ–‡ä»¶æ€»å¤§å°"""
    total_bytes = 0
    for file_item in video_files:
        try:
            size_str = file_item.get('size_gb', '')
            if not size_str:
                continue

            # è§£æä¸åŒå•ä½çš„æ–‡ä»¶å¤§å°
            if 'GB' in size_str:
                total_bytes += float(size_str.replace('GB', '')) * (1024 ** 3)
            elif 'MB' in size_str:
                total_bytes += float(size_str.replace('MB', '')) * (1024 ** 2)
            elif 'KB' in size_str:
                total_bytes += float(size_str.replace('KB', '')) * 1024
            elif size_str.isdigit():
                total_bytes += int(size_str)
        except (ValueError, KeyError):
            continue
    return total_bytes


def _format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    units = [(1024**4, 'TB'), (1024**3, 'GB'), (1024**2, 'MB'), (1024, 'KB')]
    for threshold, unit in units:
        if size_bytes >= threshold:
            return f"{size_bytes / threshold:.1f}{unit}"
    return f"{int(size_bytes)}B"





# ğŸ”„ æ™ºèƒ½åˆ†ç»„ç¼“å­˜ç®¡ç†å‡½æ•°
def generate_cache_key(video_files, folder_id):
    """ç”Ÿæˆç¼“å­˜é”®"""
    # ä½¿ç”¨æ–‡ä»¶å¤¹IDå’Œæ–‡ä»¶åˆ—è¡¨çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®
    file_info = []
    for file in video_files:
        file_info.append(f"{file['filename']}_{file.get('fileID', '')}")

    file_hash = hashlib.md5('|'.join(sorted(file_info)).encode('utf-8')).hexdigest()
    return f"grouping_{folder_id}_{file_hash}"

def get_cached_grouping_result(cache_key):
    """è·å–ç¼“å­˜çš„åˆ†ç»„ç»“æœ"""
    global grouping_cache

    if cache_key not in grouping_cache:
        return None

    cached_data = grouping_cache[cache_key]
    cache_time = cached_data.get('timestamp', 0)
    current_time = time.time()

    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
    if current_time - cache_time > GROUPING_CACHE_DURATION:
        # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤å¹¶è¿”å›None
        del grouping_cache[cache_key]
        logging.info(f"ğŸ—‘ï¸ ç¼“å­˜å·²è¿‡æœŸå¹¶æ¸…ç†: {cache_key}")
        return None

    logging.info(f"ğŸ¯ å‘½ä¸­ç¼“å­˜: {cache_key}, å‰©ä½™æœ‰æ•ˆæœŸ: {int(GROUPING_CACHE_DURATION - (current_time - cache_time))}ç§’")
    return cached_data.get('result')

def cache_grouping_result(cache_key, result):
    """ç¼“å­˜åˆ†ç»„ç»“æœ"""
    global grouping_cache

    grouping_cache[cache_key] = {
        'timestamp': time.time(),
        'result': result
    }

    logging.info(f"ğŸ’¾ ç¼“å­˜åˆ†ç»„ç»“æœ: {cache_key}")

    # æ¸…ç†è¿‡æœŸçš„ç¼“å­˜é¡¹ï¼ˆç®€å•çš„æ¸…ç†ç­–ç•¥ï¼‰
    cleanup_expired_cache()

def cleanup_expired_cache():
    """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜é¡¹"""
    global grouping_cache

    # LRUç¼“å­˜ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ¡ç›®ï¼Œæ— éœ€æ‰‹åŠ¨æ¸…ç†
    # è¿™é‡Œåªè®°å½•ä¸€ä¸‹ç¼“å­˜çŠ¶æ€
    cache_stats = grouping_cache.stats()
    logging.debug(f"ğŸ§¹ åˆ†ç»„ç¼“å­˜çŠ¶æ€: {cache_stats['size']}/{cache_stats['max_size']} é¡¹")

# ğŸ“ ç›®å½•å†…å®¹ç¼“å­˜ç®¡ç†å‡½æ•°
def get_cached_folder_content(folder_id_or_key):
    """è·å–ç¼“å­˜çš„ç›®å½•å†…å®¹"""
    # æ”¯æŒç›´æ¥ä¼ å…¥ç¼“å­˜é”®æˆ–æ–‡ä»¶å¤¹ID
    if isinstance(folder_id_or_key, str) and folder_id_or_key.startswith("folder_"):
        cache_key = folder_id_or_key
    else:
        cache_key = f"folder_{folder_id_or_key}"

    cached_data = folder_content_cache.get(cache_key)
    if cached_data is None:
        return None

    # LRUç¼“å­˜å·²ç»å¤„ç†äº†è¿‡æœŸé€»è¾‘ï¼Œç›´æ¥è¿”å›å†…å®¹
    logging.info(f"ğŸ’¾ å‘½ä¸­ç›®å½•ç¼“å­˜: {cache_key}")
    return cached_data.get('content')

def cache_folder_content(folder_id_or_key, content):
    """ç¼“å­˜ç›®å½•å†…å®¹"""
    # æ”¯æŒç›´æ¥ä¼ å…¥ç¼“å­˜é”®æˆ–æ–‡ä»¶å¤¹ID
    if isinstance(folder_id_or_key, str) and folder_id_or_key.startswith("folder_"):
        cache_key = folder_id_or_key
    else:
        cache_key = f"folder_{folder_id_or_key}"

    # ä½¿ç”¨LRUç¼“å­˜å­˜å‚¨å†…å®¹
    folder_content_cache.put(cache_key, {
        'timestamp': time.time(),
        'content': content
    })

    # ä»contentä¸­è·å–é¡¹ç›®æ•°é‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    item_count = "æœªçŸ¥"
    if isinstance(content, dict):
        if 'files_and_folders' in content:
            item_count = len(content['files_and_folders'])
        elif 'folders' in content:
            item_count = len(content['folders'])

    logging.info(f"ğŸ’¾ ç¼“å­˜ç›®å½•å†…å®¹: {cache_key} ({item_count} ä¸ªé¡¹ç›®)")

def cleanup_expired_folder_cache():
    """æ¸…ç†è¿‡æœŸçš„ç›®å½•ç¼“å­˜é¡¹"""
    # LRUç¼“å­˜è‡ªåŠ¨å¤„ç†è¿‡æœŸï¼Œè¿™é‡Œåªéœ€è¦è°ƒç”¨æ¸…ç†æ–¹æ³•
    expired_count = folder_content_cache.cleanup_expired()
    if expired_count > 0:
        logging.info(f"ğŸ§¹ æ¸…ç†äº† {expired_count} ä¸ªè¿‡æœŸç›®å½•ç¼“å­˜é¡¹")

def clear_folder_cache(folder_id=None):
    """æ¸…ç†æŒ‡å®šæ–‡ä»¶å¤¹çš„ç¼“å­˜ï¼Œå¦‚æœfolder_idä¸ºNoneåˆ™æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
    if folder_id is None:
        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
        count = folder_content_cache.size()
        folder_content_cache.clear()
        logging.info(f"ğŸ§¹ æ¸…ç†äº†æ‰€æœ‰ç›®å½•ç¼“å­˜ ({count} ä¸ªé¡¹ç›®)")
    else:
        # æ¸…ç†æŒ‡å®šæ–‡ä»¶å¤¹çš„ç¼“å­˜
        cache_key = f"folder_{folder_id}"
        folder_content_cache.delete(cache_key)
        logging.info(f"ğŸ§¹ æ¸…ç†äº†æ–‡ä»¶å¤¹ {folder_id} çš„ç¼“å­˜")

        # åŒæ—¶æ¸…ç†å­æ–‡ä»¶å¤¹çš„ç¼“å­˜ï¼ˆå› ä¸ºçˆ¶æ–‡ä»¶å¤¹å˜åŒ–å¯èƒ½å½±å“å­æ–‡ä»¶å¤¹ï¼‰
        # æ³¨æ„ï¼šLRUç¼“å­˜ä¸æ”¯æŒç›´æ¥éå†keysï¼Œè¿™é‡Œéœ€è¦é‡æ–°è®¾è®¡
        # æš‚æ—¶åªæ¸…ç†ç›´æ¥ç¼“å­˜ï¼Œå­æ–‡ä»¶å¤¹ç¼“å­˜ä¼šè‡ªç„¶è¿‡æœŸ
        logging.info(f"ğŸ§¹ å·²æ¸…ç†æ–‡ä»¶å¤¹ {folder_id} çš„ç›´æ¥ç¼“å­˜ï¼Œç›¸å…³å­æ–‡ä»¶å¤¹ç¼“å­˜å°†è‡ªç„¶è¿‡æœŸ")

# ğŸš¦ è¯·æ±‚é™æµæ§åˆ¶å‡½æ•°
def is_folder_request_rate_limited(folder_id):
    """æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦è¢«é™æµ"""
    global folder_request_tracker

    current_time = time.time()
    folder_id_str = str(folder_id)

    # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
    cleanup_expired_requests()

    # æ£€æŸ¥è¯¥æ–‡ä»¶å¤¹çš„è¯·æ±‚å†å²
    if folder_id_str not in folder_request_tracker:
        folder_request_tracker[folder_id_str] = []

    folder_requests = folder_request_tracker[folder_id_str]

    # è¿‡æ»¤å‡ºæ—¶é—´çª—å£å†…çš„è¯·æ±‚
    recent_requests = [
        req_time for req_time in folder_requests
        if current_time - req_time < FOLDER_REQUEST_LIMIT_DURATION
    ]

    # æ›´æ–°è¯·æ±‚åˆ—è¡¨
    folder_request_tracker[folder_id_str] = recent_requests

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    if len(recent_requests) >= MAX_REQUESTS_PER_FOLDER:
        oldest_request = min(recent_requests)
        remaining_time = int(FOLDER_REQUEST_LIMIT_DURATION - (current_time - oldest_request))
        logging.warning(f"ğŸš¦ æ–‡ä»¶å¤¹ {folder_id} è¯·æ±‚è¢«é™æµï¼Œå‰©ä½™ç­‰å¾…æ—¶é—´: {remaining_time}ç§’")
        return True, remaining_time

    return False, 0

def record_folder_request(folder_id):
    """è®°å½•æ–‡ä»¶å¤¹è¯·æ±‚"""
    global folder_request_tracker

    current_time = time.time()
    folder_id_str = str(folder_id)

    if folder_id_str not in folder_request_tracker:
        folder_request_tracker[folder_id_str] = []

    folder_request_tracker[folder_id_str].append(current_time)
    logging.info(f"ğŸ“ è®°å½•æ–‡ä»¶å¤¹ {folder_id} çš„è¯·æ±‚æ—¶é—´: {current_time}")

def cleanup_expired_requests():
    """æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•"""
    global folder_request_tracker

    current_time = time.time()
    expired_folders = []

    for folder_id, requests in folder_request_tracker.items():
        # è¿‡æ»¤å‡ºä»åœ¨æ—¶é—´çª—å£å†…çš„è¯·æ±‚
        valid_requests = [
            req_time for req_time in requests
            if current_time - req_time < FOLDER_REQUEST_LIMIT_DURATION
        ]

        if valid_requests:
            folder_request_tracker[folder_id] = valid_requests
        else:
            expired_folders.append(folder_id)

    # åˆ é™¤æ²¡æœ‰æœ‰æ•ˆè¯·æ±‚çš„æ–‡ä»¶å¤¹è®°å½•
    for folder_id in expired_folders:
        del folder_request_tracker[folder_id]

    if expired_folders:
        logging.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_folders)} ä¸ªæ–‡ä»¶å¤¹çš„è¿‡æœŸè¯·æ±‚è®°å½•")

def get_folder_grouping_analysis_internal(video_files, folder_id, log_func=None):
    """å†…éƒ¨åˆ†ç»„åˆ†æå‡½æ•° - ä¼˜åŒ–ç‰ˆï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    log_func = log_func or logging.info

    # ğŸ”„ æ£€æŸ¥ç¼“å­˜
    cache_key = generate_cache_key(video_files, folder_id)
    cached_result = get_cached_grouping_result(cache_key)

    if cached_result is not None:
        log_func("âš¡ ä½¿ç”¨ç¼“å­˜çš„åˆ†ç»„ç»“æœï¼Œè·³è¿‡AIåˆ†æ")
        return cached_result

    # è®¡ç®—åŸºæœ¬ä¿¡æ¯
    file_count = len(video_files)
    total_size_bytes = _calculate_total_size(video_files)
    size_str = _format_file_size(total_size_bytes)

    # æ™ºèƒ½åˆ†ç»„åˆ†æ
    try:
        if video_files:
            log_func("ğŸ¯ å¼€å§‹æ™ºèƒ½æ–‡ä»¶åˆ†ç»„åˆ†æ")
            log_func(f"ğŸ“Š æ€»æ–‡ä»¶æ•°é‡: {len(video_files)} ä¸ª")

            # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥æŒ‰æ–‡ä»¶æ•°é‡åˆ†æ‰¹ï¼Œè€Œä¸æ˜¯æŒ‰å­æ–‡ä»¶å¤¹åˆ†ç»„
            all_enhanced_groups = []

            # ä½¿ç”¨é…ç½®çš„æ‰¹å¤„ç†å¤§å°
            log_func(f"ğŸ“¦ ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {CHUNK_SIZE} ä¸ªæ–‡ä»¶/æ‰¹")

            # ğŸš€ ç®€åŒ–ç­–ç•¥ï¼šç›´æ¥æŒ‰æ–‡ä»¶æ•°é‡åˆ†æ‰¹å¤„ç†
            if len(video_files) > CHUNK_SIZE:
                batches = split_files_into_batches(video_files, CHUNK_SIZE)
                log_func(f"ğŸ“¦ åˆ†æ‰¹å¤„ç†: {len(batches)} æ‰¹ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°")

                for i, batch_files in enumerate(batches):
                    try:
                        check_task_cancelled()
                    except:
                        logging.warning("âš ï¸ ä»»åŠ¡å¯èƒ½å·²è¢«å–æ¶ˆï¼Œä½†ç»§ç»­å¤„ç†")

                    log_func(f"ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(batches)} æ‰¹: {len(batch_files)} ä¸ªæ–‡ä»¶")

                    # æ·»åŠ è¶…æ—¶ä¿æŠ¤
                    batch_start_time = time.time()
                    try:
                        batch_groups_result = process_files_for_grouping(batch_files, f"æ‰¹æ¬¡{i+1}")
                        batch_process_time = time.time() - batch_start_time

                        if batch_groups_result:
                            all_enhanced_groups.extend(batch_groups_result)
                            log_func(f"âœ… ç¬¬ {i+1} æ‰¹å¤„ç†å®Œæˆ: ç”Ÿæˆ {len(batch_groups_result)} ä¸ªåˆ†ç»„ (è€—æ—¶: {batch_process_time:.1f}ç§’)")
                        else:
                            log_func(f"â­ï¸ ç¬¬ {i+1} æ‰¹æœªç”Ÿæˆæœ‰æ•ˆåˆ†ç»„ (è€—æ—¶: {batch_process_time:.1f}ç§’)")

                    except Exception as e:
                        batch_process_time = time.time() - batch_start_time
                        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                            log_func(f"âš ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆï¼Œåœæ­¢å¤„ç†")
                            raise
                        log_func(f"âŒ ç¬¬ {i+1} æ‰¹å¤„ç†å¤±è´¥: {e} (è€—æ—¶: {batch_process_time:.1f}ç§’)")
                        continue

                    # æ›´æ–°è¿›åº¦
                    progress = 50.0 + ((i + 1) / len(batches)) * 30.0
                    log_func(f"ğŸ”„ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {progress:.3f}% - ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(batches)} æ‰¹: {len(batch_files)} ä¸ªæ–‡ä»¶")
            else:
                # å•æ‰¹å¤„ç†
                log_func(f"ğŸ“Š å•æ‰¹å¤„ç†: {len(video_files)} ä¸ªæ–‡ä»¶")
                batch_start_time = time.time()
                try:
                    all_enhanced_groups = process_files_for_grouping(video_files, "å…¨éƒ¨æ–‡ä»¶")
                    batch_process_time = time.time() - batch_start_time
                    log_func(f"âœ… å•æ‰¹å¤„ç†å®Œæˆ: ç”Ÿæˆ {len(all_enhanced_groups) if all_enhanced_groups else 0} ä¸ªåˆ†ç»„ (è€—æ—¶: {batch_process_time:.1f}ç§’)")
                except Exception as e:
                    batch_process_time = time.time() - batch_start_time
                    if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                        log_func(f"âš ï¸ ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆï¼Œåœæ­¢å¤„ç†")
                        raise
                    log_func(f"âŒ å•æ‰¹å¤„ç†å¤±è´¥: {e} (è€—æ—¶: {batch_process_time:.1f}ç§’)")
                    all_enhanced_groups = []

            # ğŸ”„ ç¬¬ä¸€æ­¥ï¼šåˆå¹¶ç›¸åŒåç§°çš„åˆ†ç»„ï¼ˆè§£å†³æ‰¹å¤„ç†å¯¼è‡´çš„é‡å¤åˆ†ç»„ï¼‰
            if len(all_enhanced_groups) > 1:
                log_func(f"ğŸ”„ å¼€å§‹åˆå¹¶ç›¸åŒåç§°çš„åˆ†ç»„: {len(all_enhanced_groups)} ä¸ªåˆ†ç»„")
                merge_start_time = time.time()
                try:
                    deduplicated_groups = merge_duplicate_named_groups(all_enhanced_groups)
                    merge_process_time = time.time() - merge_start_time

                    if len(deduplicated_groups) < len(all_enhanced_groups):
                        log_func(f"âœ… é‡å¤åˆ†ç»„åˆå¹¶å®Œæˆ: {len(all_enhanced_groups)} â†’ {len(deduplicated_groups)} ä¸ªåˆ†ç»„ (è€—æ—¶: {merge_process_time:.1f}ç§’)")
                    else:
                        log_func(f"â­ï¸ æ— é‡å¤åˆ†ç»„: ä¿æŒ {len(all_enhanced_groups)} ä¸ªåˆ†ç»„ (è€—æ—¶: {merge_process_time:.1f}ç§’)")

                    all_enhanced_groups = deduplicated_groups

                except Exception as e:
                    merge_process_time = time.time() - merge_start_time
                    log_func(f"âŒ é‡å¤åˆ†ç»„åˆå¹¶å¤±è´¥: {e} (è€—æ—¶: {merge_process_time:.1f}ç§’)")

            # ğŸ”„ ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½åˆå¹¶åŒç³»åˆ—åˆ†ç»„ï¼ˆå¯é€‰çš„AIåˆå¹¶ï¼‰
            if len(all_enhanced_groups) > 1:
                log_func(f"ğŸ”„ å¼€å§‹æ™ºèƒ½åˆå¹¶: {len(all_enhanced_groups)} ä¸ªåˆ†ç»„")
                merge_start_time = time.time()
                try:
                    merged_groups = merge_same_series_groups(all_enhanced_groups)
                    merge_process_time = time.time() - merge_start_time

                    if merged_groups and len(merged_groups) < len(all_enhanced_groups):
                        log_func(f"âœ… æ™ºèƒ½åˆå¹¶å®Œæˆ: {len(all_enhanced_groups)} â†’ {len(merged_groups)} ä¸ªåˆ†ç»„ (è€—æ—¶: {merge_process_time:.1f}ç§’)")
                        movie_info = merged_groups
                    else:
                        log_func(f"â­ï¸ æ— éœ€åˆå¹¶: ä¿æŒ {len(all_enhanced_groups)} ä¸ªåˆ†ç»„ (è€—æ—¶: {merge_process_time:.1f}ç§’)")
                        movie_info = all_enhanced_groups

                except Exception as e:
                    merge_process_time = time.time() - merge_start_time
                    log_func(f"âŒ æ™ºèƒ½åˆå¹¶å¤±è´¥: {e} (è€—æ—¶: {merge_process_time:.1f}ç§’)")
                    movie_info = all_enhanced_groups

                # æ›´æ–°è¿›åº¦
                progress = 80.0 + 10.0
                log_func(f"ğŸ”„ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {progress:.3f}% - ğŸ”„ æ™ºèƒ½åˆå¹¶å®Œæˆ")
            else:
                movie_info = all_enhanced_groups
                log_func(f"â­ï¸ è·³è¿‡åˆå¹¶: åªæœ‰ {len(all_enhanced_groups)} ä¸ªåˆ†ç»„")

            log_func(f"ğŸ¯ åˆ†ç»„åˆ†æå®Œæˆ: è¾“å‡º {len(movie_info)} ä¸ªåˆ†ç»„")
        else:
            movie_info = []
    except Exception as e:
        log_func(f"âš ï¸ ç”Ÿæˆåˆ†ç»„ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        movie_info = []

    # æ„å»ºè¿”å›ç»“æœ
    result = {
        'success': True,
        'count': file_count,
        'size': size_str,
        'total_size_bytes': int(total_size_bytes),
        'movie_info': movie_info,
        'video_files': video_files
    }

    # ğŸ”„ ç¼“å­˜ç»“æœï¼ˆä»…åœ¨æœ‰æœ‰æ•ˆåˆ†ç»„æ—¶ç¼“å­˜ï¼‰
    if movie_info and len(movie_info) > 0:
        cache_grouping_result(cache_key, result)
        log_func(f"ğŸ’¾ åˆ†ç»„ç»“æœå·²ç¼“å­˜ï¼Œæœ‰æ•ˆæœŸ: {GROUPING_CACHE_DURATION}ç§’")

    return result

@app.route('/clear_cache', methods=['POST'])
def clear_cache():
    """æ¸…ç†ç¼“å­˜API"""
    try:
        cache_type = request.form.get('cache_type', 'all')
        folder_id = request.form.get('folder_id', None)

        if cache_type == 'folder' or cache_type == 'all':
            if folder_id:
                clear_folder_cache(int(folder_id))
                message = f"å·²æ¸…ç†æ–‡ä»¶å¤¹ {folder_id} çš„ç¼“å­˜"
            else:
                clear_folder_cache()
                message = "å·²æ¸…ç†æ‰€æœ‰ç›®å½•ç¼“å­˜"

        if cache_type == 'grouping' or cache_type == 'all':
            global grouping_cache
            count = len(grouping_cache)
            grouping_cache.clear()
            message += f"ï¼Œå·²æ¸…ç† {count} ä¸ªåˆ†ç»„ç¼“å­˜"

        if cache_type == 'scraping' or cache_type == 'all':
            global scraping_cache
            count = len(scraping_cache)
            scraping_cache.clear()
            message += f"ï¼Œå·²æ¸…ç† {count} ä¸ªåˆ®å‰Šç¼“å­˜"

        logging.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: {message}")
        return jsonify({'success': True, 'message': message})

    except Exception as e:
        logging.error(f"æ¸…ç†ç¼“å­˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/cache_status', methods=['GET'])
def cache_status():
    """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
    try:
        global folder_content_cache, grouping_cache, scraping_cache

        # ç»Ÿè®¡ç›®å½•ç¼“å­˜
        folder_cache_stats = folder_content_cache.stats()
        folder_cache_count = folder_cache_stats['size']
        folder_cache_valid = folder_cache_count  # LRUç¼“å­˜è‡ªåŠ¨ç®¡ç†è¿‡æœŸ

        # ç»Ÿè®¡åˆ†ç»„ç¼“å­˜
        grouping_cache_stats = grouping_cache.stats()
        grouping_cache_count = grouping_cache_stats['size']
        grouping_cache_valid = grouping_cache_count  # LRUç¼“å­˜è‡ªåŠ¨ç®¡ç†è¿‡æœŸ

        # ç»Ÿè®¡åˆ®å‰Šç¼“å­˜
        scraping_cache_stats = scraping_cache.stats()
        scraping_cache_count = scraping_cache_stats['size']
        scraping_cache_valid = scraping_cache_count  # LRUç¼“å­˜è‡ªåŠ¨ç®¡ç†è¿‡æœŸ

        return jsonify({
            'success': True,
            'cache_status': {
                'folder_cache': {
                    'total': folder_cache_count,
                    'valid': folder_cache_valid,
                    'expired': folder_cache_count - folder_cache_valid,
                    'duration': FOLDER_CONTENT_CACHE_DURATION
                },
                'grouping_cache': {
                    'total': grouping_cache_count,
                    'valid': grouping_cache_valid,
                    'expired': grouping_cache_count - grouping_cache_valid,
                    'duration': GROUPING_CACHE_DURATION
                },
                'scraping_cache': {
                    'total': scraping_cache_count,
                    'valid': scraping_cache_valid,
                    'expired': scraping_cache_count - scraping_cache_valid,
                    'duration': SCRAPING_CACHE_DURATION
                }
            }
        })

    except Exception as e:
        logging.error(f"è·å–ç¼“å­˜çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/cancel_task', methods=['POST'])
def cancel_task():
    """å–æ¶ˆå½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    try:
        cancel_current_task()
        return jsonify({'success': True, 'message': 'ä»»åŠ¡å–æ¶ˆè¯·æ±‚å·²å‘é€'})
    except Exception as e:
        logging.error(f"å–æ¶ˆä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/performance_stats', methods=['GET'])
def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            'app_state': app_state.get_stats(),
            'performance': performance_monitor.get_stats(),
            'system_info': {
                'python_version': sys.version,
                'platform': sys.platform,
                'current_time': datetime.datetime.now().isoformat()
            }
        }
        return jsonify({'success': True, 'stats': stats})
    except Exception as e:
        logging.error(f"è·å–æ€§èƒ½ç»Ÿè®¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/reset_performance_stats', methods=['POST'])
def reset_performance_stats():
    """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
    try:
        performance_monitor.reset_stats()
        return jsonify({'success': True, 'message': 'æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®'})
    except Exception as e:
        logging.error(f"é‡ç½®æ€§èƒ½ç»Ÿè®¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/get_file_list', methods=['POST'])
def get_file_list():
    """è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆé€’å½’ï¼‰"""
    try:
        folder_id = request.form.get('folder_id', '0')
        folder_id = int(folder_id)

        logging.info(f"è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨")

        file_list = []
        get_video_files_recursively(folder_id, file_list)

        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        formatted_files = []
        for file_item in file_list:
            formatted_files.append({
                'parentFileId': file_item['parentFileId'],
                'fileId': file_item['fileId'],
                'filename': os.path.basename(file_item['file_path']),  # åªä¿ç•™æ–‡ä»¶å
                'file_name': file_item['file_path'],  # å®Œæ•´è·¯å¾„
                'size': file_item['size_gb'],
            })

        logging.info(f"æ‰¾åˆ° {len(formatted_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        return jsonify({
            'success': True,
            'files': formatted_files,
            'total_count': len(formatted_files)
        })

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/scrape_preview', methods=['POST'])
def scrape_preview():
    """åˆ®å‰Šé¢„è§ˆ"""
    try:
        # å¼€å§‹æ–°ä»»åŠ¡
        start_new_task(f"scrape_preview_{int(time.time())}")

        selected_files_json = request.form.get('files')
        if not selected_files_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©ä»»ä½•é¡¹ç›®è¿›è¡Œåˆ®å‰Šã€‚'})

        selected_items = json.loads(selected_files_json)
        logging.info(f"ğŸ¬ å¼€å§‹åˆ®å‰Šé¢„è§ˆï¼Œé€‰æ‹©è¿›è¡Œåˆ®å‰Šçš„é¡¹ç›®æ•°é‡: {len(selected_items)}")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        check_task_cancelled()

        # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶
        all_files = []

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        logging.info(f"å‰ç«¯ä¼ é€’çš„å®Œæ•´æ•°æ®: {json.dumps(selected_items, indent=2, ensure_ascii=False)}")

        # å‰ç«¯åº”è¯¥å·²ç»ä¼ é€’äº† file_name å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™è®°å½•è­¦å‘Š
        for item in selected_items:
            if not item.get('file_name'):
                logging.warning(f"é¡¹ç›®ç¼ºå°‘ file_name å­—æ®µ: {item.get('fileId')} - {item.get('name')}")

        for i, item in enumerate(selected_items):
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            check_task_cancelled()

            logging.info(f"è°ƒè¯• - å¤„ç†ç¬¬ {i+1} ä¸ªé¡¹ç›®: {item}")
            logging.info(f"è°ƒè¯• - é¡¹ç›®å­—æ®µ: name={item.get('name')}, file_name={item.get('file_name')}")

            if item.get('is_dir'):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œé€’å½’è·å–å…¶ä¸­çš„è§†é¢‘æ–‡ä»¶
                folder_id = item.get('fileId')
                folder_name = item.get('name') or item.get('filename') or item.get('file_name', 'Unknown')
                # è·å–æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„
                folder_path = item.get('file_name', '')
                logging.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶å¤¹: {folder_name} (ID: {folder_id})")
                logging.info(f"ğŸ“‚ æ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")
                folder_files = []
                try:
                    # ä¼ é€’æ­£ç¡®çš„æ–‡ä»¶å¤¹è·¯å¾„
                    get_video_files_recursively(int(folder_id), folder_files, folder_path)
                    logging.info(f"ğŸ“‚ æ–‡ä»¶å¤¹ {folder_name} ä¸­æ‰¾åˆ° {len(folder_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
                    all_files.extend(folder_files)
                except Exception as e:
                    if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
                        logging.info("ğŸ›‘ æ–‡ä»¶å¤¹éå†è¿‡ç¨‹ä¸­ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
                        raise
                    else:
                        logging.error(f"é€’å½’è·å–æ–‡ä»¶å¤¹ {folder_name} ä¸­çš„è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
            else:
                # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥æ·»åŠ 
                logging.info(f"å¤„ç†æ–‡ä»¶: {item}")
                # ä¼˜å…ˆä½¿ç”¨ file_nameï¼ˆå®Œæ•´è·¯å¾„ï¼‰ï¼Œç„¶åä½¿ç”¨ nameï¼ˆæ–‡ä»¶åï¼‰
                filename = item.get('file_name') or item.get('name')
                if filename:
                    _, ext = os.path.splitext(filename)
                    logging.info(f"æ–‡ä»¶å: {filename}, æ‰©å±•å: {ext}, æ˜¯å¦ä¸ºè§†é¢‘: {ext.lower()[1:] in SUPPORTED_MEDIA_TYPES}")
                    if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                        # åˆ›å»ºæ–‡ä»¶é¡¹å¯¹è±¡ï¼Œä¿æŒä¸ recursive_list_video_files ä¸€è‡´çš„æ ¼å¼
                        file_item = {
                            'parentFileId': item.get('parentFileId'),
                            'fileId': item.get('fileId'),
                            'filename': os.path.basename(filename),
                            'file_path': filename,
                            'size_gb': item.get('size', ''),
                            'type': 0  # æ–‡ä»¶ç±»å‹
                        }
                        all_files.append(file_item)
                        logging.info(f"æ·»åŠ æ–‡ä»¶åˆ°å¤„ç†åˆ—è¡¨: {filename}")
                    else:
                        logging.info(f"è·³è¿‡éè§†é¢‘æ–‡ä»¶: {filename}")
                else:
                    logging.warning(f"æ–‡ä»¶ç¼ºå°‘ name å’Œ file_name å­—æ®µ: {item}")

        logging.info(f"æ€»å…±æ”¶é›†åˆ° {len(all_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        logging.info(f"éœ€è¦åˆ®å‰Šçš„æ–‡ä»¶ {all_files}")

        # ğŸ” ä¼˜åŒ–æ–‡ä»¶è¿‡æ»¤é€»è¾‘ï¼Œæå‰è¯†åˆ«ä¸éœ€è¦å¤„ç†çš„æ–‡ä»¶
        files_to_scrape = []
        already_processed = 0

        for file_item in all_files:
            filename = file_item['file_path']
            # æ›´ç²¾ç¡®çš„æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å·²åŒ…å«TMDBä¿¡æ¯
            has_tmdb = 'tmdb-' in filename.lower()
            has_size_info = any(size_marker in filename for size_marker in ['GB', 'MB', 'TB'])

            # å¦‚æœæ–‡ä»¶åå·²ç»åŒ…å«TMDBä¿¡æ¯å’Œå¤§å°ä¿¡æ¯ï¼Œè·³è¿‡å¤„ç†
            if has_tmdb and has_size_info:
                already_processed += 1
                logging.debug(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {filename}")
            else:
                files_to_scrape.append(file_item)

        if already_processed > 0:
            logging.info(f"â­ï¸ è·³è¿‡ {already_processed} ä¸ªå·²å¤„ç†çš„æ–‡ä»¶")

        if not files_to_scrape:
            logging.info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†è¿‡ï¼Œæ— éœ€åˆ®å‰Š")
            return jsonify({'success': True, 'results': [], 'message': 'æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†è¿‡'})

        logging.info(f"ğŸ¯ éœ€è¦åˆ®å‰Šçš„æ–‡ä»¶æ•°é‡: {len(files_to_scrape)} (æ€»æ–‡ä»¶: {len(all_files)}, å·²å¤„ç†: {already_processed})")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        check_task_cancelled()

        # ğŸš€ ä¼˜åŒ–åˆ†å—å¤„ç†ç­–ç•¥
        chunks = [files_to_scrape[i:i + CHUNK_SIZE] for i in range(0, len(files_to_scrape), CHUNK_SIZE)]
        all_scraped_results = []

        logging.info(f"ğŸ“¦ åˆ†ä¸º {len(chunks)} ä¸ªæ‰¹æ¬¡å¤„ç† (æ¯æ‰¹æ¬¡ {CHUNK_SIZE} ä¸ªæ–‡ä»¶)")
        logging.info(f"ğŸ”§ æ€§èƒ½é…ç½®: QPS_LIMIT={QPS_LIMIT}, MAX_WORKERS={MAX_WORKERS}")

        # è®°å½•å¼€å§‹æ—¶é—´ç”¨äºæ€§èƒ½åˆ†æ
        start_time = time.time()
        completed_batches = 0

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            future_info = {}  # å­˜å‚¨ future å¯¹è±¡çš„é¢å¤–ä¿¡æ¯

            for i, chunk in enumerate(chunks):

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                check_task_cancelled()

                logging.info(f"ğŸš€ æäº¤ç¬¬ {i+1}/{len(chunks)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç† (åŒ…å« {len(chunk)} ä¸ªæ–‡ä»¶)")
                future = executor.submit(extract_movie_name_and_info, chunk)
                futures.append(future)
                future_info[future] = {'batch_num': i+1, 'batch_size': len(chunk)}

            for future in as_completed(futures):
                batch_num = future_info[future]['batch_num']
                batch_size = future_info[future]['batch_size']
                try:
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                    check_task_cancelled()

                    batch_start_time = time.time()
                    results = future.result()
                    batch_end_time = time.time()
                    batch_duration = batch_end_time - batch_start_time

                    completed_batches += 1

                    if results:
                        all_scraped_results.extend(results)
                        logging.info(f"âœ… å®Œæˆç¬¬ {batch_num} ä¸ªæ‰¹æ¬¡ ({batch_size} ä¸ªæ–‡ä»¶)ï¼Œè·å¾— {len(results)} ä¸ªç»“æœï¼Œè€—æ—¶ {batch_duration:.2f}ç§’")
                    else:
                        logging.info(f"âš ï¸ ç¬¬ {batch_num} ä¸ªæ‰¹æ¬¡æœªè·å¾—ç»“æœï¼Œè€—æ—¶ {batch_duration:.2f}ç§’")

                    # è®¡ç®—æ•´ä½“è¿›åº¦
                    progress = (completed_batches / len(chunks)) * 100
                    elapsed_time = time.time() - start_time
                    avg_time_per_batch = elapsed_time / completed_batches
                    estimated_remaining = (len(chunks) - completed_batches) * avg_time_per_batch

                    logging.info(f"ğŸ“Š è¿›åº¦: {completed_batches}/{len(chunks)} ({progress:.1f}%), é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining:.1f}ç§’")

                except Exception as exc:
                    if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(exc):
                        logging.info("ğŸ›‘ åˆ®å‰Šä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
                        raise
                    else:
                        logging.error(f'ç¬¬ {batch_num} ä¸ªæ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {exc}', exc_info=True)

        logging.info(f"ğŸ‰ åˆ®å‰Šé¢„è§ˆå®Œæˆã€‚æ€»ç»“æœ: {len(all_scraped_results)}")
        return jsonify({'success': True, 'results': all_scraped_results})

    except Exception as e:
        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
            logging.info("ğŸ›‘ åˆ®å‰Šé¢„è§ˆä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
            return jsonify({'success': False, 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True})
        else:
            logging.error(f"åˆ®å‰Šé¢„è§ˆæœŸé—´å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return jsonify({'success': False, 'error': str(e)})

@app.route('/apply_rename', methods=['POST'])
def apply_rename():
    """åº”ç”¨é‡å‘½å - æ”¯æŒæ‰¹é‡é‡å‘½åï¼Œå‚è€ƒmovie115å®ç°"""
    try:
        rename_data_json = request.form.get('rename_data')
        if not rename_data_json:
            logging.warning("æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®ã€‚")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®ã€‚'})

        rename_data = json.loads(rename_data_json)
        logging.info(f"ğŸ“‹ rename_data: {rename_data}")
        if not rename_data:
            logging.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚'})

        # æ„å»ºé‡å‘½åå­—å…¸å’ŒåŸå§‹åç§°æ˜ å°„
        namedict = {}
        original_names_map = {}  # ç”¨äºå­˜å‚¨åŸå§‹åç§°ï¼Œä»¥ä¾¿åœ¨ç»“æœä¸­æ˜¾ç¤º
        for item in rename_data:
            item_id = item.get('fileId') or item.get('id')
            item_type = item.get('type', 'file')
            new_name = item.get('suggested_name') or item.get('new_name')
            original_name = item.get('filename') or item.get('original_name') or item.get('name')

            if item_type == 'file' and item_id and new_name:
                namedict[item_id] = new_name
                original_names_map[item_id] = original_name
                logging.info(f"ğŸ“ å‡†å¤‡é‡å‘½åæ–‡ä»¶ ID: {item_id}, åŸå§‹åç§°: {original_name}, æ–°åç§°: {new_name}")

        if not namedict:
            logging.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚'})

        # åœ¨æ‰§è¡Œé‡å‘½åä¹‹å‰ï¼Œå°†é‡å‘½åæ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿æ¢å¤
        backup_data = {
            'namedict': namedict,
            'original_names_map': original_names_map
        }
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file_path = f'rename_data_backup_{timestamp}.json'
        try:
            with open(backup_file_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=4)
            logging.info(f"ğŸ’¾ é‡å‘½åæ•°æ®å·²æˆåŠŸå¤‡ä»½åˆ° {backup_file_path}")
        except Exception as e:
            logging.error(f"å¤‡ä»½é‡å‘½åæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å°è¯•é‡å‘½å

        # å°† namedict æ‹†åˆ†æˆå¤šç»„ï¼Œæ¯ç»„ 200 ä¸ªï¼ˆå‚è€ƒmovie115çš„æ‰¹æ¬¡å¤§å°ï¼‰
        chunk_size = 30
        namedict_chunks = [dict(list(namedict.items())[i:i + chunk_size]) for i in range(0, len(namedict), chunk_size)]

        all_results = []
        overall_success = True
        overall_error = []

        for i, chunk in enumerate(namedict_chunks):
            logging.info(f"ğŸ”„ å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(namedict_chunks)} æ‰¹é‡å‘½åï¼ŒåŒ…å« {len(chunk)} ä¸ªæ–‡ä»¶ã€‚")

            try:
                # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„é‡å‘½åï¼Œä½¿ç”¨æ‰¹é‡QPSé™åˆ¶ï¼ˆ1 QPSï¼‰
                rename_result = rename(chunk, use_batch_qps=True)

                # å¤„ç†APIè¿”å›ç»“æœï¼šå½“APIè¿”å›code=0æ—¶ï¼Œdataå¯èƒ½ä¸ºnullï¼Œè¿™è¡¨ç¤ºæˆåŠŸ
                # renameå‡½æ•°è¿”å›validate_api_responseçš„ç»“æœï¼ŒæˆåŠŸæ—¶å¯èƒ½è¿”å›None
                if rename_result is None:
                    # APIè¿”å›æˆåŠŸï¼ˆcode=0, data=nullï¼‰
                    logging.info(f"âœ… ç¬¬ {i+1} æ‰¹é‡å‘½åæˆåŠŸï¼ŒAPIè¿”å›æˆåŠŸçŠ¶æ€")
                    for item_id, new_name in chunk.items():
                        all_results.append({
                            'id': item_id,
                            'type': 'file',
                            'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                            'new_name': new_name,
                            'status': 'success'
                        })
                elif isinstance(rename_result, dict) and rename_result.get('success', True):
                    # å¦‚æœè¿”å›å­—å…¸ä¸”successä¸ºTrue
                    logging.info(f"âœ… ç¬¬ {i+1} æ‰¹é‡å‘½åæˆåŠŸï¼Œç»“æœ: {rename_result}")
                    for item_id, new_name in chunk.items():
                        all_results.append({
                            'id': item_id,
                            'type': 'file',
                            'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                            'new_name': new_name,
                            'status': 'success'
                        })
                else:
                    # å¤„ç†å¤±è´¥æƒ…å†µ
                    overall_success = False
                    if isinstance(rename_result, dict):
                        error_message = rename_result.get('error', 'æ‰¹é‡é‡å‘½åæœªçŸ¥é”™è¯¯')
                        details = rename_result.get('result')
                    else:
                        error_message = f'é‡å‘½åè¿”å›å¼‚å¸¸ç»“æœ: {rename_result}'
                        details = None

                    logging.error(f"âŒ ç¬¬ {i+1} æ‰¹æ‰¹é‡é‡å‘½åå¤±è´¥: {error_message}, è¯¦æƒ…: {details}")
                    overall_error.append(f"ç¬¬ {i+1} æ‰¹å¤±è´¥: {error_message} (è¯¦æƒ…: {details})")
                    # å¯¹äºå¤±è´¥çš„æ‰¹æ¬¡ï¼Œå°†è¯¥æ‰¹æ¬¡ä¸­çš„æ–‡ä»¶æ ‡è®°ä¸ºå¤±è´¥
                    for item_id, new_name in chunk.items():
                        all_results.append({
                            'id': item_id,
                            'type': 'file',
                            'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                            'new_name': new_name,
                            'status': 'failed',
                            'error': error_message
                        })

            except AccessTokenError as e:
                overall_success = False
                error_message = f'Access Tokené”™è¯¯: {str(e)}'
                logging.error(f"âŒ ç¬¬ {i+1} æ‰¹ Access Tokené”™è¯¯: {e}")
                overall_error.append(f"ç¬¬ {i+1} æ‰¹å¤±è´¥: {error_message}")
                # è®°å½•å¤±è´¥çš„é‡å‘½å
                for item_id, new_name in chunk.items():
                    all_results.append({
                        'id': item_id,
                        'type': 'file',
                        'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                        'new_name': new_name,
                        'status': 'failed',
                        'error': error_message
                    })

            except requests.HTTPError as e:
                overall_success = False
                error_message = f'HTTPé”™è¯¯: {str(e)}'
                logging.error(f"âŒ ç¬¬ {i+1} æ‰¹ HTTPé”™è¯¯: {e}")
                overall_error.append(f"ç¬¬ {i+1} æ‰¹å¤±è´¥: {error_message}")
                # è®°å½•å¤±è´¥çš„é‡å‘½å
                for item_id, new_name in chunk.items():
                    all_results.append({
                        'id': item_id,
                        'type': 'file',
                        'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                        'new_name': new_name,
                        'status': 'failed',
                        'error': error_message
                    })

            except Exception as e:
                overall_success = False
                error_message = str(e)
                logging.error(f"âŒ ç¬¬ {i+1} æ‰¹é‡å‘½åå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
                overall_error.append(f"ç¬¬ {i+1} æ‰¹å¤±è´¥: {error_message}")
                # è®°å½•å¤±è´¥çš„é‡å‘½å
                for item_id, new_name in chunk.items():
                    all_results.append({
                        'id': item_id,
                        'type': 'file',
                        'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                        'new_name': new_name,
                        'status': 'failed',
                        'error': error_message
                    })

        # é‡å‘½åæ“ä½œå®Œæˆåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
        if overall_success or any(result.get('status') == 'success' for result in all_results):
            # å¦‚æœæœ‰ä»»ä½•æ–‡ä»¶é‡å‘½åæˆåŠŸï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
            logging.info("ğŸ§¹ é‡å‘½åæ“ä½œå®Œæˆï¼Œå¼€å§‹æ¸…ç†ç›¸å…³ç¼“å­˜...")

            # æ¸…ç†åˆ®å‰Šç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œæ—§çš„åˆ®å‰Šç»“æœä¸å†æœ‰æ•ˆï¼‰
            old_scraping_size = scraping_cache.size()
            scraping_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ®å‰Šç¼“å­˜: {old_scraping_size} é¡¹")

            # æ¸…ç†åˆ†ç»„ç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œåˆ†ç»„ç»“æœå¯èƒ½ä¸å†å‡†ç¡®ï¼‰
            old_grouping_size = grouping_cache.size()
            grouping_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ†ç»„ç¼“å­˜: {old_grouping_size} é¡¹")

            # æ¸…ç†æ–‡ä»¶å¤¹å†…å®¹ç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œéœ€è¦åˆ·æ–°æ–‡ä»¶åˆ—è¡¨ï¼‰
            old_folder_size = folder_content_cache.size()
            folder_content_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶å¤¹å†…å®¹ç¼“å­˜: {old_folder_size} é¡¹")

            # æ¸…ç†è·¯å¾„ç¼“å­˜ï¼ˆå¦‚æœé‡å‘½åçš„æ˜¯æ–‡ä»¶å¤¹ï¼Œè·¯å¾„ä¿¡æ¯éœ€è¦æ›´æ–°ï¼‰
            # è¿™é‡Œä¿å®ˆä¸€äº›ï¼Œåªåœ¨ç¼“å­˜è¿‡å¤šæ—¶æ¸…ç†ï¼Œé¿å…å½±å“æ€§èƒ½
            if folder_path_cache.size() > 200:
                old_path_size = folder_path_cache.size()
                folder_path_cache.clear()
                logging.info(f"ğŸ§¹ æ¸…ç†è·¯å¾„ç¼“å­˜: {old_path_size} é¡¹")

            total_cleared = old_scraping_size + old_grouping_size + old_folder_size
            logging.info(f"ğŸ§¹ é‡å‘½ååç¼“å­˜æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {total_cleared} é¡¹ç¼“å­˜")

        # å‚è€ƒmovie115çš„è¿”å›æ ¼å¼
        if overall_success:
            logging.info(f"ğŸ‰ æ‰¹é‡é‡å‘½åå®Œæˆ: å…¨éƒ¨æˆåŠŸï¼Œå…±å¤„ç† {len(all_results)} ä¸ªæ–‡ä»¶")
            return jsonify({'success': True, 'results': all_results})
        else:
            logging.info(f"âš ï¸ æ‰¹é‡é‡å‘½åå®Œæˆ: éƒ¨åˆ†æˆ–å…¨éƒ¨å¤±è´¥ï¼Œè¯¦æƒ…: {overall_error}")
            return jsonify({
                'success': False,
                'error': 'éƒ¨åˆ†æˆ–å…¨éƒ¨æ‰¹é‡é‡å‘½åå¤±è´¥ã€‚',
                'details': overall_error,
                'results': all_results
            })

    except Exception as e:
        logging.error(f"åº”ç”¨é‡å‘½åæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})


@app.route('/move_files_direct', methods=['POST'])
def move_files_direct():
    """ç›´æ¥ç§»åŠ¨é€‰ä¸­çš„æ–‡ä»¶åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
    try:
        move_data_json = request.form.get('move_data')
        target_folder_id = request.form.get('target_folder_id')

        if not move_data_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›ç§»åŠ¨æ•°æ®ã€‚'})

        if not target_folder_id:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›ç›®æ ‡æ–‡ä»¶å¤¹IDã€‚'})

        try:
            target_folder_id = int(target_folder_id)
        except ValueError:
            return jsonify({'success': False, 'error': 'ç›®æ ‡æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—ã€‚'})

        move_data = json.loads(move_data_json)
        if not move_data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦ç§»åŠ¨ã€‚'})

        # æå–æ–‡ä»¶IDåˆ—è¡¨
        file_ids = []
        for item in move_data:
            file_id = item.get('fileId')
            if file_id:
                try:
                    file_ids.append(int(file_id))
                except ValueError:
                    logging.warning(f"æ— æ•ˆçš„æ–‡ä»¶ID: {file_id}")
                    continue

        if not file_ids:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶IDã€‚'})

        logging.info(f"ğŸ“¦ å‡†å¤‡ç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
        logging.info(f"ğŸ“‹ æ–‡ä»¶IDåˆ—è¡¨: {file_ids}")

        # è°ƒç”¨ç§»åŠ¨API
        result = move(file_ids, target_folder_id)

        if result.get("success"):
            logging.info(f"âœ… æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ã€‚',
                'moved_count': len(file_ids),
                'target_folder_id': target_folder_id
            })
        else:
            error_message = result.get("message", "ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹IDæ˜¯å¦æ­£ç¡®ã€‚")
            logging.error(f"ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {error_message}")
            return jsonify({'success': False, 'error': error_message})

    except json.JSONDecodeError as e:
        logging.error(f"è§£æç§»åŠ¨æ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': 'ç§»åŠ¨æ•°æ®æ ¼å¼é”™è¯¯ã€‚'})
    except Exception as e:
        logging.error(f"ç›´æ¥ç§»åŠ¨æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/rename_files', methods=['POST'])
def rename_files():
    """é‡å‘½åé€‰ä¸­çš„æ–‡ä»¶"""
    try:
        rename_data_json = request.form.get('rename_data')
        if not rename_data_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®ã€‚'})

        rename_data = json.loads(rename_data_json)
        if not rename_data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚'})

        logging.info(f"å‡†å¤‡é‡å‘½å {len(rename_data)} ä¸ªæ–‡ä»¶")

        # æ„å»ºé‡å‘½åå­—å…¸
        rename_dict = {}
        for item in rename_data:
            file_id = item.get('fileId')
            # æ”¯æŒå¤šç§æ ¼å¼ï¼šnewNameï¼ˆæ™®é€šé‡å‘½åï¼‰ã€suggested_nameï¼ˆåˆ®å‰Šé¢„è§ˆé‡å‘½åï¼‰ã€new_nameï¼ˆæ™ºèƒ½é‡å‘½åï¼‰
            new_name = item.get('newName') or item.get('suggested_name') or item.get('new_name')
            if file_id and new_name:
                rename_dict[file_id] = new_name

        if not rename_dict:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„é‡å‘½åæ•°æ®ã€‚'})

        # åˆ†æ‰¹å¤„ç†é‡å‘½åï¼Œæ¯æ‰¹æœ€å¤š30ä¸ªæ–‡ä»¶
        BATCH_SIZE = 30
        file_items = list(rename_dict.items())
        total_files = len(file_items)
        batches = [file_items[i:i + BATCH_SIZE] for i in range(0, total_files, BATCH_SIZE)]

        logging.info(f"æ€»å…± {total_files} ä¸ªæ–‡ä»¶ï¼Œåˆ†ä¸º {len(batches)} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªæ–‡ä»¶")

        successful_renames = []
        failed_renames = []

        for batch_index, batch in enumerate(batches):
            batch_dict = dict(batch)
            batch_size = len(batch_dict)

            logging.info(f"å¤„ç†ç¬¬ {batch_index + 1}/{len(batches)} æ‰¹ï¼ŒåŒ…å« {batch_size} ä¸ªæ–‡ä»¶")

            try:
                # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„é‡å‘½åï¼Œä½¿ç”¨æ‰¹é‡QPSé™åˆ¶ï¼ˆ1 QPSï¼‰
                result = rename(batch_dict, use_batch_qps=True)
                logging.info(f"ç¬¬ {batch_index + 1} æ‰¹é‡å‘½åæˆåŠŸï¼Œç»“æœ: {result}")

                # è®°å½•æˆåŠŸçš„é‡å‘½å
                for file_id, new_name in batch_dict.items():
                    successful_renames.append({
                        'fileId': file_id,
                        'newName': new_name,
                        'status': 'success'
                    })

            except Exception as e:
                logging.error(f"ç¬¬ {batch_index + 1} æ‰¹é‡å‘½åå‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                # è®°å½•å¤±è´¥çš„é‡å‘½å
                for file_id, new_name in batch_dict.items():
                    failed_renames.append({
                        'fileId': file_id,
                        'newName': new_name,
                        'status': 'failed',
                        'error': str(e)
                    })

        # æ±‡æ€»ç»“æœ
        total_successful = len(successful_renames)
        total_failed = len(failed_renames)

        logging.info(f"æ‰¹é‡é‡å‘½åå®Œæˆ: æˆåŠŸ {total_successful} ä¸ªï¼Œå¤±è´¥ {total_failed} ä¸ª")

        # æ„å»ºè¯¦ç»†ç»“æœæ•°ç»„ï¼ŒåŒ…å«åŸå§‹æ–‡ä»¶åä¿¡æ¯
        all_results = []

        # æ·»åŠ æˆåŠŸçš„ç»“æœ
        for success_item in successful_renames:
            # ä»åŸå§‹é‡å‘½åæ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡ä»¶å
            original_name = "æœªçŸ¥"
            for orig_item in rename_data:
                if str(orig_item.get('fileId')) == str(success_item['fileId']):
                    original_name = orig_item.get('original_name', orig_item.get('newName', 'æœªçŸ¥'))
                    break

            all_results.append({
                'status': 'success',
                'original_name': original_name,
                'new_name': success_item['newName'],
                'error': ''
            })

        # æ·»åŠ å¤±è´¥çš„ç»“æœ
        for failed_item in failed_renames:
            # ä»åŸå§‹é‡å‘½åæ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡ä»¶å
            original_name = "æœªçŸ¥"
            for orig_item in rename_data:
                if str(orig_item.get('fileId')) == str(failed_item['fileId']):
                    original_name = orig_item.get('original_name', orig_item.get('newName', 'æœªçŸ¥'))
                    break

            all_results.append({
                'status': 'failed',
                'original_name': original_name,
                'new_name': failed_item['newName'],
                'error': failed_item.get('error', 'æœªçŸ¥é”™è¯¯')
            })

        # å¦‚æœæœ‰ä»»ä½•æ–‡ä»¶é‡å‘½åæˆåŠŸï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
        if total_successful > 0:
            logging.info("ğŸ§¹ é‡å‘½åæ“ä½œå®Œæˆï¼Œå¼€å§‹æ¸…ç†ç›¸å…³ç¼“å­˜...")

            # æ¸…ç†åˆ®å‰Šç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œæ—§çš„åˆ®å‰Šç»“æœä¸å†æœ‰æ•ˆï¼‰
            old_scraping_size = scraping_cache.size()
            scraping_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ®å‰Šç¼“å­˜: {old_scraping_size} é¡¹")

            # æ¸…ç†åˆ†ç»„ç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œåˆ†ç»„ç»“æœå¯èƒ½ä¸å†å‡†ç¡®ï¼‰
            old_grouping_size = grouping_cache.size()
            grouping_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†åˆ†ç»„ç¼“å­˜: {old_grouping_size} é¡¹")

            # æ¸…ç†æ–‡ä»¶å¤¹å†…å®¹ç¼“å­˜ï¼ˆå› ä¸ºæ–‡ä»¶åå·²æ”¹å˜ï¼Œéœ€è¦åˆ·æ–°æ–‡ä»¶åˆ—è¡¨ï¼‰
            old_folder_size = folder_content_cache.size()
            folder_content_cache.clear()
            logging.info(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶å¤¹å†…å®¹ç¼“å­˜: {old_folder_size} é¡¹")

            total_cleared = old_scraping_size + old_grouping_size + old_folder_size
            logging.info(f"ğŸ§¹ é‡å‘½ååç¼“å­˜æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {total_cleared} é¡¹ç¼“å­˜")

        if total_failed == 0:
            return jsonify({
                'success': True,
                'message': f'æ‰¹é‡é‡å‘½åæˆåŠŸï¼Œå…±å¤„ç† {total_successful} ä¸ªæ–‡ä»¶',
                'successful_count': total_successful,
                'failed_count': total_failed,
                'results': all_results
            })
        elif total_successful == 0:
            return jsonify({
                'success': False,
                'error': f'æ‰¹é‡é‡å‘½åå…¨éƒ¨å¤±è´¥ï¼Œå…± {total_failed} ä¸ªæ–‡ä»¶å¤±è´¥',
                'successful_count': total_successful,
                'failed_count': total_failed,
                'results': all_results
            })
        else:
            return jsonify({
                'success': True,
                'message': f'æ‰¹é‡é‡å‘½åéƒ¨åˆ†æˆåŠŸ: æˆåŠŸ {total_successful} ä¸ªï¼Œå¤±è´¥ {total_failed} ä¸ª',
                'successful_count': total_successful,
                'failed_count': total_failed,
                'results': all_results
            })

    except json.JSONDecodeError as e:
        logging.error(f"è§£æé‡å‘½åæ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': 'é‡å‘½åæ•°æ®æ ¼å¼é”™è¯¯ã€‚'})
    except Exception as e:
        logging.error(f"é‡å‘½åæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/delete_files', methods=['POST'])
def delete_files():
    """åˆ é™¤é€‰ä¸­çš„æ–‡ä»¶"""
    try:
        delete_data_json = request.form.get('delete_data')
        if not delete_data_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›åˆ é™¤æ•°æ®ã€‚'})

        delete_data = json.loads(delete_data_json)
        if not delete_data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦åˆ é™¤ã€‚'})

        logging.info(f"å‡†å¤‡åˆ é™¤ {len(delete_data)} ä¸ªæ–‡ä»¶")

        # æ”¶é›†æ–‡ä»¶ID
        file_ids = []
        for item in delete_data:
            file_id = item.get('fileId')
            if file_id:
                file_ids.append(file_id)

        if not file_ids:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶IDã€‚'})

        # åˆ†æ‰¹å¤„ç†åˆ é™¤ï¼Œæ¯æ‰¹æœ€å¤š100ä¸ªæ–‡ä»¶
        BATCH_SIZE = 100
        total_files = len(file_ids)
        batches = [file_ids[i:i + BATCH_SIZE] for i in range(0, total_files, BATCH_SIZE)]

        logging.info(f"æ€»å…± {total_files} ä¸ªæ–‡ä»¶ï¼Œåˆ†ä¸º {len(batches)} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š {BATCH_SIZE} ä¸ªæ–‡ä»¶")

        successful_deletes = 0
        failed_deletes = 0

        for batch_index, batch in enumerate(batches):
            batch_size = len(batch)
            logging.info(f"å¤„ç†ç¬¬ {batch_index + 1}/{len(batches)} æ‰¹ï¼ŒåŒ…å« {batch_size} ä¸ªæ–‡ä»¶")

            try:
                # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„åˆ é™¤
                result = delete(batch)
                logging.info(f"ç¬¬ {batch_index + 1} æ‰¹åˆ é™¤æˆåŠŸï¼Œç»“æœ: {result}")
                successful_deletes += batch_size

            except Exception as e:
                logging.error(f"ç¬¬ {batch_index + 1} æ‰¹åˆ é™¤å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                failed_deletes += batch_size

        logging.info(f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ {successful_deletes} ä¸ªï¼Œå¤±è´¥ {failed_deletes} ä¸ª")

        if failed_deletes == 0:
            return jsonify({
                'success': True,
                'message': f'æ‰¹é‡åˆ é™¤æˆåŠŸï¼Œå…±åˆ é™¤ {successful_deletes} ä¸ªæ–‡ä»¶',
                'successful_count': successful_deletes,
                'failed_count': failed_deletes
            })
        elif successful_deletes == 0:
            return jsonify({
                'success': False,
                'error': f'æ‰¹é‡åˆ é™¤å…¨éƒ¨å¤±è´¥ï¼Œå…± {failed_deletes} ä¸ªæ–‡ä»¶å¤±è´¥',
                'successful_count': successful_deletes,
                'failed_count': failed_deletes
            })
        else:
            return jsonify({
                'success': True,
                'message': f'æ‰¹é‡åˆ é™¤éƒ¨åˆ†æˆåŠŸ: æˆåŠŸ {successful_deletes} ä¸ªï¼Œå¤±è´¥ {failed_deletes} ä¸ª',
                'successful_count': successful_deletes,
                'failed_count': failed_deletes
            })

    except json.JSONDecodeError as e:
        logging.error(f"è§£æåˆ é™¤æ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': 'åˆ é™¤æ•°æ®æ ¼å¼é”™è¯¯ã€‚'})
    except Exception as e:
        logging.error(f"åˆ é™¤æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

# ================================
# æ™ºèƒ½æ–‡ä»¶å¤¹å‘½åç›¸å…³å‡½æ•°
# ================================

def validate_folder_id_for_naming(folder_id_str):
    """éªŒè¯æ–‡ä»¶å¤¹IDçš„æœ‰æ•ˆæ€§"""
    if not folder_id_str or folder_id_str == 'null' or folder_id_str == 'undefined':
        return None, 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'

    try:
        folder_id = int(folder_id_str)
        return folder_id, None
    except (ValueError, TypeError):
        return None, 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'


def get_sampled_video_files(folder_id, max_files=50):
    """è·å–æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘æ–‡ä»¶ï¼Œå¦‚æœæ•°é‡è¿‡å¤šåˆ™è¿›è¡Œé‡‡æ ·"""
    video_files = []

    try:
        # ä¸ºæ™ºèƒ½é‡å‘½ååŠŸèƒ½ä½¿ç”¨ä¼˜åŒ–çš„æ‰«æç­–ç•¥
        get_video_files_for_naming(folder_id, video_files)
    except Exception as e:
        if "ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ" in str(e):
            raise TaskCancelledException("æ–‡ä»¶éå†è¿‡ç¨‹ä¸­ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
        else:
            raise FileSystemError(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

    if not video_files:
        raise ValidationError('æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶')

    # å¦‚æœæ–‡ä»¶æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œéšæœºå–æ ·
    if len(video_files) > max_files:
        import random
        sampled_video_files = random.sample(video_files, max_files)
        logging.info(f"ğŸ“Š æ–‡ä»¶æ•°é‡ {len(video_files)} è¶…è¿‡{max_files}ä¸ªï¼Œéšæœºå–æ · {max_files} ä¸ªæ–‡ä»¶è¿›è¡ŒAIåˆ†æ")
    else:
        sampled_video_files = video_files
        logging.info(f"ğŸ“Š æ–‡ä»¶æ•°é‡ {len(video_files)} åœ¨é™åˆ¶å†…ï¼Œä½¿ç”¨å…¨éƒ¨æ–‡ä»¶è¿›è¡ŒAIåˆ†æ")

    return video_files, sampled_video_files


def get_folder_naming_prompt():
    """è·å–æ–‡ä»¶å¤¹å‘½åçš„AIæç¤ºè¯"""
    return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“æ–‡ä»¶å¤¹å‘½ååŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹è§†é¢‘æ–‡ä»¶åˆ—è¡¨ï¼Œä¸ºåŒ…å«è¿™äº›æ–‡ä»¶çš„æ–‡ä»¶å¤¹å»ºè®®ä¸€ä¸ªåˆé€‚ä¸”ä¸€è‡´çš„åç§°ã€‚

**ğŸš¨ é‡è¦æé†’ï¼š**
- æ–‡ä»¶å¤¹åç§°æ€»é•¿åº¦å¿…é¡»ä¸¥æ ¼æ§åˆ¶åœ¨30ä¸ªå­—ç¬¦ä»¥å†…
- å¯¹äºæ··åˆå†…å®¹ï¼Œç»å¯¹ä¸èƒ½åˆ—å‡ºæ‰€æœ‰ä½œå“åç§°ï¼Œå¿…é¡»ä½¿ç”¨ç®€æ´çš„æè¿°æ€§åç§°
- åªè¿”å›ä¸€ä¸ªæœ€ä½³å»ºè®®ï¼Œä¸è¦æä¾›å¤šä¸ªé€‰é¡¹

**æ ¸å¿ƒå‘½åè§„åˆ™ï¼š**

**ç”µå½±å‘½åè§„åˆ™ï¼š**
1. å•éƒ¨ç”µå½±ï¼š`ç”µå½±å (å¹´ä»½)`
   - ç¤ºä¾‹ï¼š`å¤ä»‡è€…è”ç›Ÿ (2012)` (11å­—ç¬¦)
   - ç¤ºä¾‹ï¼š`è‚–ç”³å…‹çš„æ•‘èµ (1994)` (13å­—ç¬¦)

2. ç”µå½±ç³»åˆ—/åˆé›†ï¼š
   - 2-3éƒ¨ä½œå“ï¼š`ç”µå½±åç³»åˆ— (å¹´ä»½èŒƒå›´)`
   - ç¤ºä¾‹ï¼š`å¤ä»‡è€…è”ç›Ÿç³»åˆ— (2012-2019)` (17å­—ç¬¦)
   - 4éƒ¨ä»¥ä¸Šï¼š`ç”µå½±å ç³»åˆ—åˆé›†`
   - ç¤ºä¾‹ï¼š`é€Ÿåº¦ä¸æ¿€æƒ… ç³»åˆ—åˆé›†` (10å­—ç¬¦)

**ç”µè§†å‰§å‘½åè§„åˆ™ï¼š**
1. å•å­£ç”µè§†å‰§ï¼š`å‰§å (å¹´ä»½) S01`
   - ç¤ºä¾‹ï¼š`æƒåŠ›çš„æ¸¸æˆ (2011) S01` (14å­—ç¬¦)
   - ç¤ºä¾‹ï¼š`è€å‹è®° (1994) S01` (12å­—ç¬¦)

2. å¤šå­£ç”µè§†å‰§ï¼š
   - 2-4å­£ï¼š`å‰§å (å¹´ä»½) S01-S04`
   - ç¤ºä¾‹ï¼š`ç»å‘½æ¯’å¸ˆ (2008) S01-S05` (16å­—ç¬¦)
   - 5å­£ä»¥ä¸Šï¼š`å‰§å å®Œæ•´ç³»åˆ—`
   - ç¤ºä¾‹ï¼š`è€å‹è®° å®Œæ•´ç³»åˆ—` (8å­—ç¬¦)

**æ··åˆå†…å®¹å‘½åè§„åˆ™ï¼ˆé‡è¦ï¼‰ï¼š**
å½“æ–‡ä»¶å¤¹åŒ…å«å¤šä¸ªä¸åŒIPçš„å†…å®¹æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç®€æ´çš„æè¿°æ€§åç§°ï¼š

1. **å¤šä¸ªç”µè§†å‰§ç³»åˆ—**ï¼š
   - `ç”µè§†å‰§åˆé›†` (5å­—ç¬¦)
   - `å‰§é›†æ”¶è—` (4å­—ç¬¦)
   - `ç»å…¸å‰§é›†` (4å­—ç¬¦)

2. **å¤šä¸ªç”µå½±**ï¼š
   - `ç”µå½±åˆé›†` (4å­—ç¬¦)
   - `å½±ç‰‡æ”¶è—` (4å­—ç¬¦)
   - `ç»å…¸ç”µå½±` (4å­—ç¬¦)

3. **æŒ‰å¹´ä»£åˆ†ç±»**ï¼š
   - `2020å¹´ä»£å‰§é›†` (7å­—ç¬¦)
   - `ç»å…¸è€ç‰‡` (4å­—ç¬¦)

4. **æŒ‰ç±»å‹åˆ†ç±»**ï¼š
   - `åŠ¨ä½œç‰‡åˆé›†` (5å­—ç¬¦)
   - `ç§‘å¹»å‰§é›†` (4å­—ç¬¦)

**âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆç»å¯¹ä¸è¦è¿™æ ·åšï¼‰ï¼š**
- `å‰§å1 (å¹´ä»½) S01, å‰§å2 (å¹´ä»½) S01, å‰§å3 (å¹´ä»½) S01` (å¤ªé•¿ï¼Œè¶…è¿‡30å­—ç¬¦)
- `ç”µå½±1 (å¹´ä»½), ç”µå½±2 (å¹´ä»½), ç”µå½±3 (å¹´ä»½)` (å¤ªé•¿ï¼Œåˆ—å‡ºæ‰€æœ‰åç§°)

**âœ… æ­£ç¡®ç¤ºä¾‹ï¼š**
- `ç”µè§†å‰§åˆé›†` (ç®€æ´æè¿°æ€§åç§°)
- `ç»å…¸å‰§é›†` (æŒ‰ç‰¹å¾åˆ†ç±»)
- `2020å¹´ä»£å‰§é›†` (æŒ‰å¹´ä»£åˆ†ç±»)

**ä¸€è‡´æ€§ä¿è¯æœºåˆ¶ï¼š**
1. **é•¿åº¦æ§åˆ¶**ï¼šæ€»é•¿åº¦ä¸¥æ ¼ä¸è¶…è¿‡30ä¸ªå­—ç¬¦
2. **æ ¼å¼ç»Ÿä¸€**ï¼šä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿æ ¼å¼
3. **é¿å…ç‰¹æ®Šå­—ç¬¦**ï¼šä¸ä½¿ç”¨ / \\ : * ? " < > |
4. **ç®€æ´åŸåˆ™**ï¼šæ··åˆå†…å®¹å¿…é¡»ä½¿ç”¨æè¿°æ€§åç§°

**åˆ†ææ­¥éª¤ï¼š**
1. ç»Ÿè®¡æ–‡ä»¶ä¸­çš„ä¸åŒIPæ•°é‡
2. **å…³é”®åˆ¤æ–­**ï¼š
   - å¦‚æœæ˜¯å•ä¸€IPï¼šä½¿ç”¨å¯¹åº”çš„ç”µå½±/ç”µè§†å‰§è§„åˆ™
   - **å¦‚æœæ˜¯å¤šä¸ªä¸åŒIPï¼šå¿…é¡»ä½¿ç”¨æ··åˆå†…å®¹çš„æè¿°æ€§åç§°è§„åˆ™ï¼Œç»å¯¹ä¸èƒ½åˆ—å‡ºæ‰€æœ‰ä½œå“åç§°**
3. æ£€æŸ¥åç§°é•¿åº¦æ˜¯å¦åœ¨30å­—ç¬¦ä»¥å†…
4. ç¡®ä¿æ ¼å¼ç¬¦åˆè§„èŒƒ

**ğŸš¨ ç‰¹åˆ«å¼ºè°ƒï¼ˆæ··åˆå†…å®¹å¤„ç†ï¼‰ï¼š**
- å½“æ–‡ä»¶å¤¹åŒ…å«2ä¸ªæˆ–ä»¥ä¸Šä¸åŒIPçš„å†…å®¹æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç®€æ´çš„æè¿°æ€§åç§°
- ç»å¯¹ä¸èƒ½ä½¿ç”¨"ä½œå“1, ä½œå“2, ä½œå“3"çš„æ ¼å¼
- å¿…é¡»ä½¿ç”¨"ç”µè§†å‰§åˆé›†"ã€"å‰§é›†æ”¶è—"ã€"ç»å…¸å‰§é›†"ç­‰æè¿°æ€§åç§°
- ä¸è¦è¿”å›æ•°ç»„æˆ–å¤šä¸ªå»ºè®®ï¼Œåªè¿”å›ä¸€ä¸ªæœ€ä½³çš„æè¿°æ€§åç§°

**è¾“å‡ºè¦æ±‚ï¼š**
åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«æœ€ä½³å»ºè®®ï¼š
{
    "suggested_name": "å»ºè®®çš„æ–‡ä»¶å¤¹åç§°ï¼ˆ30å­—ç¬¦ä»¥å†…ï¼‰",
    "media_type": "movie/tv/mixed",
    "title_source": "ä¸­æ–‡å®˜æ–¹è¯‘å/ä¸­æ–‡é€šç”¨è¯‘å/æè¿°æ€§åç§°",
    "year_range": "å¹´ä»½æˆ–å¹´ä»½èŒƒå›´",
    "content_count": "ä½œå“æ•°é‡ç»Ÿè®¡",
    "reasoning": "å‘½åç†ç”±ï¼ˆè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªåç§°ï¼Œå­—ç¬¦æ•°ç»Ÿè®¡ï¼‰"
}

æ–‡ä»¶åˆ—è¡¨ï¼š
"""


def generate_folder_name_with_ai(file_list):
    """ä½¿ç”¨AIç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®"""
    user_input_content = repr(file_list)
    folder_name_prompt = get_folder_naming_prompt()

    max_retries = AI_MAX_RETRIES
    retry_delay = AI_RETRY_DELAY
    suggested_name = None

    # æ„å»ºå®Œæ•´çš„æç¤ºè¯
    full_prompt = f"{folder_name_prompt}\n\n{user_input_content}"

    for attempt in range(max_retries):
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            check_task_cancelled()

            # ä½¿ç”¨ç»Ÿä¸€çš„AI APIè°ƒç”¨å‡½æ•°
            ai_content = call_ai_api(full_prompt, GROUPING_MODEL)

            if not ai_content:
                logging.warning(f"AI APIè°ƒç”¨è¿”å›ç©ºç»“æœ (å°è¯• {attempt + 1}/{max_retries})")
                continue

            logging.info(f"AIåŸå§‹å“åº”: {ai_content}")

            # è§£æAIå“åº”
            suggested_name = parse_folder_name_from_ai_response(ai_content)

            if suggested_name:
                break

        except Exception as e:
            logging.error(f"AIè¯·æ±‚å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
            else:
                raise AIServiceError(f'AIæœåŠ¡è¯·æ±‚å¤±è´¥: {str(e)}')

    if not suggested_name:
        raise AIServiceError('AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ–‡ä»¶å¤¹åç§°å»ºè®®')

    return suggested_name


def parse_folder_name_from_ai_response(ai_content):
    """ä»AIå“åº”ä¸­è§£ææ–‡ä»¶å¤¹åç§°"""
    # é¦–å…ˆå°è¯•è§£æJSONæ ¼å¼
    json_data = parse_json_from_ai_response(ai_content)

    if json_data:
        if isinstance(json_data, dict) and 'suggested_name' in json_data:
            suggested_name = json_data['suggested_name']
            media_type = json_data.get('media_type', '')
            reasoning = json_data.get('reasoning', '')
            logging.info(f"ğŸ¯ æˆåŠŸè§£æJSONæ ¼å¼çš„å»ºè®®åç§°: {suggested_name}")
            logging.info(f"ğŸ“Š åª’ä½“ç±»å‹: {media_type}, å‘½åç†ç”±: {reasoning}")
            return suggested_name
        elif isinstance(json_data, list) and len(json_data) > 0:
            first_item = json_data[0]
            if isinstance(first_item, dict) and 'suggested_name' in first_item:
                suggested_name = first_item['suggested_name']
                media_type = first_item.get('media_type', '')
                reasoning = first_item.get('reasoning', '')
                logging.info(f"ğŸ¯ æˆåŠŸè§£æJSONæ•°ç»„æ ¼å¼çš„å»ºè®®åç§°: {suggested_name}")
                logging.info(f"ğŸ“Š åª’ä½“ç±»å‹: {media_type}, å‘½åç†ç”±: {reasoning}")
                return suggested_name

    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”å†…å®¹
    logging.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹")

    # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„æ ¼å¼åŒ–å­—ç¬¦
    clean_content = ai_content.strip()
    clean_content = re.sub(r'```json|```', '', clean_content)
    clean_content = re.sub(r'[{}"\[\]]', '', clean_content)
    clean_content = re.sub(r'suggested_name\s*:\s*', '', clean_content)
    clean_content = clean_content.strip()

    if clean_content and len(clean_content) <= 100:  # åˆç†çš„æ–‡ä»¶å¤¹åç§°é•¿åº¦
        return clean_content

    return None


def clean_suggested_folder_name(suggested_name):
    """æ¸…ç†å’ŒéªŒè¯å»ºè®®çš„æ–‡ä»¶å¤¹åç§°"""
    if not suggested_name or not isinstance(suggested_name, str):
        return None

    # æ¸…ç†å»ºè®®çš„åç§°
    suggested_name = suggested_name.strip().strip('"\'')

    # ç§»é™¤å¯èƒ½çš„éæ³•å­—ç¬¦
    invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
    for char in invalid_chars:
        suggested_name = suggested_name.replace(char, '')

    # é™åˆ¶é•¿åº¦
    if len(suggested_name) > 50:
        suggested_name = suggested_name[:50]

    # ç¡®ä¿åç§°ä¸ä¸ºç©º
    if not suggested_name:
        return None

    return suggested_name


@app.route('/suggest_folder_name', methods=['POST'])
@performance_monitor_decorator('suggest_folder_name')
def suggest_folder_name():
    """æ ¹æ®æ–‡ä»¶å¤¹å†…å®¹æ™ºèƒ½å»ºè®®æ–‡ä»¶å¤¹åç§°"""
    try:
        # å¼€å§‹æ–°ä»»åŠ¡
        start_new_task(f"suggest_name_{int(time.time())}")

        # æ¸…ç†æ“ä½œç›¸å…³ç¼“å­˜
        clear_operation_related_caches(operation_type="renaming")

        folder_id_str = request.form.get('folder_id', '0')

        # éªŒè¯æ–‡ä»¶å¤¹ID
        folder_id, error_msg = validate_folder_id_for_naming(folder_id_str)
        if error_msg:
            return jsonify({'success': False, 'error': error_msg})

        logging.info(f"ä¸ºæ–‡ä»¶å¤¹ {folder_id} ç”Ÿæˆæ™ºèƒ½å‘½åå»ºè®®")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        check_task_cancelled()

        # è·å–æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘æ–‡ä»¶ï¼ˆé‡‡æ ·å¤„ç†ï¼‰
        video_files, sampled_video_files = get_sampled_video_files(folder_id)

        # å‡†å¤‡AIåˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        file_list = [{'fileId': item['fileId'], 'filename': item['filename']} for item in sampled_video_files]

        # ä½¿ç”¨AIç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®
        suggested_name = generate_folder_name_with_ai(file_list)

        # æ¸…ç†å’ŒéªŒè¯å»ºè®®çš„åç§°
        clean_name = clean_suggested_folder_name(suggested_name)

        if clean_name:
            logging.info(f"ğŸ¤– AIç”Ÿæˆçš„æ–‡ä»¶å¤¹åç§°å»ºè®®: {clean_name}")
            return jsonify({
                'success': True,
                'suggested_name': clean_name,
                'file_count': len(video_files)
            })
        else:
            return jsonify({'success': False, 'error': 'AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ–‡ä»¶å¤¹åç§°å»ºè®®'})

    except TaskCancelledException:
        logging.info("ğŸ›‘ æ–‡ä»¶å¤¹å‘½åå»ºè®®ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
        return jsonify({'success': False, 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True})
    except (ValidationError, FileSystemError, AIServiceError) as e:
        logging.error(f"æ™ºèƒ½æ–‡ä»¶å¤¹å‘½åå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.error(f"æ™ºèƒ½æ–‡ä»¶å¤¹å‘½åæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'ç³»ç»Ÿå†…éƒ¨é”™è¯¯: {str(e)}'})

@app.route('/organize_files_by_groups', methods=['POST'])
def organize_files_by_groups():
    """æ ¹æ®movie_infoæ™ºèƒ½åˆ†ç»„å¹¶ç§»åŠ¨æ–‡ä»¶"""
    try:
        folder_id_str = request.form.get('folder_id', '0')
        create_subfolders = request.form.get('create_subfolders', 'true').lower() == 'true'

        # éªŒè¯æ–‡ä»¶å¤¹ID
        folder_id, error_msg = validate_folder_id(folder_id_str)
        if error_msg:
            return jsonify({'success': False, 'error': error_msg})

        logging.info(f"å¼€å§‹æ•´ç†æ–‡ä»¶å¤¹ {folder_id} çš„æ–‡ä»¶åˆ†ç»„")

        # è·å–æ–‡ä»¶å¤¹å±æ€§å’Œmovie_info
        video_files = []
        try:
            get_video_files_recursively(folder_id, video_files)
        except Exception as e:
            logging.error(f"é€’å½’è·å–è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}'})

        if not video_files:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶'})

        # ç”Ÿæˆmovie_infoåˆ†ç»„ - ä½¿ç”¨é‡æ„åçš„å‡½æ•°
        logging.info(f"ğŸ“Š å¼€å§‹æ™ºèƒ½æ–‡ä»¶åˆ†ç»„ï¼Œå…± {len(video_files)} ä¸ªæ–‡ä»¶")

        # è°ƒç”¨é‡æ„åçš„åˆ†ç»„å¤„ç†å‡½æ•°
        logging.info(f"ğŸ¯ å¼€å§‹è°ƒç”¨æ™ºèƒ½åˆ†ç»„å¤„ç†å‡½æ•°ï¼Œæ–‡ä»¶æ•°é‡: {len(video_files)}")
        logging.info(f"ğŸ”§ ä½¿ç”¨AIæ¨¡å‹: {GROUPING_MODEL}")
        logging.info(f"ğŸŒ APIåœ°å€: {AI_API_URL}")

        movie_info = process_files_for_grouping(video_files, f"æ–‡ä»¶å¤¹{folder_id}")

        if not movie_info:
            logging.warning("âš ï¸ æ™ºèƒ½åˆ†ç»„æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆåˆ†ç»„ï¼Œå°è¯•é‡è¯•æœºåˆ¶")
            logging.info(f"ğŸ” è¯Šæ–­ä¿¡æ¯ - APIå¯†é’¥çŠ¶æ€: {'å·²é…ç½®' if AI_API_KEY else 'æœªé…ç½®'}")
            logging.info(f"ğŸ” è¯Šæ–­ä¿¡æ¯ - APIåœ°å€: {AI_API_URL}")
            logging.info(f"ğŸ” è¯Šæ–­ä¿¡æ¯ - åˆ†ç»„æ¨¡å‹: {GROUPING_MODEL}")

            # å¦‚æœç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œå°è¯•é‡è¯•
            max_retries = GROUPING_MAX_RETRIES
            for attempt in range(max_retries):
                try:
                    logging.info(f"ğŸ”„ é‡è¯•æ™ºèƒ½åˆ†ç»„ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                    time.sleep(GROUPING_RETRY_DELAY)  # ä½¿ç”¨å…¨å±€é…ç½®çš„é‡è¯•å»¶è¿Ÿ
                    movie_info = process_files_for_grouping(video_files, f"æ–‡ä»¶å¤¹{folder_id}_é‡è¯•{attempt+1}")
                    if movie_info:
                        logging.info(f"âœ… é‡è¯•æˆåŠŸï¼Œè·å¾— {len(movie_info)} ä¸ªåˆ†ç»„")
                        break
                except Exception as e:
                    logging.error(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•å¤±è´¥: {e}")
                    continue

        if not movie_info or not isinstance(movie_info, list) or len(movie_info) == 0:
            error_msg = 'æ— æ³•åˆ†ææ–‡ä»¶åˆ†ç»„ä¿¡æ¯ã€‚å¯èƒ½åŸå› ï¼š1) AI APIè¿æ¥å¤±è´¥ 2) æ¨¡å‹ä¸å¯ç”¨ 3) æ–‡ä»¶åæ ¼å¼æ— æ³•è¯†åˆ«ã€‚è¯·æ£€æŸ¥AIé…ç½®æˆ–ç¨åé‡è¯•ã€‚'
            logging.error(f"âŒ æ™ºèƒ½åˆ†ç»„å®Œå…¨å¤±è´¥: {error_msg}")
            return jsonify({'success': False, 'error': error_msg})

        successful_operations = []
        failed_operations = []

        if create_subfolders:
            # åˆ›å»ºå­æ–‡ä»¶å¤¹å¹¶ç§»åŠ¨æ–‡ä»¶
            for group in movie_info:
                group_name = group.get('group_name', 'æœªçŸ¥åˆ†ç»„')
                # å…¼å®¹ä¸åŒçš„å­—æ®µåç§°
                file_ids = group.get('fileIds', []) or group.get('files', [])

                if not file_ids:
                    continue

                try:
                    # åˆ›å»ºå­æ–‡ä»¶å¤¹
                    folder_result = create_folder_in_cloud(group_name, folder_id)
                    if folder_result:
                        new_folder_id = folder_result.get('dirID')
                        if new_folder_id:
                            # ç§»åŠ¨æ–‡ä»¶åˆ°æ–°æ–‡ä»¶å¤¹
                            move_result = move(file_ids, new_folder_id)
                            if move_result.get('success'):
                                successful_operations.append({
                                    'group_name': group_name,
                                    'folder_id': new_folder_id,
                                    'file_count': len(file_ids),
                                    'operation': 'created_and_moved'
                                })
                            else:
                                failed_operations.append({
                                    'group_name': group_name,
                                    'error': f'ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {move_result.get("message", "æœªçŸ¥é”™è¯¯")}'
                                })
                        else:
                            failed_operations.append({
                                'group_name': group_name,
                                'error': 'åˆ›å»ºæ–‡ä»¶å¤¹æˆåŠŸä½†æœªè¿”å›æ–‡ä»¶å¤¹ID'
                            })
                    else:
                        failed_operations.append({
                            'group_name': group_name,
                            'error': 'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥'
                        })

                except Exception as e:
                    logging.error(f"å¤„ç†åˆ†ç»„ {group_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    failed_operations.append({
                        'group_name': group_name,
                        'error': str(e)
                    })

        # è¿”å›ç»“æœ
        if create_subfolders:
            total_successful = len(successful_operations)
            total_failed = len(failed_operations)

            logging.info(f"æ–‡ä»¶æ•´ç†å®Œæˆ: æˆåŠŸ {total_successful} ä¸ªåˆ†ç»„ï¼Œå¤±è´¥ {total_failed} ä¸ªåˆ†ç»„")

            return jsonify({
                'success': total_failed == 0,
                'message': f'æ–‡ä»¶æ•´ç†å®Œæˆ: æˆåŠŸå¤„ç† {total_successful} ä¸ªåˆ†ç»„' + (f'ï¼Œå¤±è´¥ {total_failed} ä¸ªåˆ†ç»„' if total_failed > 0 else ''),
                'successful_count': total_successful,
                'failed_count': total_failed,
                'successful_operations': successful_operations,
                'failed_operations': failed_operations,
                'movie_info': movie_info,
                'video_files': video_files,
                'count': len(video_files),
                'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB"
            })
        else:
            # åªè¿”å›åˆ†ç»„ä¿¡æ¯ï¼Œä¸æ‰§è¡Œæ–‡ä»¶ç§»åŠ¨
            logging.info(f"è¿”å›åˆ†ç»„å»ºè®®: {len(movie_info)} ä¸ªåˆ†ç»„")
            return jsonify({
                'success': True,
                'message': f'å‘ç° {len(movie_info)} ä¸ªå»ºè®®åˆ†ç»„',
                'movie_info': movie_info,
                'video_files': video_files,
                'count': len(video_files),
                'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB"
            })

    except Exception as e:
        logging.error(f"æ™ºèƒ½æ–‡ä»¶åˆ†ç»„æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/create_folder', methods=['POST'])
def create_folder():
    """åˆ›å»ºæ–°æ–‡ä»¶å¤¹"""
    try:
        folder_name = request.form.get('folder_name')
        parent_id = request.form.get('parent_id', '0')

        if not folder_name:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©ºã€‚'})

        # éªŒè¯æ–‡ä»¶å¤¹åç§°
        folder_name = folder_name.strip()
        if not folder_name:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©ºã€‚'})

        # æ£€æŸ¥æ–‡ä»¶å¤¹åç§°æ˜¯å¦åŒ…å«éæ³•å­—ç¬¦
        invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        for char in invalid_chars:
            if char in folder_name:
                return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤¹åç§°ä¸èƒ½åŒ…å«å­—ç¬¦: {char}'})

        try:
            parent_id = int(parent_id)
        except ValueError:
            parent_id = 0

        logging.info(f"ğŸ“ å‡†å¤‡åˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}ï¼Œçˆ¶ç›®å½•ID: {parent_id}")

        # ä½¿ç”¨123pan APIåˆ›å»ºæ–‡ä»¶å¤¹
        try:
            result = create_folder_in_cloud(folder_name, parent_id)

            if result:
                dir_id = result.get('dirID') if result else None
                logging.info(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {folder_name}ï¼Œæ–°æ–‡ä»¶å¤¹ID: {dir_id}")
                return jsonify({
                    'success': True,
                    'message': f'æ–‡ä»¶å¤¹ "{folder_name}" åˆ›å»ºæˆåŠŸ',
                    'dir_id': dir_id
                })
            else:
                logging.error(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: APIè¿”å›ç©ºç»“æœ")
                return jsonify({'success': False, 'error': 'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥'})

        except Exception as api_error:
            logging.error(f"è°ƒç”¨123pan APIåˆ›å»ºæ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {api_error}", exc_info=True)
            return jsonify({'success': False, 'error': f'APIè°ƒç”¨å¤±è´¥: {str(api_error)}'})

    except Exception as e:
        logging.error(f"åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/restart', methods=['POST'])
def restart_app():
    """é‡å¯åº”ç”¨ç¨‹åº"""
    logging.info("æ¥æ”¶åˆ°é‡å¯åº”ç”¨ç¨‹åºçš„è¯·æ±‚ã€‚")
    try:
        response = jsonify({'success': True, 'message': 'åº”ç”¨ç¨‹åºæ­£åœ¨é‡å¯...'})

        def restart_process_async():
            time.sleep(1)
            print("Initiating new process...")
            try:
                # æ£€æµ‹æ˜¯å¦ä¸ºæ‰“åŒ…ç¯å¢ƒ
                is_packaged = getattr(sys, 'frozen', False)

                if is_packaged:
                    # æ‰“åŒ…ç¯å¢ƒï¼šä½¿ç”¨å½“å‰å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
                    if hasattr(sys, '_MEIPASS'):
                        # PyInstalleræ‰“åŒ…ç¯å¢ƒ
                        executable_path = sys.executable
                        # å°è¯•è·å–åŸå§‹å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
                        if len(sys.argv) > 0 and os.path.exists(sys.argv[0]):
                            executable_path = sys.argv[0]
                        elif os.path.exists(os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-mac')):
                            executable_path = os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-mac')
                        elif os.path.exists(os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-win.exe')):
                            executable_path = os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-win.exe')
                        elif os.path.exists(os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-linux')):
                            executable_path = os.path.join(os.path.dirname(sys.executable), 'pan123-scraper-linux')

                        command = [executable_path]
                        print(f"æ‰“åŒ…ç¯å¢ƒé‡å¯å‘½ä»¤: {command}")
                    else:
                        # å…¶ä»–æ‰“åŒ…ç¯å¢ƒ
                        command = [sys.executable]
                else:
                    # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨Pythonè§£é‡Šå™¨
                    command = [sys.executable] + sys.argv
                    print(f"å¼€å‘ç¯å¢ƒé‡å¯å‘½ä»¤: {command}")

                # å¯åŠ¨æ–°è¿›ç¨‹
                subprocess.Popen(command,
                                stdout=subprocess.DEVNULL,
                                stderr=subprocess.DEVNULL,
                                cwd=os.getcwd())
                print("New process started. Exiting old process.")

                # å»¶è¿Ÿé€€å‡ºï¼Œç¡®ä¿å“åº”å‘é€å®Œæˆ
                time.sleep(0.5)
                os._exit(0)

            except Exception as e:
                print(f"Failed to start new process: {e}")
                logging.error(f"é‡å¯è¿›ç¨‹å¤±è´¥: {e}", exc_info=True)

                # å¤‡ç”¨é‡å¯æ–¹æ³•ï¼šä½¿ç”¨å®‰å…¨çš„subprocess
                try:
                    print("å°è¯•å¤‡ç”¨é‡å¯æ–¹æ³•...")
                    if is_packaged:
                        if sys.platform == "darwin":  # macOS
                            subprocess.Popen(["nohup", "./pan123-scraper-mac"],
                                           stdout=subprocess.DEVNULL,
                                           stderr=subprocess.DEVNULL)
                        elif sys.platform == "win32":  # Windows
                            subprocess.Popen(["pan123-scraper-win.exe"],
                                           creationflags=subprocess.CREATE_NEW_CONSOLE)
                        else:  # Linux
                            subprocess.Popen(["nohup", "./pan123-scraper-linux"],
                                           stdout=subprocess.DEVNULL,
                                           stderr=subprocess.DEVNULL)
                    else:
                        # å¼€å‘ç¯å¢ƒ
                        subprocess.Popen([sys.executable] + sys.argv,
                                       stdout=subprocess.DEVNULL,
                                       stderr=subprocess.DEVNULL)

                    print("å¤‡ç”¨é‡å¯æ–¹æ³•æ‰§è¡Œå®Œæˆ")
                    time.sleep(0.5)
                    os._exit(0)
                except Exception as backup_e:
                    print(f"å¤‡ç”¨é‡å¯æ–¹æ³•ä¹Ÿå¤±è´¥: {backup_e}")
                    logging.error(f"å¤‡ç”¨é‡å¯æ–¹æ³•å¤±è´¥: {backup_e}", exc_info=True)

        Thread(target=restart_process_async).start()
        return response
    except Exception as e:
        logging.error(f"é‡å¯åº”ç”¨ç¨‹åºå¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'é‡å¯å¤±è´¥: {str(e)}'})

@app.route('/restart_status', methods=['GET'])
def restart_status():
    """æ£€æŸ¥é‡å¯åŠŸèƒ½çŠ¶æ€"""
    try:
        is_packaged = getattr(sys, 'frozen', False)

        status = {
            'restart_available': True,
            'environment': 'packaged' if is_packaged else 'development',
            'platform': sys.platform,
            'executable_path': sys.executable,
            'argv': sys.argv,
            'working_directory': os.getcwd()
        }

        if is_packaged:
            # æ£€æŸ¥å¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            possible_executables = []
            if sys.platform == "darwin":
                possible_executables = [
                    './pan123-scraper-mac',
                    'pan123-scraper-mac',
                    './dist/pan123-scraper-mac',
                    'dist/pan123-scraper-mac',
                    sys.executable  # å½“å‰è¿è¡Œçš„å¯æ‰§è¡Œæ–‡ä»¶
                ]
            elif sys.platform == "win32":
                possible_executables = [
                    './pan123-scraper-win.exe',
                    'pan123-scraper-win.exe',
                    './dist/pan123-scraper-win.exe',
                    'dist/pan123-scraper-win.exe',
                    sys.executable
                ]
            else:
                possible_executables = [
                    './pan123-scraper-linux',
                    'pan123-scraper-linux',
                    './dist/pan123-scraper-linux',
                    'dist/pan123-scraper-linux',
                    sys.executable
                ]

            executable_found = False
            for exe in possible_executables:
                if os.path.exists(exe):
                    status['detected_executable'] = os.path.abspath(exe)
                    executable_found = True
                    break

            # å¦‚æœå½“å‰å°±æ˜¯ä»å¯æ‰§è¡Œæ–‡ä»¶å¯åŠ¨çš„ï¼Œé‚£ä¹ˆé‡å¯åŠŸèƒ½åº”è¯¥å¯ç”¨
            if not executable_found and sys.executable and os.path.exists(sys.executable):
                status['detected_executable'] = sys.executable
                executable_found = True

            if not executable_found:
                status['restart_available'] = False
                status['error'] = 'æœªæ‰¾åˆ°å¯æ‰§è¡Œæ–‡ä»¶'
            else:
                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ£€æµ‹åˆ°çš„å¯æ‰§è¡Œæ–‡ä»¶æœ‰æ‰§è¡Œæƒé™
                detected_exe = status.get('detected_executable')
                if detected_exe and not os.access(detected_exe, os.X_OK):
                    status['restart_available'] = False
                    status['error'] = f'å¯æ‰§è¡Œæ–‡ä»¶ç¼ºå°‘æ‰§è¡Œæƒé™: {detected_exe}'

        return jsonify(status)
    except Exception as e:
        logging.error(f"æ£€æŸ¥é‡å¯çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'restart_available': False,
            'error': f'çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}'
        })

@app.route('/delete_empty_folders', methods=['POST'])
def delete_empty_folders():
    """åˆ é™¤æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹æˆ–ä¸åŒ…å«è§†é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹"""
    try:
        folder_id = request.form.get('folder_id', '0')

        # éªŒè¯è¾“å…¥
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        try:
            folder_id = int(folder_id)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'})

        logging.info(f"å¼€å§‹åˆ é™¤æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„ç©ºæ–‡ä»¶å¤¹æˆ–ä¸åŒ…å«è§†é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹")

        # é€’å½’è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
        def get_all_subfolders(parent_id):
            """é€’å½’è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹"""
            all_folders = []
            try:
                # ä½¿ç”¨ç°æœ‰çš„ list_all_files å‡½æ•°
                folder_content = get_all_files_in_folder(parent_id, limit=100)
                for item in folder_content:
                    if item.get('type') == 1:  # æ–‡ä»¶å¤¹
                        folder_info = {
                            'fileId': item['fileId'],
                            'filename': item['filename'],
                            'parentFileId': item['parentFileId']
                        }
                        all_folders.append(folder_info)
                        # é€’å½’è·å–å­æ–‡ä»¶å¤¹
                        sub_folders = get_all_subfolders(item['fileId'])
                        all_folders.extend(sub_folders)
            except Exception as e:
                logging.error(f"è·å–å­æ–‡ä»¶å¤¹å¤±è´¥: {e}")

            return all_folders

        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºæˆ–ä¸åŒ…å«è§†é¢‘æ–‡ä»¶
        def is_folder_empty_or_no_videos(folder_id):
            """æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºæˆ–ä¸åŒ…å«è§†é¢‘æ–‡ä»¶"""
            try:
                # ä½¿ç”¨ç°æœ‰çš„ list_all_files å‡½æ•°
                folder_content = get_all_files_in_folder(folder_id, limit=100)

                # å¦‚æœæ–‡ä»¶å¤¹å®Œå…¨ä¸ºç©ºï¼Œå¯ä»¥åˆ é™¤
                if len(folder_content) == 0:
                    return True

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘æ–‡ä»¶
                has_video_files = False
                for item in folder_content:
                    if item.get('type') == 0:  # æ–‡ä»¶ç±»å‹
                        filename = item.get('filename', '').lower()
                        if any(filename.endswith(ext) for ext in SUPPORTED_MEDIA_TYPES):
                            has_video_files = True
                            break

                # å¦‚æœä¸åŒ…å«è§†é¢‘æ–‡ä»¶ï¼Œå¯ä»¥åˆ é™¤
                return not has_video_files

            except Exception as e:
                logging.error(f"æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦åŒ…å«è§†é¢‘æ–‡ä»¶å¤±è´¥: {e}")
                return False

        # åˆ é™¤æ–‡ä»¶å¤¹
        def delete_folder(folder_id):
            """åˆ é™¤æŒ‡å®šçš„æ–‡ä»¶å¤¹"""
            try:
                # ä½¿ç”¨ç°æœ‰çš„ delete å‡½æ•°
                result = delete([folder_id])
                return result.get('success', False)
            except Exception as e:
                logging.error(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥: {e}")
                return False

        # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
        all_folders = get_all_subfolders(folder_id)
        logging.info(f"æ‰¾åˆ° {len(all_folders)} ä¸ªå­æ–‡ä»¶å¤¹")

        # æŒ‰æ·±åº¦æ’åºï¼Œä»æœ€æ·±çš„å¼€å§‹åˆ é™¤ï¼ˆé¿å…åˆ é™¤çˆ¶æ–‡ä»¶å¤¹åå­æ–‡ä»¶å¤¹æ— æ³•è®¿é—®ï¼‰
        # è¿™é‡Œç®€å•æŒ‰fileIdå€’åºæ’åºï¼Œé€šå¸¸fileIdè¶Šå¤§çš„æ–‡ä»¶å¤¹è¶Šæ·±
        all_folders.sort(key=lambda x: x['fileId'], reverse=True)

        deleted_count = 0
        for folder in all_folders:
            if is_folder_empty_or_no_videos(folder['fileId']):
                if delete_folder(folder['fileId']):
                    deleted_count += 1
                    logging.info(f"åˆ é™¤ç©ºæ–‡ä»¶å¤¹æˆ–æ— è§†é¢‘æ–‡ä»¶å¤¹: {folder['filename']} (ID: {folder['fileId']})")
                else:
                    logging.warning(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥: {folder['filename']} (ID: {folder['fileId']})")

        logging.info(f"åˆ é™¤ç©ºæ–‡ä»¶å¤¹æˆ–æ— è§†é¢‘æ–‡ä»¶å¤¹å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶å¤¹")
        return jsonify({
            'success': True,
            'deleted_count': deleted_count,
            'message': f'æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªç©ºæ–‡ä»¶å¤¹æˆ–æ— è§†é¢‘æ–‡ä»¶å¤¹'
        })

    except Exception as e:
        logging.error(f"åˆ é™¤ç©ºæ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/execute_selected_groups', methods=['POST'])
def execute_selected_groups():
    """æ‰§è¡Œé€‰ä¸­çš„åˆ†ç»„"""
    try:
        folder_id = request.form.get('folder_id', '0')
        selected_groups_json = request.form.get('selected_groups')

        # éªŒè¯è¾“å…¥
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        if not selected_groups_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©è¦æ‰§è¡Œçš„åˆ†ç»„'})

        try:
            folder_id = int(folder_id)
            selected_groups = json.loads(selected_groups_json)
        except (ValueError, json.JSONDecodeError) as e:
            return jsonify({'success': False, 'error': f'æ•°æ®æ ¼å¼é”™è¯¯: {e}'})

        logging.info(f"å¼€å§‹æ‰§è¡Œé€‰ä¸­çš„åˆ†ç»„ï¼Œæ–‡ä»¶å¤¹ID: {folder_id}ï¼Œé€‰ä¸­åˆ†ç»„æ•°: {len(selected_groups)}")

        success_count = 0
        failed_count = 0

        for group in selected_groups:
            try:
                # è·å–åˆ†ç»„åç§°å’Œæ–‡ä»¶IDåˆ—è¡¨
                group_name = group.get('group_name', '')
                # å…¼å®¹ä¸åŒçš„å­—æ®µåç§°ï¼šfileIds æˆ– files
                file_ids = group.get('fileIds', []) or group.get('files', [])

                if not group_name or not file_ids:
                    logging.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆåˆ†ç»„: group_name='{group_name}', file_ids={len(file_ids) if file_ids else 0}, åŸå§‹æ•°æ®: {group}")
                    failed_count += 1
                    continue

                logging.info(f"ğŸ¯ å‡†å¤‡æ‰§è¡Œåˆ†ç»„ '{group_name}': {len(file_ids)} ä¸ªæ–‡ä»¶")

                # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœå·²å­˜åœ¨ä¼šè¿”å›ç°æœ‰æ–‡ä»¶å¤¹IDï¼‰
                result = create_folder_in_cloud(group_name, folder_id)
                if result and 'data' in result and result['data'] and result['data'].get('dirID'):
                    target_folder_id = result['data']['dirID']
                    logging.info(f"ğŸ“ ä½¿ç”¨æ–‡ä»¶å¤¹ '{group_name}'ï¼ŒID: {target_folder_id}")

                    # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹
                    move_result = move(file_ids, target_folder_id)
                    if move_result.get('success', False):
                        success_count += 1
                        logging.info(f"âœ… æˆåŠŸæ‰§è¡Œåˆ†ç»„ '{group_name}': ç§»åŠ¨äº† {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
                    else:
                        failed_count += 1
                        logging.error(f"âŒ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œåˆ†ç»„ '{group_name}': {move_result}")
                else:
                    failed_count += 1
                    logging.error(f"âŒ åˆ›å»º/è·å–æ–‡ä»¶å¤¹å¤±è´¥ï¼Œåˆ†ç»„ '{group_name}': {result}")

            except Exception as e:
                failed_count += 1
                logging.error(f"æ‰§è¡Œåˆ†ç»„æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

        logging.info(f"åˆ†ç»„æ‰§è¡Œå®Œæˆ: æˆåŠŸ {success_count} ä¸ªåˆ†ç»„ï¼Œå¤±è´¥ {failed_count} ä¸ªåˆ†ç»„")
        return jsonify({
            'success': True,
            'success_count': success_count,
            'failed_count': failed_count,
            'message': f'åˆ†ç»„æ‰§è¡Œå®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª'
        })

    except Exception as e:
        logging.error(f"æ‰§è¡Œé€‰ä¸­åˆ†ç»„æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

def get_process_using_port(port):
    """è·å–å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹ä¿¡æ¯"""
    import subprocess
    import sys

    try:
        # éªŒè¯ç«¯å£å·æ˜¯å¦ä¸ºæœ‰æ•ˆæ•´æ•°
        if not isinstance(port, int) or port < 1 or port > 65535:
            logging.warning(f"æ— æ•ˆçš„ç«¯å£å·: {port}")
            return None

        if sys.platform == "win32":
            # Windowsç³»ç»Ÿä½¿ç”¨netstatå‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["netstat", "-ano"],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if f':{port}' in line and 'LISTENING' in line:
                        parts = line.split()
                        if len(parts) >= 5:
                            pid = parts[-1]
                            return int(pid) if pid.isdigit() else None
        else:
            # Linux/macOSç³»ç»Ÿä½¿ç”¨lsofå‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["lsof", f"-ti:{port}"],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and result.stdout.strip():
                pid = result.stdout.strip().split('\n')[0]
                return int(pid) if pid.isdigit() else None
    except Exception as e:
        logging.debug(f"è·å–ç«¯å£ {port} å ç”¨è¿›ç¨‹ä¿¡æ¯å¤±è´¥: {e}")

    return None


def kill_process_by_pid(pid):
    """æ ¹æ®PIDç»“æŸè¿›ç¨‹"""
    import subprocess
    import sys

    try:
        # éªŒè¯PIDæ˜¯å¦ä¸ºæœ‰æ•ˆæ•´æ•°
        if not isinstance(pid, int) or pid <= 0:
            logging.warning(f"æ— æ•ˆçš„PID: {pid}")
            return False

        if sys.platform == "win32":
            # Windowsç³»ç»Ÿä½¿ç”¨taskkillå‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["taskkill", "/F", "/PID", str(pid)],
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        else:
            # Linux/macOSç³»ç»Ÿä½¿ç”¨killå‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["kill", "-9", str(pid)],
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
    except Exception as e:
        logging.error(f"ç»“æŸè¿›ç¨‹ {pid} å¤±è´¥: {e}")
        return False


def get_process_name_by_pid(pid):
    """æ ¹æ®PIDè·å–è¿›ç¨‹åç§°"""
    import subprocess
    import sys

    try:
        # éªŒè¯PIDæ˜¯å¦ä¸ºæœ‰æ•ˆæ•´æ•°
        if not isinstance(pid, int) or pid <= 0:
            logging.warning(f"æ— æ•ˆçš„PID: {pid}")
            return "æ— æ•ˆPID"

        if sys.platform == "win32":
            # Windowsç³»ç»Ÿä½¿ç”¨tasklistå‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["tasklist", "/FI", f"PID eq {pid}", "/FO", "CSV", "/NH"],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                if lines:
                    # CSVæ ¼å¼ï¼šè¿›ç¨‹å,PID,ä¼šè¯å,ä¼šè¯å·,å†…å­˜ä½¿ç”¨
                    parts = lines[0].split(',')
                    if len(parts) >= 1:
                        return parts[0].strip('"')
        else:
            # Linux/macOSç³»ç»Ÿä½¿ç”¨pså‘½ä»¤ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰
            result = subprocess.run(["ps", "-p", str(pid), "-o", "comm="],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
    except Exception as e:
        logging.debug(f"è·å–è¿›ç¨‹ {pid} åç§°å¤±è´¥: {e}")

    return "æœªçŸ¥è¿›ç¨‹"


def kill_port_process(port, force=True):
    """ç»“æŸå ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹

    Args:
        port (int): ç«¯å£å·
        force (bool): æ˜¯å¦å¼ºåˆ¶ç»“æŸè¿›ç¨‹

    Returns:
        bool: æ˜¯å¦æˆåŠŸç»“æŸè¿›ç¨‹
    """
    try:
        pid = get_process_using_port(port)
        if pid is None:
            logging.debug(f"ç«¯å£ {port} æœªè¢«å ç”¨")
            return True

        process_name = get_process_name_by_pid(pid)
        logging.info(f"ğŸ” æ£€æµ‹åˆ°ç«¯å£ {port} è¢«è¿›ç¨‹å ç”¨: PID={pid}, è¿›ç¨‹å={process_name}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå·±çš„è¿›ç¨‹ï¼ˆé¿å…è¯¯æ€ï¼‰
        current_pid = os.getpid()
        if pid == current_pid:
            logging.warning(f"âš ï¸ æ£€æµ‹åˆ°å ç”¨ç«¯å£çš„æ˜¯å½“å‰è¿›ç¨‹ï¼Œè·³è¿‡ç»“æŸæ“ä½œ")
            return False

        if force:
            logging.info(f"ğŸ”ª æ­£åœ¨ç»“æŸå ç”¨ç«¯å£ {port} çš„è¿›ç¨‹: PID={pid}, è¿›ç¨‹å={process_name}")
            success = kill_process_by_pid(pid)
            if success:
                logging.info(f"âœ… æˆåŠŸç»“æŸè¿›ç¨‹: PID={pid}, è¿›ç¨‹å={process_name}")
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç«¯å£é‡Šæ”¾
                import time
                time.sleep(1)
                return True
            else:
                logging.error(f"âŒ ç»“æŸè¿›ç¨‹å¤±è´¥: PID={pid}, è¿›ç¨‹å={process_name}")
                return False
        else:
            logging.info(f"â„¹ï¸ å‘ç°å ç”¨ç«¯å£ {port} çš„è¿›ç¨‹: PID={pid}, è¿›ç¨‹å={process_name}ï¼Œä½†æœªè®¾ç½®å¼ºåˆ¶ç»“æŸ")
            return False

    except Exception as e:
        logging.error(f"å¤„ç†ç«¯å£ {port} å ç”¨è¿›ç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False


def find_available_port(start_port=5001, max_attempts=10, kill_occupied=True):
    """æŸ¥æ‰¾å¯ç”¨ç«¯å£ï¼Œå¯é€‰æ‹©ç»“æŸå ç”¨è¿›ç¨‹

    Args:
        start_port (int): èµ·å§‹ç«¯å£å·
        max_attempts (int): æœ€å¤§å°è¯•æ¬¡æ•°
        kill_occupied (bool): æ˜¯å¦ç»“æŸå ç”¨ç«¯å£çš„è¿›ç¨‹

    Returns:
        int or None: å¯ç”¨ç«¯å£å·ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    import socket

    # é¦–å…ˆå°è¯•é»˜è®¤ç«¯å£
    if kill_occupied:
        logging.info(f"ğŸ” æ£€æŸ¥ç«¯å£ {start_port} æ˜¯å¦è¢«å ç”¨...")
        if kill_port_process(start_port):
            # å°è¯•ä½¿ç”¨é‡Šæ”¾çš„ç«¯å£
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('localhost', start_port))
                    logging.info(f"âœ… ç«¯å£ {start_port} ç°åœ¨å¯ç”¨")
                    return start_port
            except OSError:
                logging.warning(f"âš ï¸ ç«¯å£ {start_port} ä»ç„¶è¢«å ç”¨")

    # å¦‚æœé»˜è®¤ç«¯å£ä¸å¯ç”¨ï¼ŒæŸ¥æ‰¾å…¶ä»–ç«¯å£
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            if kill_occupied and port == start_port:
                # å·²ç»å°è¯•è¿‡ç»“æŸè¿›ç¨‹ï¼Œè·³è¿‡
                continue
            continue

    return None

def start_flask_app():
    """å¯åŠ¨Flaskåº”ç”¨ï¼Œå¸¦é”™è¯¯å¤„ç†"""
    logging.info("å¯åŠ¨ Flask åº”ç”¨ç¨‹åºã€‚")

    # è·å–ç«¯å£é…ç½®
    default_port = int(os.environ.get('PORT', 5001))

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç»“æŸå ç”¨è¿›ç¨‹åŠŸèƒ½
    kill_occupied_process = app_config.get('KILL_OCCUPIED_PORT_PROCESS', True)

    if kill_occupied_process:
        logging.info(f"ğŸ” å¯ç”¨è‡ªåŠ¨ç»“æŸå ç”¨ç«¯å£è¿›ç¨‹åŠŸèƒ½")
    else:
        logging.info(f"â„¹ï¸ è‡ªåŠ¨ç»“æŸå ç”¨ç«¯å£è¿›ç¨‹åŠŸèƒ½å·²ç¦ç”¨")

    # æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨ï¼Œå¯é€‰æ‹©ç»“æŸå ç”¨è¿›ç¨‹
    available_port = find_available_port(default_port, kill_occupied=kill_occupied_process)
    if available_port is None:
        logging.error(f"âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ï¼ˆå°è¯•èŒƒå›´ï¼š{default_port}-{default_port+9}ï¼‰")
        if not kill_occupied_process:
            logging.info(f"ğŸ’¡ æç¤ºï¼šå¯ä»¥åœ¨é…ç½®ä¸­å¯ç”¨ 'KILL_OCCUPIED_PORT_PROCESS' æ¥è‡ªåŠ¨ç»“æŸå ç”¨ç«¯å£çš„è¿›ç¨‹")
        return

    if available_port != default_port:
        if kill_occupied_process:
            logging.info(f"ğŸ”„ ç«¯å£ {default_port} å¤„ç†å®Œæˆï¼Œä½¿ç”¨ç«¯å£ {available_port}")
        else:
            logging.warning(f"âš ï¸ ç«¯å£ {default_port} è¢«å ç”¨ï¼Œä½¿ç”¨ç«¯å£ {available_port}")

    logging.info(f"ğŸŒ å¯åŠ¨æœåŠ¡å™¨ï¼Œç«¯å£: {available_port}")

    try:
        # å¯åŠ¨ç¼“å­˜æ¸…ç†åå°ä»»åŠ¡
        start_cache_cleanup_task()

        # æ£€æµ‹æ˜¯å¦ä¸ºæ‰“åŒ…ç¯å¢ƒ
        import sys
        is_packaged = getattr(sys, 'frozen', False)
        debug_mode = not is_packaged  # æ‰“åŒ…ç¯å¢ƒä¸‹ç¦ç”¨è°ƒè¯•æ¨¡å¼

        if is_packaged:
            logging.info("ğŸ æ£€æµ‹åˆ°æ‰“åŒ…ç¯å¢ƒï¼Œç¦ç”¨è°ƒè¯•æ¨¡å¼")

        app.run(debug=debug_mode, port=available_port, host='0.0.0.0', threaded=True, use_reloader=False)

    except OSError as e:
        if "Address already in use" in str(e):
            logging.error(f"âŒ ç«¯å£ {available_port} è¢«å ç”¨ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–ç«¯å£...")
            # é€’å½’å°è¯•ä¸‹ä¸€ä¸ªç«¯å£
            os.environ['PORT'] = str(available_port + 1)
            start_flask_app()
        else:
            logging.error(f"âŒ å¯åŠ¨æœåŠ¡å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    except Exception as e:
        logging.error(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)

if __name__ == '__main__':
    start_flask_app()







